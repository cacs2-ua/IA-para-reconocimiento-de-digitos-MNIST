{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 1A: DecisionStump y ADABoost Binario sin mejoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def balance_training_dataset(digit, X_train, y_train):\n",
    "    # Find indices of the target digit and calculate how many there are\n",
    "    target_indices = np.where(y_train == digit)[0]\n",
    "    num_target_samples = len(target_indices)\n",
    "    #print(f\"Total elements for target digit {digit}: {num_target_samples}\")\n",
    "\n",
    "    # Determine how many samples each non-target digit should have\n",
    "    num_samples_per_other_digit = num_target_samples // 9\n",
    "    #print(f\"Each non-target digit will have {num_samples_per_other_digit} samples.\")\n",
    "\n",
    "    # Initialize lists to collect balanced dataset samples\n",
    "    X_train_balanced = []\n",
    "    y_train_balanced = []\n",
    "\n",
    "    # Add target digit samples to the balanced dataset\n",
    "    X_train_balanced.extend(X_train[target_indices])\n",
    "    y_train_balanced.extend(y_train[target_indices])\n",
    "\n",
    "    # Collect samples for each non-target digit class to balance the dataset\n",
    "    for i in range(10):  # There are 10 digit classes (0-9)\n",
    "        if i == digit:\n",
    "            continue  # Skip the target digit\n",
    "        indices = np.where(y_train == i)[0]\n",
    "        if len(indices) >= num_samples_per_other_digit:\n",
    "            balanced_indices = np.random.choice(indices, num_samples_per_other_digit, replace=False)\n",
    "        else:\n",
    "            balanced_indices = np.random.choice(indices, num_samples_per_other_digit, replace=True)\n",
    "        X_train_balanced.extend(X_train[balanced_indices])\n",
    "        y_train_balanced.extend([i] * num_samples_per_other_digit)\n",
    "        #print(f\"Collected {len(balanced_indices)} samples for digit {i}.\")\n",
    "\n",
    "    # Convert lists to numpy arrays for training\n",
    "    X_train_balanced = np.array(X_train_balanced)\n",
    "    y_train_balanced = np.array(y_train_balanced)\n",
    "\n",
    "    # Convert labels to binary (1 for the specified digit, -1 for all others)\n",
    "    y_train_binary_balanced = np.where(y_train_balanced == digit, 1, -1)\n",
    "\n",
    "    # Shuffle the balanced training set\n",
    "    shuffle_indices = np.random.permutation(len(X_train_balanced))\n",
    "    X_train_balanced = X_train_balanced[shuffle_indices]\n",
    "    y_train_binary_balanced = y_train_binary_balanced[shuffle_indices]\n",
    "\n",
    "    return X_train_balanced, y_train_binary_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, img_shape): # Inicializamos la clase\n",
    "        self.feature_index = (np.random.randint(0, img_shape[0]), np.random.randint(0, img_shape[1])) # Elegimos un índice de característica aleatorio en 2D\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.img_shape = img_shape # Inicializamos la forma de la imagen\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index[0], self.feature_index[1]] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, img_rows, img_cols = X.shape # Obtenemos el número de muestras y el tamaño de la imagen\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump((img_rows, img_cols)) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index[0], clf.feature_index[1]]), max(X[:, clf.feature_index[0], clf.feature_index[1]])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_on_mnist\n",
    "    if verboseParam:\n",
    "        print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train) # Convertimos las etiquetas a binarias\n",
    "    \n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A) # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    if verboseParam:\n",
    "        print(f\"Accuracy for digit {digit}: {accuracy:.4f}\") # Mostramos la precisión\n",
    "\n",
    "    return y_test_binary, np.sign(y_pred), accuracy # Devolvemos las etiquetas verdaderas, las predicciones, y la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    all_true_labels = [] # Inicializamos las etiquetas verdaderas\n",
    "    all_pred_labels = [] # Inicializamos las predicciones\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        y_test_binary, y_pred, accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        all_true_labels.extend(y_test_binary) # Guardamos las etiquetas verdaderas\n",
    "        all_pred_labels.extend(y_pred) # Guardamos las predicciones\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "\n",
    "        if verboseParam: # Si verboseParam es True\n",
    "            print(f\"Confusion Matrix for digit {digit}:\") # Mostramos la matriz de confusión para cada dígito\n",
    "            print_confusion_matrix(y_test_binary, y_pred) # Mostramos la matriz de confusión para cada dígito\n",
    "\n",
    "    if verboseParam: # Si verboseParam es True\n",
    "        print(\"Overall Confusion Matrix:\") # Mostramos la matriz de confusión general\n",
    "        print_confusion_matrix(all_true_labels, all_pred_labels) # Mostramos la matriz de confusión general\n",
    "\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "\n",
    "def print_confusion_matrix(true_labels, pred_labels): # Creamos la función para mostrar la matriz de confusión\n",
    "    num_classes = 2 # Número de clases (1 y -1)\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int) # Inicializamos la matriz de confusión\n",
    "\n",
    "    for t, p in zip(true_labels, pred_labels): # Para cada par de etiquetas verdaderas y predicciones\n",
    "        matrix[int((t + 1) / 2), int((p + 1) / 2)] += 1 # Actualizamos la matriz de confusión\n",
    "\n",
    "    print(matrix) # Imprimimos la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=50, A=20, verboseParam=False)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for all digits:\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=50, A=50, verboseParam=False) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 1B: Experimentación con los parámetros T y A del método AdaboostBinario.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def experiment_adaboost_fixed_T(T_fixed, A_values, repetitions=3, verboseParam=False):\n",
    "    accuracies = []\n",
    "    execution_times = []\n",
    "\n",
    "    for A in A_values:\n",
    "        acc = []\n",
    "        exec_time = []\n",
    "        for _ in range(repetitions):\n",
    "            start_time = time.time()\n",
    "            y_test_binary, y_pred, accuracy = run_adaboost_on_mnist(digit=0, T=T_fixed, A=A, verboseParam=verboseParam)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            acc.append(accuracy)\n",
    "            exec_time.append(end_time - start_time)\n",
    "\n",
    "        avg_accuracy = np.mean(acc)\n",
    "        avg_execution_time = np.mean(exec_time)\n",
    "        accuracies.append(avg_accuracy)\n",
    "        execution_times.append(avg_execution_time)\n",
    "\n",
    "    # Plotting results\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(A_values, accuracies, marker='o')\n",
    "    plt.xlabel('A values')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.title(f'Accuracy vs A (T fixed at {T_fixed})')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(A_values, execution_times, marker='o')\n",
    "    plt.xlabel('A values')\n",
    "    plt.ylabel('Average Execution Time (s)')\n",
    "    plt.title(f'Execution Time vs A (T fixed at {T_fixed})')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def experiment_adaboost_fixed_A(A_fixed, T_values, repetitions=3, verboseParam=False):\n",
    "    accuracies = []\n",
    "    execution_times = []\n",
    "\n",
    "    for T in T_values:\n",
    "        acc = []\n",
    "        exec_time = []\n",
    "        for _ in range(repetitions):\n",
    "            start_time = time.time()\n",
    "            y_test_binary, y_pred, accuracy = run_adaboost_on_mnist(digit=0, T=T, A=A_fixed, verboseParam=verboseParam)\n",
    "            end_time = time.time()\n",
    "\n",
    "            acc.append(accuracy)\n",
    "            exec_time.append(end_time - start_time)\n",
    "\n",
    "        avg_accuracy = np.mean(acc)\n",
    "        avg_execution_time = np.mean(exec_time)\n",
    "        accuracies.append(avg_accuracy)\n",
    "        execution_times.append(avg_execution_time)\n",
    "\n",
    "    # Plotting results\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(T_values, accuracies, marker='o')\n",
    "    plt.xlabel('T values')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.title(f'Accuracy vs T (A fixed at {A_fixed})')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(T_values, execution_times, marker='o')\n",
    "    plt.xlabel('T values')\n",
    "    plt.ylabel('Average Execution Time (s)')\n",
    "    plt.title(f'Execution Time vs T (A fixed at {A_fixed})')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    T_fixed = 10\n",
    "    A_values = [10, 20, 40, 80, 160]\n",
    "\n",
    "    experiment_adaboost_fixed_T(T_fixed, A_values, repetitions=3, verboseParam=False)\n",
    "\n",
    "    A_fixed = 20\n",
    "    T_values = [5, 10, 20, 40, 80]\n",
    "\n",
    "    experiment_adaboost_fixed_A(A_fixed, T_values, repetitions=3, verboseParam=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tareas 1C y 1D: ADABoost Binario con mejoras y ADABoost Multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclase con ADABoosti Binario sin Mejoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, img_shape): # Inicializamos la clase\n",
    "        self.feature_index = (np.random.randint(0, img_shape[0]), np.random.randint(0, img_shape[1])) # Elegimos un índice de característica aleatorio en 2D\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.img_shape = img_shape # Inicializamos la forma de la imagen\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index[0], self.feature_index[1]] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, img_rows, img_cols = X.shape # Obtenemos el número de muestras y el tamaño de la imagen\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump((img_rows, img_cols)) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index[0], clf.feature_index[1]]), max(X[:, clf.feature_index[0], clf.feature_index[1]])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train) # Convertimos las etiquetas a binarias\n",
    "    \n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A) # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Classifier 1/50: error = 0.2957585644371941, alpha = 0.4337888923022548\n",
      "Classifier 2/50: error = 0.3026060338665315, alpha = 0.4174593704210046\n",
      "Classifier 3/50: error = 0.3301200543250551, alpha = 0.3538210606649647\n",
      "Classifier 4/50: error = 0.3866372785746548, alpha = 0.2307347436005932\n",
      "Classifier 5/50: error = 0.4271370138948748, alpha = 0.14677086696308053\n",
      "Classifier 6/50: error = 0.4193603168547039, alpha = 0.16269995127860043\n",
      "Classifier 7/50: error = 0.3895996861414339, alpha = 0.22449761394304185\n",
      "Classifier 8/50: error = 0.42795019761089714, alpha = 0.14510961327773547\n",
      "Classifier 9/50: error = 0.4366643684066019, alpha = 0.127355367309269\n",
      "Classifier 10/50: error = 0.36622303752575003, alpha = 0.274227279139486\n",
      "Classifier 11/50: error = 0.4247236554943056, alpha = 0.15170589467672954\n",
      "Classifier 12/50: error = 0.43220069040737036, alpha = 0.13643899327581335\n",
      "Classifier 13/50: error = 0.3931090333200794, alpha = 0.2171310682071655\n",
      "Classifier 14/50: error = 0.4442924733143536, alpha = 0.11187952770581296\n",
      "Classifier 15/50: error = 0.4377991362766743, alpha = 0.12504949126665957\n",
      "Classifier 16/50: error = 0.4067662687631325, alpha = 0.18867486474443762\n",
      "Classifier 17/50: error = 0.40724905859818905, alpha = 0.18767468859413733\n",
      "Classifier 18/50: error = 0.3784909225937536, alpha = 0.2479792131858585\n",
      "Classifier 19/50: error = 0.392494225503463, alpha = 0.21841992770114857\n",
      "Classifier 20/50: error = 0.4442141230084413, alpha = 0.1120382007474707\n",
      "Classifier 21/50: error = 0.3938033541712996, alpha = 0.21567637389596975\n",
      "Classifier 22/50: error = 0.4514615129218541, alpha = 0.09738365928070129\n",
      "Classifier 23/50: error = 0.4388958079341186, alpha = 0.1228222842439867\n",
      "Classifier 24/50: error = 0.4222026035608488, alpha = 0.15686898996020354\n",
      "Classifier 25/50: error = 0.4127138557385796, alpha = 0.17637883016230957\n",
      "Classifier 26/50: error = 0.4206282017131242, alpha = 0.1600975489601702\n",
      "Classifier 27/50: error = 0.450267169132715, alpha = 0.09979564116681672\n",
      "Classifier 28/50: error = 0.4108109476561095, alpha = 0.18030698824925462\n",
      "Classifier 29/50: error = 0.4466562706773003, alpha = 0.1070950254962406\n",
      "Classifier 30/50: error = 0.4381430025574964, alpha = 0.12435100878399091\n",
      "Classifier 31/50: error = 0.466297502386505, alpha = 0.06750735777362928\n",
      "Classifier 32/50: error = 0.45507698476855496, alpha = 0.09008896313054306\n",
      "Classifier 33/50: error = 0.44767036193060955, alpha = 0.1050439380476085\n",
      "Classifier 34/50: error = 0.4592636749138663, alpha = 0.08165363770748235\n",
      "Classifier 35/50: error = 0.43774398744453324, alpha = 0.12516152426778507\n",
      "Classifier 36/50: error = 0.46396626040378397, alpha = 0.07219263556660535\n",
      "Classifier 37/50: error = 0.4389295053169338, alpha = 0.12275386825656474\n",
      "Classifier 38/50: error = 0.42851363186148705, alpha = 0.14395903980260838\n",
      "Classifier 39/50: error = 0.4583351553319818, alpha = 0.08352337283612708\n",
      "Classifier 40/50: error = 0.45103759582381575, alpha = 0.09823963075746286\n",
      "Classifier 41/50: error = 0.45734512375614067, alpha = 0.08551761535366338\n",
      "Classifier 42/50: error = 0.46179133722606935, alpha = 0.07656659796417718\n",
      "Classifier 43/50: error = 0.43652289440874303, alpha = 0.12764293989700454\n",
      "Classifier 44/50: error = 0.46745637953646835, alpha = 0.06517938599738679\n",
      "Classifier 45/50: error = 0.44608047168170606, alpha = 0.1082600274428363\n",
      "Classifier 46/50: error = 0.4367610670597004, alpha = 0.12715882111217414\n",
      "Classifier 47/50: error = 0.4705070072216203, alpha = 0.05905453960991421\n",
      "Classifier 48/50: error = 0.4684723644477562, alpha = 0.06313903958280875\n",
      "Classifier 49/50: error = 0.44429110875715017, alpha = 0.11188229112424061\n",
      "Classifier 50/50: error = 0.4554901622866572, alpha = 0.08925594537364104\n",
      "Accuracy for digit 3: 0.891\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=50, A=20, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n",
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Classifier 1/50: error = 0.24052342760658513, alpha = 0.5749059891486037\n",
      "Classifier 2/50: error = 0.25252229805391857, alpha = 0.542602460222201\n",
      "Classifier 3/50: error = 0.2738234236395714, alpha = 0.4876548718878465\n",
      "Classifier 4/50: error = 0.2829808030110401, alpha = 0.46486177630565095\n",
      "Classifier 5/50: error = 0.2611382918594022, alpha = 0.5200303242735971\n",
      "Classifier 6/50: error = 0.29958325374514294, alpha = 0.4246415773946476\n",
      "Classifier 7/50: error = 0.3240640077562412, alpha = 0.3675786671426079\n",
      "Classifier 8/50: error = 0.29023366169318043, alpha = 0.4471247430794601\n",
      "Classifier 9/50: error = 0.3557339195461215, alpha = 0.29696438733156594\n",
      "Classifier 10/50: error = 0.3811802478103294, alpha = 0.242270841810073\n",
      "Classifier 11/50: error = 0.4040401035043397, alpha = 0.19432961891415823\n",
      "Classifier 12/50: error = 0.31559058803262474, alpha = 0.38705526391328177\n",
      "Classifier 13/50: error = 0.38848440973445697, alpha = 0.2268437051044694\n",
      "Classifier 14/50: error = 0.3827078110074197, alpha = 0.23903533661584958\n",
      "Classifier 15/50: error = 0.38754861176891064, alpha = 0.2288141411848405\n",
      "Classifier 16/50: error = 0.3876215458531959, alpha = 0.22866050704721502\n",
      "Classifier 17/50: error = 0.3893751331176789, alpha = 0.22496978664235515\n",
      "Classifier 18/50: error = 0.4427945206769201, alpha = 0.11491412449428805\n",
      "Classifier 19/50: error = 0.4237066283390126, alpha = 0.1537877776586801\n",
      "Classifier 20/50: error = 0.4160215301001945, alpha = 0.1695635507495102\n",
      "Classifier 21/50: error = 0.411716824653185, alpha = 0.17843631419464323\n",
      "Classifier 22/50: error = 0.40367593526295914, alpha = 0.19508591818016965\n",
      "Classifier 23/50: error = 0.36941323610272825, alpha = 0.267367429301572\n",
      "Classifier 24/50: error = 0.42297401302004345, alpha = 0.15528828065301473\n",
      "Classifier 25/50: error = 0.4325801901862012, alpha = 0.1356658575013743\n",
      "Classifier 26/50: error = 0.433251554815006, alpha = 0.13429851431385767\n",
      "Classifier 27/50: error = 0.4230617892846258, alpha = 0.1551084656124888\n",
      "Classifier 28/50: error = 0.41402261322394235, alpha = 0.17368030299735246\n",
      "Classifier 29/50: error = 0.43220770380288853, alpha = 0.13642470377104035\n",
      "Classifier 30/50: error = 0.4245856648148375, alpha = 0.15198828854748503\n",
      "Classifier 31/50: error = 0.4535612853309653, alpha = 0.09314588040849259\n",
      "Classifier 32/50: error = 0.4401687617232224, alpha = 0.12023858764773447\n",
      "Classifier 33/50: error = 0.43856766630565713, alpha = 0.12348857196970313\n",
      "Classifier 34/50: error = 0.4219885790023083, alpha = 0.15730768911528445\n",
      "Classifier 35/50: error = 0.40303413547727596, alpha = 0.19641933577312407\n",
      "Classifier 36/50: error = 0.42665454777974965, alpha = 0.147756877124268\n",
      "Classifier 37/50: error = 0.4313205700601537, alpha = 0.13823264380348388\n",
      "Classifier 38/50: error = 0.4125257240816861, alpha = 0.17676694703802887\n",
      "Classifier 39/50: error = 0.41761424230410205, alpha = 0.16628745044441812\n",
      "Classifier 40/50: error = 0.4120874822657225, alpha = 0.17767124749395205\n",
      "Classifier 41/50: error = 0.4307882917469707, alpha = 0.13931783431555123\n",
      "Classifier 42/50: error = 0.44581558230075236, alpha = 0.1087960706671505\n",
      "Classifier 43/50: error = 0.4148063676803156, alpha = 0.17206548018466214\n",
      "Classifier 44/50: error = 0.43388361860152513, alpha = 0.1330116698114266\n",
      "Classifier 45/50: error = 0.43957249793726116, alpha = 0.12144861498579677\n",
      "Classifier 46/50: error = 0.4378085573827568, alpha = 0.12503035291830036\n",
      "Classifier 47/50: error = 0.45596906790354197, alpha = 0.08829056600332726\n",
      "Classifier 48/50: error = 0.40833541121589, alpha = 0.18542548912771573\n",
      "Classifier 49/50: error = 0.4281211921210908, alpha = 0.14476038993686852\n",
      "Classifier 50/50: error = 0.4423224637183333, alpha = 0.11587086565865701\n",
      "Accuracy for digit 0: 0.9609\n",
      "Running AdaBoost for digit: 1\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Classifier 1/50: error = 0.17881777052584724, alpha = 0.7621888996194138\n",
      "Classifier 2/50: error = 0.2912034579655779, alpha = 0.44477316466161376\n",
      "Classifier 3/50: error = 0.26280916172562824, alpha = 0.5157093248834823\n",
      "Classifier 4/50: error = 0.21628798249833614, alpha = 0.6437154279603065\n",
      "Classifier 5/50: error = 0.3122226510030195, alpha = 0.394874303214081\n",
      "Classifier 6/50: error = 0.35011662385100606, alpha = 0.309263307683446\n",
      "Classifier 7/50: error = 0.3879866959390478, alpha = 0.2278914853124372\n",
      "Classifier 8/50: error = 0.31711317197962896, alpha = 0.3835352138743555\n",
      "Classifier 9/50: error = 0.39593754942744086, alpha = 0.21121054606933007\n",
      "Classifier 10/50: error = 0.34174424437427686, alpha = 0.32776545409539803\n",
      "Classifier 11/50: error = 0.3658094363013358, alpha = 0.2751184754255174\n",
      "Classifier 12/50: error = 0.3552976013318036, alpha = 0.29791653044085364\n",
      "Classifier 13/50: error = 0.40574884844229364, alpha = 0.19078383784997563\n",
      "Classifier 14/50: error = 0.4161318323680422, alpha = 0.16933655106566414\n",
      "Classifier 15/50: error = 0.35076262728091545, alpha = 0.3078443410299957\n",
      "Classifier 16/50: error = 0.3985125267840106, alpha = 0.20583338831386427\n",
      "Classifier 17/50: error = 0.3929832982823019, alpha = 0.2173945964100589\n",
      "Classifier 18/50: error = 0.4014251312238911, alpha = 0.1997652838950504\n",
      "Classifier 19/50: error = 0.4245807900090095, alpha = 0.1519982651340492\n",
      "Classifier 20/50: error = 0.4130784615763121, alpha = 0.1756267954622514\n",
      "Classifier 21/50: error = 0.40961287827679804, alpha = 0.18278297291882603\n",
      "Classifier 22/50: error = 0.43515062121624226, alpha = 0.1304334412521329\n",
      "Classifier 23/50: error = 0.39398861693108733, alpha = 0.21528837595173358\n",
      "Classifier 24/50: error = 0.4164578929192922, alpha = 0.168665626469822\n",
      "Classifier 25/50: error = 0.33011542210175726, alpha = 0.35383153417255697\n",
      "Classifier 26/50: error = 0.3741193186608178, alpha = 0.2572924837745826\n",
      "Classifier 27/50: error = 0.4386447804996584, alpha = 0.12333198271921897\n",
      "Classifier 28/50: error = 0.4240582752652218, alpha = 0.1530677978645031\n",
      "Classifier 29/50: error = 0.4442466543126631, alpha = 0.11197231849807021\n",
      "Classifier 30/50: error = 0.4215027406234637, alpha = 0.15830376480007274\n",
      "Classifier 31/50: error = 0.38506899802650907, alpha = 0.23404376780929498\n",
      "Classifier 32/50: error = 0.43469552095250613, alpha = 0.13135932636630793\n",
      "Classifier 33/50: error = 0.43741767912518015, alpha = 0.12582447272418365\n",
      "Classifier 34/50: error = 0.44526833976334085, alpha = 0.10990369528956366\n",
      "Classifier 35/50: error = 0.3858105597699437, alpha = 0.2324784719222411\n",
      "Classifier 36/50: error = 0.4295727706362322, alpha = 0.14179722354694174\n",
      "Classifier 37/50: error = 0.4303527024782503, alpha = 0.14020614108779383\n",
      "Classifier 38/50: error = 0.3896981033826955, alpha = 0.2242907008327045\n",
      "Classifier 39/50: error = 0.45877078396371873, alpha = 0.08264608731386625\n",
      "Classifier 40/50: error = 0.42895997476247694, alpha = 0.1430478441130641\n",
      "Classifier 41/50: error = 0.44074131282005424, alpha = 0.11907701136863746\n",
      "Classifier 42/50: error = 0.37929992409608515, alpha = 0.2462603735128061\n",
      "Classifier 43/50: error = 0.4171309363852471, alpha = 0.16728120085288464\n",
      "Classifier 44/50: error = 0.4460227791110587, alpha = 0.10837677168669796\n",
      "Classifier 45/50: error = 0.4467031470337907, alpha = 0.1070001943430582\n",
      "Classifier 46/50: error = 0.4496046361707683, alpha = 0.10113412648675552\n",
      "Classifier 47/50: error = 0.4492750813799593, alpha = 0.10180004532087035\n",
      "Classifier 48/50: error = 0.4496782446346582, alpha = 0.10098540089553501\n",
      "Classifier 49/50: error = 0.4328539618213315, alpha = 0.13510821647063648\n",
      "Classifier 50/50: error = 0.42508505697525956, alpha = 0.15096641082998408\n",
      "Accuracy for digit 1: 0.9639\n",
      "Running AdaBoost for digit: 2\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Classifier 1/50: error = 0.31201745552198723, alpha = 0.39535216630387565\n",
      "Classifier 2/50: error = 0.28395713357094543, alpha = 0.4624583728159681\n",
      "Classifier 3/50: error = 0.33185275313550666, alpha = 0.3499086113245768\n",
      "Classifier 4/50: error = 0.3924580018266993, alpha = 0.21849588787426824\n",
      "Classifier 5/50: error = 0.3111657117197483, alpha = 0.39733756302342155\n",
      "Classifier 6/50: error = 0.36709537024852024, alpha = 0.27234903402303356\n",
      "Classifier 7/50: error = 0.3490754851240133, alpha = 0.31155274710623365\n",
      "Classifier 8/50: error = 0.33917148191087276, alpha = 0.3334942761092445\n",
      "Classifier 9/50: error = 0.33900660164213015, alpha = 0.33386213562048705\n",
      "Classifier 10/50: error = 0.37060347584843734, alpha = 0.26481438462088547\n",
      "Classifier 11/50: error = 0.417047472106609, alpha = 0.16745284919061848\n",
      "Classifier 12/50: error = 0.39019493835568936, alpha = 0.22324643937222818\n",
      "Classifier 13/50: error = 0.4267008563320044, alpha = 0.1476622245504149\n",
      "Classifier 14/50: error = 0.4454926113864768, alpha = 0.10944973484838065\n",
      "Classifier 15/50: error = 0.4083563408475741, alpha = 0.18538217440683366\n",
      "Classifier 16/50: error = 0.43836385305982745, alpha = 0.12390246726378502\n",
      "Classifier 17/50: error = 0.38842208035273074, alpha = 0.22697489319295358\n",
      "Classifier 18/50: error = 0.39045323294458567, alpha = 0.22270373798060805\n",
      "Classifier 19/50: error = 0.4368243123015437, alpha = 0.12703027639945652\n",
      "Classifier 20/50: error = 0.43521485328268394, alpha = 0.1303027813652868\n",
      "Classifier 21/50: error = 0.41361785245859095, alpha = 0.17451461001253832\n",
      "Classifier 22/50: error = 0.4277227681997884, alpha = 0.1455741485135254\n",
      "Classifier 23/50: error = 0.4368263883711882, alpha = 0.1270260568994567\n",
      "Classifier 24/50: error = 0.4234075598899101, alpha = 0.15440023008349948\n",
      "Classifier 25/50: error = 0.4192760421824959, alpha = 0.1628730066788177\n",
      "Classifier 26/50: error = 0.445667826526081, alpha = 0.10909510358567366\n",
      "Classifier 27/50: error = 0.45206668259521465, alpha = 0.09616194956038611\n",
      "Classifier 28/50: error = 0.44003854174532697, alpha = 0.1205028194074693\n",
      "Classifier 29/50: error = 0.45401570868762625, alpha = 0.09222920336373479\n",
      "Classifier 30/50: error = 0.452939943917911, alpha = 0.09439952276811478\n",
      "Classifier 31/50: error = 0.45007469093830976, alpha = 0.1001844592020636\n",
      "Classifier 32/50: error = 0.44564177841712815, alpha = 0.10914782260509316\n",
      "Classifier 33/50: error = 0.45512526996658287, alpha = 0.08999160769573183\n",
      "Classifier 34/50: error = 0.4616469107120118, alpha = 0.07685715414803057\n",
      "Classifier 35/50: error = 0.4455203452440153, alpha = 0.1093936003579643\n",
      "Classifier 36/50: error = 0.4687843794773966, alpha = 0.06251254316441303\n",
      "Classifier 37/50: error = 0.4613323781031228, alpha = 0.07748997339100204\n",
      "Classifier 38/50: error = 0.44285158607577535, alpha = 0.11479848145406865\n",
      "Classifier 39/50: error = 0.44642345584302234, alpha = 0.10756603978296904\n",
      "Classifier 40/50: error = 0.4495571847771253, alpha = 0.1012300041910347\n",
      "Classifier 41/50: error = 0.45678916792242075, alpha = 0.08663778606863369\n",
      "Classifier 42/50: error = 0.46465087411311046, alpha = 0.070816395277862\n",
      "Classifier 43/50: error = 0.46141007386340727, alpha = 0.07733364881113163\n",
      "Classifier 44/50: error = 0.46380969232967995, alpha = 0.07250741369863951\n",
      "Classifier 45/50: error = 0.47489796268336815, alpha = 0.05024631746258448\n",
      "Classifier 46/50: error = 0.45955128981212656, alpha = 0.0810745913946774\n",
      "Classifier 47/50: error = 0.44158308899565746, alpha = 0.1173698181546384\n",
      "Classifier 48/50: error = 0.4587458191028546, alpha = 0.0826963590599611\n",
      "Classifier 49/50: error = 0.47080635763414946, alpha = 0.05845376964223461\n",
      "Classifier 50/50: error = 0.4692515488557666, alpha = 0.06157460307428731\n",
      "Accuracy for digit 2: 0.91\n",
      "Running AdaBoost for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2986133768352368, alpha = 0.42695479154917626\n",
      "Classifier 2/50: error = 0.34046952984031564, alpha = 0.3306012688957759\n",
      "Classifier 3/50: error = 0.28439939148346816, alpha = 0.46137132175213397\n",
      "Classifier 4/50: error = 0.3523314101254853, alpha = 0.3044034456593691\n",
      "Classifier 5/50: error = 0.39384739557063586, alpha = 0.2155841316930041\n",
      "Classifier 6/50: error = 0.3636456573188424, alpha = 0.27978781302626154\n",
      "Classifier 7/50: error = 0.40384853155513356, alpha = 0.1947274453079166\n",
      "Classifier 8/50: error = 0.4098899140514052, alpha = 0.18221024222111565\n",
      "Classifier 9/50: error = 0.37497205019365587, alpha = 0.25547243897208904\n",
      "Classifier 10/50: error = 0.36972074477787487, alpha = 0.26670750324362485\n",
      "Classifier 11/50: error = 0.4234111711320121, alpha = 0.15439283405518178\n",
      "Classifier 12/50: error = 0.4164614834876318, alpha = 0.16865823910803437\n",
      "Classifier 13/50: error = 0.4318564124338172, alpha = 0.13714051368027305\n",
      "Classifier 14/50: error = 0.3989747882663106, alpha = 0.20486932781009304\n",
      "Classifier 15/50: error = 0.40838788928753833, alpha = 0.18531688493152074\n",
      "Classifier 16/50: error = 0.4159761370650049, alpha = 0.16965697366143637\n",
      "Classifier 17/50: error = 0.41297370502646236, alpha = 0.1758428457806505\n",
      "Classifier 18/50: error = 0.4190418766246301, alpha = 0.16335390912078776\n",
      "Classifier 19/50: error = 0.44241545740848753, alpha = 0.11568237410211162\n",
      "Classifier 20/50: error = 0.41143522488829604, alpha = 0.17901769659096883\n",
      "Classifier 21/50: error = 0.41054227796434084, alpha = 0.18086204200524633\n",
      "Classifier 22/50: error = 0.44720118443621665, alpha = 0.10599277969905313\n",
      "Classifier 23/50: error = 0.4233340940896012, alpha = 0.15455069584227518\n",
      "Classifier 24/50: error = 0.44647419784512327, alpha = 0.10746337815614863\n",
      "Classifier 25/50: error = 0.42539548899315793, alpha = 0.1503313492937123\n",
      "Classifier 26/50: error = 0.41499578940071913, alpha = 0.1716753352923357\n",
      "Classifier 27/50: error = 0.4428645835670185, alpha = 0.11477214246345865\n",
      "Classifier 28/50: error = 0.4305536037833795, alpha = 0.1397964114087752\n",
      "Classifier 29/50: error = 0.4411438187317267, alpha = 0.11826060978059723\n",
      "Classifier 30/50: error = 0.43716190400627, alpha = 0.12634419840494085\n",
      "Classifier 31/50: error = 0.43634211880832596, alpha = 0.12801043097231538\n",
      "Classifier 32/50: error = 0.43897223142981523, alpha = 0.12266712283118594\n",
      "Classifier 33/50: error = 0.46010087027131963, alpha = 0.07996828831807301\n",
      "Classifier 34/50: error = 0.4442833217649924, alpha = 0.11189806090069711\n",
      "Classifier 35/50: error = 0.4577461578738249, alpha = 0.08470972257863846\n",
      "Classifier 36/50: error = 0.4579517351780671, alpha = 0.08429562503709662\n",
      "Classifier 37/50: error = 0.4580785586675884, alpha = 0.08404017691457906\n",
      "Classifier 38/50: error = 0.446350093049267, alpha = 0.10771447197157086\n",
      "Classifier 39/50: error = 0.45254779365076386, alpha = 0.0951908923062929\n",
      "Classifier 40/50: error = 0.45258973711881956, alpha = 0.09510624362614932\n",
      "Classifier 41/50: error = 0.46012721371668097, alpha = 0.07991526400401955\n",
      "Classifier 42/50: error = 0.4473923151445206, alpha = 0.10560622343248051\n",
      "Classifier 43/50: error = 0.42120964091842017, alpha = 0.15890483432535568\n",
      "Classifier 44/50: error = 0.44562691959004386, alpha = 0.10917789579947935\n",
      "Classifier 45/50: error = 0.45304047585421514, alpha = 0.0941966657030008\n",
      "Classifier 46/50: error = 0.45101194533278544, alpha = 0.09829142870417061\n",
      "Classifier 47/50: error = 0.45017728989413835, alpha = 0.09997719912083325\n",
      "Classifier 48/50: error = 0.4700257673573578, alpha = 0.06002043515946362\n",
      "Classifier 49/50: error = 0.4645948719062132, alpha = 0.07092896322541202\n",
      "Classifier 50/50: error = 0.45842132543195324, alpha = 0.08334983007407239\n",
      "Accuracy for digit 3: 0.8995\n",
      "Running AdaBoost for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.30916716596764515, alpha = 0.4020078773044001\n",
      "Classifier 2/50: error = 0.31807377644750484, alpha = 0.3813190589678485\n",
      "Classifier 3/50: error = 0.31102351138573936, alpha = 0.39766931884759293\n",
      "Classifier 4/50: error = 0.33652990609116307, alpha = 0.33939838013445234\n",
      "Classifier 5/50: error = 0.32852015514058386, alpha = 0.3574429045715893\n",
      "Classifier 6/50: error = 0.38241274614577936, alpha = 0.2396599238078403\n",
      "Classifier 7/50: error = 0.3283269921383627, alpha = 0.35788079373209125\n",
      "Classifier 8/50: error = 0.3311033859635888, alpha = 0.35159941953134816\n",
      "Classifier 9/50: error = 0.3780083095505917, alpha = 0.24900527741201767\n",
      "Classifier 10/50: error = 0.369022742656943, alpha = 0.2682057720878482\n",
      "Classifier 11/50: error = 0.3698118356978878, alpha = 0.2665120620306665\n",
      "Classifier 12/50: error = 0.3836664234257966, alpha = 0.23700742560752963\n",
      "Classifier 13/50: error = 0.4170309589161505, alpha = 0.1674868105259753\n",
      "Classifier 14/50: error = 0.39054408757123765, alpha = 0.22251287449928386\n",
      "Classifier 15/50: error = 0.3806115848486924, alpha = 0.24347658557047075\n",
      "Classifier 16/50: error = 0.39924183778369193, alpha = 0.2043125590688046\n",
      "Classifier 17/50: error = 0.4341835073183352, alpha = 0.13240126752320233\n",
      "Classifier 18/50: error = 0.40099101483411115, alpha = 0.20066878901969853\n",
      "Classifier 19/50: error = 0.43186502991663656, alpha = 0.13712295257246035\n",
      "Classifier 20/50: error = 0.3922268381986871, alpha = 0.21898069090885325\n",
      "Classifier 21/50: error = 0.4389980069980145, alpha = 0.12261479242901942\n",
      "Classifier 22/50: error = 0.42686137844055827, alpha = 0.1473341449858656\n",
      "Classifier 23/50: error = 0.4103741511430474, alpha = 0.1812094369281114\n",
      "Classifier 24/50: error = 0.4197057085125473, alpha = 0.16199080112427028\n",
      "Classifier 25/50: error = 0.4468638000553792, alpha = 0.1066752068402973\n",
      "Classifier 26/50: error = 0.43054942595511014, alpha = 0.13980493143683173\n",
      "Classifier 27/50: error = 0.43751258193151904, alpha = 0.1256316509073053\n",
      "Classifier 28/50: error = 0.4239416982683043, alpha = 0.15330646605117795\n",
      "Classifier 29/50: error = 0.42378573324250424, alpha = 0.15362580049251895\n",
      "Classifier 30/50: error = 0.45207495406364395, alpha = 0.0961452532036163\n",
      "Classifier 31/50: error = 0.44063604826586156, alpha = 0.1192905451826756\n",
      "Classifier 32/50: error = 0.43054952260181334, alpha = 0.13980473434075177\n",
      "Classifier 33/50: error = 0.44812343199685784, alpha = 0.10412785011199524\n",
      "Classifier 34/50: error = 0.44341107964902354, alpha = 0.11366482905038959\n",
      "Classifier 35/50: error = 0.4486872696515922, alpha = 0.10298803792088942\n",
      "Classifier 36/50: error = 0.4450696875785902, alpha = 0.11030583570391392\n",
      "Classifier 37/50: error = 0.4486562920631636, alpha = 0.10305065295656048\n",
      "Classifier 38/50: error = 0.4388098250440367, alpha = 0.12299686097889166\n",
      "Classifier 39/50: error = 0.45980987575300536, alpha = 0.0805540345167096\n",
      "Classifier 40/50: error = 0.46233521697922053, alpha = 0.07547254009751912\n",
      "Classifier 41/50: error = 0.45527827937997056, alpha = 0.08968311241325867\n",
      "Classifier 42/50: error = 0.4628428199725202, alpha = 0.07445161856977632\n",
      "Classifier 43/50: error = 0.474671079713429, alpha = 0.05070124040735945\n",
      "Classifier 44/50: error = 0.46561161086415004, alpha = 0.06888553075650738\n",
      "Classifier 45/50: error = 0.441323896777182, alpha = 0.11789540885926462\n",
      "Classifier 46/50: error = 0.4287136467434693, alpha = 0.1435506861311828\n",
      "Classifier 47/50: error = 0.434336819422074, alpha = 0.13208924948718678\n",
      "Classifier 48/50: error = 0.45758856575001383, alpha = 0.08502718244759803\n",
      "Classifier 49/50: error = 0.4559937648282273, alpha = 0.08824078633450134\n",
      "Classifier 50/50: error = 0.4479413299689752, alpha = 0.10449603146339179\n",
      "Accuracy for digit 4: 0.9025\n",
      "Running AdaBoost for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3888735123166343, alpha = 0.22602491276780973\n",
      "Classifier 2/50: error = 0.3796810799083881, alpha = 0.24545104799348968\n",
      "Classifier 3/50: error = 0.4024320998985895, alpha = 0.19767076544064904\n",
      "Classifier 4/50: error = 0.3710994706404753, alpha = 0.26375148077829247\n",
      "Classifier 5/50: error = 0.38994219146709885, alpha = 0.22377760977089733\n",
      "Classifier 6/50: error = 0.36953817768854114, alpha = 0.26709927219640517\n",
      "Classifier 7/50: error = 0.3977091357251852, alpha = 0.20750978421826297\n",
      "Classifier 8/50: error = 0.34124694550213264, alpha = 0.32887117048143977\n",
      "Classifier 9/50: error = 0.3835160624084615, alpha = 0.23732538216045204\n",
      "Classifier 10/50: error = 0.41899921902891957, alpha = 0.16344152247057855\n",
      "Classifier 11/50: error = 0.3459223744227967, alpha = 0.318505819656309\n",
      "Classifier 12/50: error = 0.38102307370295785, alpha = 0.2426040309618411\n",
      "Classifier 13/50: error = 0.3852297543879297, alpha = 0.23370434630020231\n",
      "Classifier 14/50: error = 0.43284960354019986, alpha = 0.13511709312814174\n",
      "Classifier 15/50: error = 0.42415112461818505, alpha = 0.15287771968329866\n",
      "Classifier 16/50: error = 0.40577468833660324, alpha = 0.19073025460347753\n",
      "Classifier 17/50: error = 0.40636931073092364, alpha = 0.1894975061055732\n",
      "Classifier 18/50: error = 0.4194222542465482, alpha = 0.1625727709431059\n",
      "Classifier 19/50: error = 0.42378412338056104, alpha = 0.15362909680608092\n",
      "Classifier 20/50: error = 0.45570280111428685, alpha = 0.08882728701632941\n",
      "Classifier 21/50: error = 0.42956538338151207, alpha = 0.14181229714766402\n",
      "Classifier 22/50: error = 0.4281130959620679, alpha = 0.1447769239903164\n",
      "Classifier 23/50: error = 0.436549954820471, alpha = 0.12758793288295753\n",
      "Classifier 24/50: error = 0.45285544759162055, alpha = 0.09457002857628467\n",
      "Classifier 25/50: error = 0.43784947619839587, alpha = 0.12494723013185496\n",
      "Classifier 26/50: error = 0.4324286837847934, alpha = 0.13597449447606638\n",
      "Classifier 27/50: error = 0.4462898320759414, alpha = 0.10783639927509124\n",
      "Classifier 28/50: error = 0.4361542184702918, alpha = 0.12839244207793607\n",
      "Classifier 29/50: error = 0.4275500942824373, alpha = 0.14592688474543963\n",
      "Classifier 30/50: error = 0.41228329214833803, alpha = 0.17726716349967503\n",
      "Classifier 31/50: error = 0.4320562825657335, alpha = 0.13673323062852694\n",
      "Classifier 32/50: error = 0.4136745161776347, alpha = 0.17439779832393906\n",
      "Classifier 33/50: error = 0.4603624277360787, alpha = 0.07944184303772478\n",
      "Classifier 34/50: error = 0.4403369705182724, alpha = 0.11989729680327589\n",
      "Classifier 35/50: error = 0.4545215066984136, alpha = 0.09120907341557398\n",
      "Classifier 36/50: error = 0.453763457615434, alpha = 0.09273803295311442\n",
      "Classifier 37/50: error = 0.44851682977707086, alpha = 0.10333255821835084\n",
      "Classifier 38/50: error = 0.446792088951256, alpha = 0.10682026957336879\n",
      "Classifier 39/50: error = 0.46309724344654146, alpha = 0.07393996525649255\n",
      "Classifier 40/50: error = 0.4596096717625787, alpha = 0.08095705942811912\n",
      "Classifier 41/50: error = 0.4475956702176819, alpha = 0.10519497826468327\n",
      "Classifier 42/50: error = 0.43730615342747414, alpha = 0.12605108053022632\n",
      "Classifier 43/50: error = 0.46048600493176756, alpha = 0.07919313046816011\n",
      "Classifier 44/50: error = 0.44777905851328814, alpha = 0.10482414233319066\n",
      "Classifier 45/50: error = 0.4514751455096472, alpha = 0.09735613478883628\n",
      "Classifier 46/50: error = 0.458194820844269, alpha = 0.08380601102286195\n",
      "Classifier 47/50: error = 0.4651719354743684, alpha = 0.06976911477201099\n",
      "Classifier 48/50: error = 0.46543151097035024, alpha = 0.0692474514996305\n",
      "Classifier 49/50: error = 0.462374802639688, alpha = 0.07539291742900003\n",
      "Classifier 50/50: error = 0.4716176413537183, alpha = 0.056825805145887906\n",
      "Accuracy for digit 5: 0.8737\n",
      "Running AdaBoost for digit: 6\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2622770687177751, alpha = 0.5170834314094018\n",
      "Classifier 2/50: error = 0.27973856205056324, alpha = 0.4728793970578081\n",
      "Classifier 3/50: error = 0.30444137009918687, alpha = 0.4131183913840453\n",
      "Classifier 4/50: error = 0.3407574344797131, alpha = 0.3299603297611685\n",
      "Classifier 5/50: error = 0.2640595321750077, alpha = 0.512487325359739\n",
      "Classifier 6/50: error = 0.331043752535589, alpha = 0.3517340542023011\n",
      "Classifier 7/50: error = 0.320356859316718, alpha = 0.3760661536694937\n",
      "Classifier 8/50: error = 0.35561212715365853, alpha = 0.2972301126448939\n",
      "Classifier 9/50: error = 0.3894607764121752, alpha = 0.22478969093142315\n",
      "Classifier 10/50: error = 0.2969842069840738, alpha = 0.4308501971424071\n",
      "Classifier 11/50: error = 0.3738896006336109, alpha = 0.2577830719457714\n",
      "Classifier 12/50: error = 0.3894495402085535, alpha = 0.2248133182576831\n",
      "Classifier 13/50: error = 0.3844076318724711, alpha = 0.2354407367254079\n",
      "Classifier 14/50: error = 0.39193500593208397, alpha = 0.21959287472501895\n",
      "Classifier 15/50: error = 0.42339959303339386, alpha = 0.15441654671605132\n",
      "Classifier 16/50: error = 0.3651691592083759, alpha = 0.2764989373623683\n",
      "Classifier 17/50: error = 0.40419610310904064, alpha = 0.19400570842729786\n",
      "Classifier 18/50: error = 0.39532681053145735, alpha = 0.21248766902040295\n",
      "Classifier 19/50: error = 0.4264809743202368, alpha = 0.1481116767829807\n",
      "Classifier 20/50: error = 0.37296920056105, alpha = 0.2597499086089437\n",
      "Classifier 21/50: error = 0.4026521586167423, alpha = 0.1972132666343145\n",
      "Classifier 22/50: error = 0.3808230128047031, alpha = 0.24302821111241385\n",
      "Classifier 23/50: error = 0.4241408476174929, alpha = 0.15289875788576113\n",
      "Classifier 24/50: error = 0.37668871250832925, alpha = 0.251813451340452\n",
      "Classifier 25/50: error = 0.4273615008689009, alpha = 0.14631218234037022\n",
      "Classifier 26/50: error = 0.4305745939458789, alpha = 0.13975360555676447\n",
      "Classifier 27/50: error = 0.412951598127024, alpha = 0.17588844121229982\n",
      "Classifier 28/50: error = 0.3988275622397742, alpha = 0.20517633136510546\n",
      "Classifier 29/50: error = 0.41970208923484087, alpha = 0.16199823130291463\n",
      "Classifier 30/50: error = 0.3992265260283676, alpha = 0.20434447900778338\n",
      "Classifier 31/50: error = 0.4216236324015483, alpha = 0.15805588098686413\n",
      "Classifier 32/50: error = 0.45597222567533446, alpha = 0.0882842011041783\n",
      "Classifier 33/50: error = 0.449456419325208, alpha = 0.10143361153457608\n",
      "Classifier 34/50: error = 0.41873942762911764, alpha = 0.1639751551634245\n",
      "Classifier 35/50: error = 0.4321529665313139, alpha = 0.13653623018025843\n",
      "Classifier 36/50: error = 0.4318740483058521, alpha = 0.13710457456892786\n",
      "Classifier 37/50: error = 0.4295320367913722, alpha = 0.14188034124394025\n",
      "Classifier 38/50: error = 0.4394606232454844, alpha = 0.12167568711352178\n",
      "Classifier 39/50: error = 0.4143918849791056, alpha = 0.17291935596201244\n",
      "Classifier 40/50: error = 0.4245719747816705, alpha = 0.15201630610909725\n",
      "Classifier 41/50: error = 0.44874943310082194, alpha = 0.10286238929550431\n",
      "Classifier 42/50: error = 0.4178481105428159, alpha = 0.16580669874083145\n",
      "Classifier 43/50: error = 0.42285618729139895, alpha = 0.15552966952106295\n",
      "Classifier 44/50: error = 0.46039091766672, alpha = 0.07938450307965103\n",
      "Classifier 45/50: error = 0.4270436150880408, alpha = 0.14696172280012632\n",
      "Classifier 46/50: error = 0.4367818510068691, alpha = 0.12711657768659462\n",
      "Classifier 47/50: error = 0.44835300223551333, alpha = 0.10366373565188518\n",
      "Classifier 48/50: error = 0.4346140565113008, alpha = 0.13152508642871213\n",
      "Classifier 49/50: error = 0.4412888395716036, alpha = 0.11796650293036215\n",
      "Classifier 50/50: error = 0.466362248130896, alpha = 0.06737727640350247\n",
      "Accuracy for digit 6: 0.9506\n",
      "Running AdaBoost for digit: 7\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Classifier 1/50: error = 0.34240561896400346, alpha = 0.3262961221267729\n",
      "Classifier 2/50: error = 0.30196299028705165, alpha = 0.4189838312667094\n",
      "Classifier 3/50: error = 0.35317228241290577, alpha = 0.30256199501638437\n",
      "Classifier 4/50: error = 0.2960536820646279, alpha = 0.4330806519575683\n",
      "Classifier 5/50: error = 0.3603167893410771, alpha = 0.28699472772365325\n",
      "Classifier 6/50: error = 0.28551678162077787, alpha = 0.45862935126737336\n",
      "Classifier 7/50: error = 0.3279815949146988, alpha = 0.35866411738349524\n",
      "Classifier 8/50: error = 0.34587784078618844, alpha = 0.31860423513084013\n",
      "Classifier 9/50: error = 0.37028428095908805, alpha = 0.26549872013739206\n",
      "Classifier 10/50: error = 0.34524069607242047, alpha = 0.32001292474753845\n",
      "Classifier 11/50: error = 0.36485965065614534, alpha = 0.27716661802849957\n",
      "Classifier 12/50: error = 0.34664775371689927, alpha = 0.31690363300068036\n",
      "Classifier 13/50: error = 0.3361844130656228, alpha = 0.34017226121212346\n",
      "Classifier 14/50: error = 0.306925056979231, alpha = 0.40726726616806347\n",
      "Classifier 15/50: error = 0.3679725956434654, alpha = 0.2704621436904991\n",
      "Classifier 16/50: error = 0.41511460978325987, alpha = 0.17143063177655066\n",
      "Classifier 17/50: error = 0.4085726206828833, alpha = 0.1849346147646595\n",
      "Classifier 18/50: error = 0.39547232189147685, alpha = 0.21218332675785426\n",
      "Classifier 19/50: error = 0.41265354454488334, alpha = 0.17650324682828453\n",
      "Classifier 20/50: error = 0.416251588568217, alpha = 0.16909011496349804\n",
      "Classifier 21/50: error = 0.4099239787342975, alpha = 0.1821398266713194\n",
      "Classifier 22/50: error = 0.428802406216799, alpha = 0.14336948857001788\n",
      "Classifier 23/50: error = 0.40006778223751704, alpha = 0.20259134500478973\n",
      "Classifier 24/50: error = 0.4451765665198195, alpha = 0.11008947152264585\n",
      "Classifier 25/50: error = 0.43243046965338183, alpha = 0.13597085629460612\n",
      "Classifier 26/50: error = 0.4248916886625411, alpha = 0.15136205215077309\n",
      "Classifier 27/50: error = 0.41743635493106357, alpha = 0.16665317596643114\n",
      "Classifier 28/50: error = 0.4089519280547378, alpha = 0.1841498701076255\n",
      "Classifier 29/50: error = 0.4398577563067758, alpha = 0.12086968224823412\n",
      "Classifier 30/50: error = 0.4405549287105478, alpha = 0.11945510718237336\n",
      "Classifier 31/50: error = 0.4305767928529561, alpha = 0.13974912129072692\n",
      "Classifier 32/50: error = 0.4054529526556349, alpha = 0.19139750340510017\n",
      "Classifier 33/50: error = 0.44361600980314386, alpha = 0.11324967011303792\n",
      "Classifier 34/50: error = 0.4253002739871705, alpha = 0.15052612111198074\n",
      "Classifier 35/50: error = 0.4288033815996144, alpha = 0.14336749743184757\n",
      "Classifier 36/50: error = 0.44098798344782963, alpha = 0.11857667136685254\n",
      "Classifier 37/50: error = 0.422284098857455, alpha = 0.15670195984590723\n",
      "Classifier 38/50: error = 0.43648651654098847, alpha = 0.12771688816742907\n",
      "Classifier 39/50: error = 0.4624563620746507, alpha = 0.07522887164200001\n",
      "Classifier 40/50: error = 0.4596810297108054, alpha = 0.08081340778595927\n",
      "Classifier 41/50: error = 0.43730807828973073, alpha = 0.12604716931519125\n",
      "Classifier 42/50: error = 0.4412385744442142, alpha = 0.11806843989580885\n",
      "Classifier 43/50: error = 0.44991001403990216, alpha = 0.1005171408360325\n",
      "Classifier 44/50: error = 0.45639944054758996, alpha = 0.0874231596050533\n",
      "Classifier 45/50: error = 0.4551527849506053, alpha = 0.08993613113988748\n",
      "Classifier 46/50: error = 0.46667256685359093, alpha = 0.06675384325414772\n",
      "Classifier 47/50: error = 0.4609501527052161, alpha = 0.078259069569804\n",
      "Classifier 48/50: error = 0.4637717620798659, alpha = 0.07258367414204756\n",
      "Classifier 49/50: error = 0.44156976480054677, alpha = 0.11739683541846396\n",
      "Classifier 50/50: error = 0.46023066675885616, alpha = 0.07970703716375352\n",
      "Accuracy for digit 7: 0.9388\n",
      "Running AdaBoost for digit: 8\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Classifier 1/50: error = 0.32236560977694206, alpha = 0.37146077967820823\n",
      "Classifier 2/50: error = 0.36427857147463616, alpha = 0.278420790309496\n",
      "Classifier 3/50: error = 0.33714133506071653, alpha = 0.3380297789787379\n",
      "Classifier 4/50: error = 0.29333878081866566, alpha = 0.4396115902293786\n",
      "Classifier 5/50: error = 0.3491476575772167, alpha = 0.3113939397271359\n",
      "Classifier 6/50: error = 0.40789617638166953, alpha = 0.18633466239801447\n",
      "Classifier 7/50: error = 0.3874941012123257, alpha = 0.22892897347589522\n",
      "Classifier 8/50: error = 0.3644153890223161, alpha = 0.2781254134143783\n",
      "Classifier 9/50: error = 0.41172401957005866, alpha = 0.17842146135119757\n",
      "Classifier 10/50: error = 0.4041965410665269, alpha = 0.19400479912890936\n",
      "Classifier 11/50: error = 0.39759360596253224, alpha = 0.20775094877296246\n",
      "Classifier 12/50: error = 0.42892760652580475, alpha = 0.14311391494902923\n",
      "Classifier 13/50: error = 0.4331772100738511, alpha = 0.1344499047956306\n",
      "Classifier 14/50: error = 0.43432919269828746, alpha = 0.13210477065339865\n",
      "Classifier 15/50: error = 0.4211823176914187, alpha = 0.15896087279137358\n",
      "Classifier 16/50: error = 0.4344143387093056, alpha = 0.1319314933646724\n",
      "Classifier 17/50: error = 0.4481043032990294, alpha = 0.10416652397420878\n",
      "Classifier 18/50: error = 0.44441195950399204, alpha = 0.11163755811991391\n",
      "Classifier 19/50: error = 0.43595246614745314, alpha = 0.12880265648348213\n",
      "Classifier 20/50: error = 0.46210187780900913, alpha = 0.075941898366177\n",
      "Classifier 21/50: error = 0.4449547703396388, alpha = 0.11053848397524588\n",
      "Classifier 22/50: error = 0.44838496756860496, alpha = 0.10359911594270373\n",
      "Classifier 23/50: error = 0.4442889102872532, alpha = 0.11188674333562079\n",
      "Classifier 24/50: error = 0.436495801068292, alpha = 0.12769801461654992\n",
      "Classifier 25/50: error = 0.42851150840340624, alpha = 0.1439633753451074\n",
      "Classifier 26/50: error = 0.45358702965022957, alpha = 0.09309394400360241\n",
      "Classifier 27/50: error = 0.4496956825328635, alpha = 0.10095016834821649\n",
      "Classifier 28/50: error = 0.4347092541633941, alpha = 0.13133138337364486\n",
      "Classifier 29/50: error = 0.4360778990686929, alpha = 0.1285476140060134\n",
      "Classifier 30/50: error = 0.4573304813711318, alpha = 0.08554711488772972\n",
      "Classifier 31/50: error = 0.4646944892789666, alpha = 0.0707287273007594\n",
      "Classifier 32/50: error = 0.43968688943918166, alpha = 0.1212164473728548\n",
      "Classifier 33/50: error = 0.43318391653003896, alpha = 0.13443624798145393\n",
      "Classifier 34/50: error = 0.4623315453781091, alpha = 0.07547992521097341\n",
      "Classifier 35/50: error = 0.4499656553633117, alpha = 0.10040473129679645\n",
      "Classifier 36/50: error = 0.4558448282209613, alpha = 0.08854099288677868\n",
      "Classifier 37/50: error = 0.4756312780709438, alpha = 0.04877608824728113\n",
      "Classifier 38/50: error = 0.4584748170354466, alpha = 0.083242102875917\n",
      "Classifier 39/50: error = 0.45075146048313197, alpha = 0.09881747498774701\n",
      "Classifier 40/50: error = 0.4632156311615792, alpha = 0.07370189716500297\n",
      "Classifier 41/50: error = 0.44651921076650614, alpha = 0.10737230954029249\n",
      "Classifier 42/50: error = 0.4606939884741146, alpha = 0.07877456298778152\n",
      "Classifier 43/50: error = 0.4406916661657274, alpha = 0.11917772045970579\n",
      "Classifier 44/50: error = 0.4661339034641504, alpha = 0.06783605629928027\n",
      "Classifier 45/50: error = 0.461281505218693, alpha = 0.07759233214670752\n",
      "Classifier 46/50: error = 0.4566067160808742, alpha = 0.08700544731554496\n",
      "Classifier 47/50: error = 0.4634922858396948, alpha = 0.07314559951392008\n",
      "Classifier 48/50: error = 0.4643017761567374, alpha = 0.07151813340125546\n",
      "Classifier 49/50: error = 0.4702956840812311, alpha = 0.05947867219388207\n",
      "Classifier 50/50: error = 0.4506476503189435, alpha = 0.09902713364672433\n",
      "Accuracy for digit 8: 0.8799\n",
      "Running AdaBoost for digit: 9\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Classifier 1/50: error = 0.32820642124726856, alpha = 0.35815418759820056\n",
      "Classifier 2/50: error = 0.3654313721792137, alpha = 0.27593347215957115\n",
      "Classifier 3/50: error = 0.33643415891556316, alpha = 0.3396128080934544\n",
      "Classifier 4/50: error = 0.3865243907709722, alpha = 0.2309727668447036\n",
      "Classifier 5/50: error = 0.39229132281898, alpha = 0.21884544172055984\n",
      "Classifier 6/50: error = 0.3842474284093127, alpha = 0.23577926167329094\n",
      "Classifier 7/50: error = 0.3828131944600608, alpha = 0.2388123074786735\n",
      "Classifier 8/50: error = 0.37070786866658145, alpha = 0.2645906250320867\n",
      "Classifier 9/50: error = 0.3357135873474161, alpha = 0.3412275114220886\n",
      "Classifier 10/50: error = 0.41466907544422726, alpha = 0.17234828833065247\n",
      "Classifier 11/50: error = 0.38753639870621054, alpha = 0.22883986878603102\n",
      "Classifier 12/50: error = 0.429630147452877, alpha = 0.141680149049985\n",
      "Classifier 13/50: error = 0.41897214866616617, alpha = 0.1634971228893217\n",
      "Classifier 14/50: error = 0.38476522965402243, alpha = 0.23468529004143937\n",
      "Classifier 15/50: error = 0.4308718047807053, alpha = 0.13914754934605217\n",
      "Classifier 16/50: error = 0.4276647575517102, alpha = 0.14569264796122522\n",
      "Classifier 17/50: error = 0.4371160506458436, alpha = 0.1264373779243741\n",
      "Classifier 18/50: error = 0.4554463242471612, alpha = 0.08934432248728566\n",
      "Classifier 19/50: error = 0.44491124051984154, alpha = 0.11062661257028356\n",
      "Classifier 20/50: error = 0.42345488854304225, alpha = 0.1543032996509691\n",
      "Classifier 21/50: error = 0.44253504962567647, alpha = 0.1154399812614028\n",
      "Classifier 22/50: error = 0.4303604034081636, alpha = 0.1401904345071294\n",
      "Classifier 23/50: error = 0.41714621200277113, alpha = 0.16724978686199846\n",
      "Classifier 24/50: error = 0.4200954300469775, alpha = 0.16119082775405888\n",
      "Classifier 25/50: error = 0.4212528079045704, alpha = 0.15881630317996112\n",
      "Classifier 26/50: error = 0.4528617793113917, alpha = 0.0945572515587527\n",
      "Classifier 27/50: error = 0.45241008130905513, alpha = 0.09546882758236155\n",
      "Classifier 28/50: error = 0.4334124535860615, alpha = 0.13397089214237765\n",
      "Classifier 29/50: error = 0.4624433914010365, alpha = 0.07525496012952108\n",
      "Classifier 30/50: error = 0.4497415070780342, alpha = 0.10085758295294228\n",
      "Classifier 31/50: error = 0.44497631803893445, alpha = 0.1104948600657819\n",
      "Classifier 32/50: error = 0.45298155564707043, alpha = 0.09431555614094109\n",
      "Classifier 33/50: error = 0.4654469785017112, alpha = 0.06921636792633742\n",
      "Classifier 34/50: error = 0.4304900867403048, alpha = 0.13992594667283947\n",
      "Classifier 35/50: error = 0.46382174250450925, alpha = 0.07248318646589774\n",
      "Classifier 36/50: error = 0.4350417502893933, alpha = 0.13065491494293233\n",
      "Classifier 37/50: error = 0.4313340229391822, alpha = 0.1382052207433838\n",
      "Classifier 38/50: error = 0.47012277825841947, alpha = 0.059825715833855236\n",
      "Classifier 39/50: error = 0.44296984502467285, alpha = 0.11455883940268807\n",
      "Classifier 40/50: error = 0.45036310626929854, alpha = 0.0996018533660176\n",
      "Classifier 41/50: error = 0.44519214287857145, alpha = 0.11005793982504539\n",
      "Classifier 42/50: error = 0.4471558991843653, alpha = 0.1060843724165281\n",
      "Classifier 43/50: error = 0.4543938701927501, alpha = 0.09146648199503751\n",
      "Classifier 44/50: error = 0.4441362266130048, alpha = 0.1121959601107754\n",
      "Classifier 45/50: error = 0.43652240074670845, alpha = 0.12764394339494872\n",
      "Classifier 46/50: error = 0.45529103942907234, alpha = 0.08965738656365652\n",
      "Classifier 47/50: error = 0.4678285503824532, alpha = 0.06443191382690575\n",
      "Classifier 48/50: error = 0.46268669743132773, alpha = 0.07476560497709536\n",
      "Classifier 49/50: error = 0.4563503068138476, alpha = 0.08752218088263004\n",
      "Classifier 50/50: error = 0.454391258059264, alpha = 0.09147175009339216\n",
      "Accuracy for digit 9: 0.8661\n",
      "Accuracies for all digits: {0: 0.9609, 1: 0.9639, 2: 0.91, 3: 0.8995, 4: 0.9025, 5: 0.8737, 6: 0.9506, 7: 0.9388, 8: 0.8799, 9: 0.8661}\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=50, A=50, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Classifier 1/50: error = 0.22608695652173919, alpha = 0.6152699156987816\n",
      "Classifier 2/50: error = 0.30183308020289334, alpha = 0.4192920320417054\n",
      "Classifier 3/50: error = 0.19549038530040463, alpha = 0.7073608594780536\n",
      "Classifier 4/50: error = 0.3261160058515066, alpha = 0.36290240809753027\n",
      "Classifier 5/50: error = 0.2700381241115829, alpha = 0.49721457914557854\n",
      "Classifier 6/50: error = 0.2547486560044247, alpha = 0.5367220696840952\n",
      "Classifier 7/50: error = 0.2868659165721218, alpha = 0.4553272704379755\n",
      "Classifier 8/50: error = 0.4242603001388516, alpha = 0.15265423312185747\n",
      "Classifier 9/50: error = 0.373951667640087, alpha = 0.2576505090500228\n",
      "Classifier 10/50: error = 0.35361074931667325, alpha = 0.3016025737378443\n",
      "Classifier 11/50: error = 0.42265260336412724, alpha = 0.15594679309277928\n",
      "Classifier 12/50: error = 0.36754733682857565, alpha = 0.27137662939830576\n",
      "Classifier 13/50: error = 0.36137649351483647, alpha = 0.28469737784585025\n",
      "Classifier 14/50: error = 0.42860835895581856, alpha = 0.1437656374739474\n",
      "Classifier 15/50: error = 0.3925388709276737, alpha = 0.2183263107408885\n",
      "Classifier 16/50: error = 0.4567766397203925, alpha = 0.08666303107483597\n",
      "Classifier 17/50: error = 0.4313101050037607, alpha = 0.1382539764707683\n",
      "Classifier 18/50: error = 0.32153813232957584, alpha = 0.37335606620136547\n",
      "Classifier 19/50: error = 0.42787271337223265, alpha = 0.14526787147371775\n",
      "Classifier 20/50: error = 0.4055675568204541, alpha = 0.1911598061327171\n",
      "Classifier 21/50: error = 0.41609141563848073, alpha = 0.16941972581401477\n",
      "Classifier 22/50: error = 0.37685115854756446, alpha = 0.25146754820404627\n",
      "Classifier 23/50: error = 0.41287705613763614, alpha = 0.17604218923886636\n",
      "Classifier 24/50: error = 0.4223458514397019, alpha = 0.15657539953402105\n",
      "Classifier 25/50: error = 0.43537387377158443, alpha = 0.12997932334336545\n",
      "Classifier 26/50: error = 0.4603280529747346, alpha = 0.0795110277321444\n",
      "Classifier 27/50: error = 0.3959051472470714, alpha = 0.2112782855451909\n",
      "Classifier 28/50: error = 0.3967905694874496, alpha = 0.2094279209542605\n",
      "Classifier 29/50: error = 0.44684095571443655, alpha = 0.10672141764320749\n",
      "Classifier 30/50: error = 0.40891583642146173, alpha = 0.18422453000355665\n",
      "Classifier 31/50: error = 0.4481016375620396, alpha = 0.1041719135107789\n",
      "Classifier 32/50: error = 0.43199638834108367, alpha = 0.13685527465617034\n",
      "Classifier 33/50: error = 0.4382305349306086, alpha = 0.12417322690761576\n",
      "Classifier 34/50: error = 0.44242438002192574, alpha = 0.11566428903353579\n",
      "Classifier 35/50: error = 0.43323941704619795, alpha = 0.13432323040744948\n",
      "Classifier 36/50: error = 0.4313427655757741, alpha = 0.138187399401315\n",
      "Classifier 37/50: error = 0.4482700549599805, alpha = 0.103831422235004\n",
      "Classifier 38/50: error = 0.4387185101714399, alpha = 0.12318227175810337\n",
      "Classifier 39/50: error = 0.44671137510779724, alpha = 0.10698354909733183\n",
      "Classifier 40/50: error = 0.4593675807267351, alpha = 0.0814444410163825\n",
      "Classifier 41/50: error = 0.4747147628778111, alpha = 0.05061364968927417\n",
      "Classifier 42/50: error = 0.45967030187356417, alpha = 0.08083500392622027\n",
      "Classifier 43/50: error = 0.4526926741176749, alpha = 0.09489850592521863\n",
      "Classifier 44/50: error = 0.4284748319788251, alpha = 0.14403825979905088\n",
      "Classifier 45/50: error = 0.443928693637038, alpha = 0.11261629260906238\n",
      "Classifier 46/50: error = 0.4363188665831295, alpha = 0.12805770193193713\n",
      "Classifier 47/50: error = 0.42586377036624434, alpha = 0.14937359735137887\n",
      "Classifier 48/50: error = 0.4420562022549184, alpha = 0.11641060397876665\n",
      "Classifier 49/50: error = 0.4605757807361207, alpha = 0.07901245301411003\n",
      "Classifier 50/50: error = 0.4514954881314496, alpha = 0.09731506286548193\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2192390417562855, alpha = 0.6350531909275953\n",
      "Classifier 2/50: error = 0.3470614676054282, alpha = 0.31599054464127524\n",
      "Classifier 3/50: error = 0.27938422499433296, alpha = 0.4737590522028901\n",
      "Classifier 4/50: error = 0.33566929574793947, alpha = 0.3413268187398625\n",
      "Classifier 5/50: error = 0.3509463836795672, alpha = 0.3074409340787567\n",
      "Classifier 6/50: error = 0.30166043507729223, alpha = 0.4197017346804545\n",
      "Classifier 7/50: error = 0.34396383792653684, alpha = 0.32283969142868113\n",
      "Classifier 8/50: error = 0.3846999282910506, alpha = 0.23482322341805634\n",
      "Classifier 9/50: error = 0.3341650803083045, alpha = 0.344703324020913\n",
      "Classifier 10/50: error = 0.33023731169969284, alpha = 0.3535559652733073\n",
      "Classifier 11/50: error = 0.4278613032431674, alpha = 0.14529117677753453\n",
      "Classifier 12/50: error = 0.3632078403891845, alpha = 0.2807340443491876\n",
      "Classifier 13/50: error = 0.2805554729068531, alpha = 0.4708539770544076\n",
      "Classifier 14/50: error = 0.299932460801525, alpha = 0.4238097480594233\n",
      "Classifier 15/50: error = 0.4149924636058233, alpha = 0.17168218486204856\n",
      "Classifier 16/50: error = 0.3389015949718862, alpha = 0.334096458333853\n",
      "Classifier 17/50: error = 0.3620374530225493, alpha = 0.28326595492141066\n",
      "Classifier 18/50: error = 0.42105340614181785, alpha = 0.1592252767932102\n",
      "Classifier 19/50: error = 0.416042728430412, alpha = 0.16951992370102603\n",
      "Classifier 20/50: error = 0.37031049853730347, alpha = 0.26544250200260183\n",
      "Classifier 21/50: error = 0.37679492699949124, alpha = 0.2515872777143803\n",
      "Classifier 22/50: error = 0.4095691370879062, alpha = 0.1828734122226802\n",
      "Classifier 23/50: error = 0.4329831050330879, alpha = 0.13484519575354484\n",
      "Classifier 24/50: error = 0.414153182276614, alpha = 0.17341122062141803\n",
      "Classifier 25/50: error = 0.4147835888670979, alpha = 0.172112400353222\n",
      "Classifier 26/50: error = 0.3894449893238079, alpha = 0.22482288786246118\n",
      "Classifier 27/50: error = 0.4383723346706224, alpha = 0.12388524232707104\n",
      "Classifier 28/50: error = 0.4225543407743445, alpha = 0.156148142791015\n",
      "Classifier 29/50: error = 0.44318712614345157, alpha = 0.11411857117818067\n",
      "Classifier 30/50: error = 0.40465344371223466, alpha = 0.1930563385994554\n",
      "Classifier 31/50: error = 0.4284272563473611, alpha = 0.144135400207377\n",
      "Classifier 32/50: error = 0.446881073325756, alpha = 0.10664026581000642\n",
      "Classifier 33/50: error = 0.45270200143826567, alpha = 0.09487968281404403\n",
      "Classifier 34/50: error = 0.4481891514146594, alpha = 0.1039949828091696\n",
      "Classifier 35/50: error = 0.43426043735220166, alpha = 0.13224469770194844\n",
      "Classifier 36/50: error = 0.447790120444432, alpha = 0.10480177453222365\n",
      "Classifier 37/50: error = 0.42121134781935443, alpha = 0.15890133359637987\n",
      "Classifier 38/50: error = 0.43278051357534775, alpha = 0.13525781380727034\n",
      "Classifier 39/50: error = 0.40039838432244723, alpha = 0.20190272422997677\n",
      "Classifier 40/50: error = 0.4676409908806777, alpha = 0.06480860142989425\n",
      "Classifier 41/50: error = 0.45082742590255565, alpha = 0.09866405804230445\n",
      "Classifier 42/50: error = 0.44248020472492766, alpha = 0.11555114075735191\n",
      "Classifier 43/50: error = 0.45327126817210006, alpha = 0.09373099362709704\n",
      "Classifier 44/50: error = 0.44579760564829246, alpha = 0.10883245136158852\n",
      "Classifier 45/50: error = 0.43153164483355444, alpha = 0.13780240160278023\n",
      "Classifier 46/50: error = 0.4419744981985883, alpha = 0.11657623970654976\n",
      "Classifier 47/50: error = 0.43864371721389717, alpha = 0.12333414180245998\n",
      "Classifier 48/50: error = 0.4482070417752742, alpha = 0.10395881385952407\n",
      "Classifier 49/50: error = 0.44679977655812725, alpha = 0.10680471827710553\n",
      "Classifier 50/50: error = 0.41699933236254433, alpha = 0.167551855354414\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Classifier 1/50: error = 0.33383685800604235, alpha = 0.3454410868898294\n",
      "Classifier 2/50: error = 0.32019531459383876, alpha = 0.37643718084831046\n",
      "Classifier 3/50: error = 0.34566321670493594, alpha = 0.3190786195163977\n",
      "Classifier 4/50: error = 0.3618903165737645, alpha = 0.2835845065105109\n",
      "Classifier 5/50: error = 0.28806905745944644, alpha = 0.4523903407248115\n",
      "Classifier 6/50: error = 0.3432307857527038, alpha = 0.3244648094467225\n",
      "Classifier 7/50: error = 0.41157330953297044, alpha = 0.17873259625244697\n",
      "Classifier 8/50: error = 0.3661172467159805, alpha = 0.27445518858559637\n",
      "Classifier 9/50: error = 0.3667703910594987, alpha = 0.2730485363900999\n",
      "Classifier 10/50: error = 0.3822265547989103, alpha = 0.2400541442118353\n",
      "Classifier 11/50: error = 0.39707726245206065, alpha = 0.20882909041432768\n",
      "Classifier 12/50: error = 0.368678852014735, alpha = 0.2689443692279753\n",
      "Classifier 13/50: error = 0.41835339793527215, alpha = 0.16476826267461073\n",
      "Classifier 14/50: error = 0.4079740654603008, alpha = 0.1861734174283983\n",
      "Classifier 15/50: error = 0.43929091327083863, alpha = 0.1220201713950878\n",
      "Classifier 16/50: error = 0.41501513233931075, alpha = 0.17163549827322858\n",
      "Classifier 17/50: error = 0.4343377910224782, alpha = 0.1320872721851196\n",
      "Classifier 18/50: error = 0.4048958882520204, alpha = 0.19255319993628223\n",
      "Classifier 19/50: error = 0.44071432974830316, alpha = 0.1191317466948008\n",
      "Classifier 20/50: error = 0.4126615625607466, alpha = 0.17648670605807082\n",
      "Classifier 21/50: error = 0.42117233916843344, alpha = 0.15898133845102375\n",
      "Classifier 22/50: error = 0.4080435940565479, alpha = 0.18602948830663046\n",
      "Classifier 23/50: error = 0.4248208418197198, alpha = 0.15150702013033368\n",
      "Classifier 24/50: error = 0.40256702969593217, alpha = 0.19739023872028116\n",
      "Classifier 25/50: error = 0.45552832843571817, alpha = 0.0891790038751011\n",
      "Classifier 26/50: error = 0.43772053045503756, alpha = 0.12520917730179298\n",
      "Classifier 27/50: error = 0.43814168108677953, alpha = 0.1243536928056627\n",
      "Classifier 28/50: error = 0.4161390427380349, alpha = 0.16932171288218512\n",
      "Classifier 29/50: error = 0.4465654661087973, alpha = 0.10727872914884294\n",
      "Classifier 30/50: error = 0.4642061093568144, alpha = 0.07171044995511439\n",
      "Classifier 31/50: error = 0.46133556947557597, alpha = 0.07748355224606854\n",
      "Classifier 32/50: error = 0.4636288482451163, alpha = 0.07287101629957299\n",
      "Classifier 33/50: error = 0.43956281049419266, alpha = 0.12146827710149749\n",
      "Classifier 34/50: error = 0.4645969689944297, alpha = 0.07092474791427222\n",
      "Classifier 35/50: error = 0.4549754216630706, alpha = 0.09029374615349542\n",
      "Classifier 36/50: error = 0.4499063110462197, alpha = 0.10052462190919709\n",
      "Classifier 37/50: error = 0.43835247680352274, alpha = 0.12392557092579201\n",
      "Classifier 38/50: error = 0.4548301509920443, alpha = 0.09058667044691557\n",
      "Classifier 39/50: error = 0.4706525492497009, alpha = 0.0587624442598437\n",
      "Classifier 40/50: error = 0.4751215566315732, alpha = 0.049798009664117696\n",
      "Classifier 41/50: error = 0.45168960049908846, alpha = 0.09692316481495124\n",
      "Classifier 42/50: error = 0.4467993089773761, alpha = 0.10680566414694638\n",
      "Classifier 43/50: error = 0.4650264683112241, alpha = 0.0700614735491476\n",
      "Classifier 44/50: error = 0.4567350974417159, alpha = 0.08674674180688124\n",
      "Classifier 45/50: error = 0.46297442416914625, alpha = 0.07418695370274397\n",
      "Classifier 46/50: error = 0.4553334315106643, alpha = 0.08957191969233425\n",
      "Classifier 47/50: error = 0.4421576959874609, alpha = 0.11620485819835306\n",
      "Classifier 48/50: error = 0.4691598553984575, alpha = 0.06175868825303161\n",
      "Classifier 49/50: error = 0.43974830296899126, alpha = 0.12109180857944425\n",
      "Classifier 50/50: error = 0.46505761329212636, alpha = 0.06999887760360265\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.28597063621533436, alpha = 0.4575174760300126\n",
      "Classifier 2/50: error = 0.3783112562343167, alpha = 0.2483611347282499\n",
      "Classifier 3/50: error = 0.4271793936100291, alpha = 0.14668426961582437\n",
      "Classifier 4/50: error = 0.31955731426496126, alpha = 0.3779034747618132\n",
      "Classifier 5/50: error = 0.395310279207019, alpha = 0.21252224733458072\n",
      "Classifier 6/50: error = 0.365023116135648, alpha = 0.2768139556473892\n",
      "Classifier 7/50: error = 0.35954430778468804, alpha = 0.28867126201410576\n",
      "Classifier 8/50: error = 0.3314736059568836, alpha = 0.3507638458409054\n",
      "Classifier 9/50: error = 0.4117406020698674, alpha = 0.17838722952564703\n",
      "Classifier 10/50: error = 0.46077470474309296, alpha = 0.07861212869019503\n",
      "Classifier 11/50: error = 0.4331424367777794, alpha = 0.1345207168297723\n",
      "Classifier 12/50: error = 0.4292016196292041, alpha = 0.14255463185945602\n",
      "Classifier 13/50: error = 0.4536762755491671, alpha = 0.09291390384286163\n",
      "Classifier 14/50: error = 0.4112223803638806, alpha = 0.17945720843761553\n",
      "Classifier 15/50: error = 0.39351722946552736, alpha = 0.21627573389862956\n",
      "Classifier 16/50: error = 0.4092298447622072, alpha = 0.18357503381381118\n",
      "Classifier 17/50: error = 0.41691344713541234, alpha = 0.16772849850419658\n",
      "Classifier 18/50: error = 0.38572902952936794, alpha = 0.23265051188503855\n",
      "Classifier 19/50: error = 0.40884364299173526, alpha = 0.18437387691422918\n",
      "Classifier 20/50: error = 0.4191724503973613, alpha = 0.1630857424164044\n",
      "Classifier 21/50: error = 0.4413067688783312, alpha = 0.11793014314116958\n",
      "Classifier 22/50: error = 0.42109792851404965, alpha = 0.15913395669885882\n",
      "Classifier 23/50: error = 0.3531343065594271, alpha = 0.3026451164289528\n",
      "Classifier 24/50: error = 0.44707369808105907, alpha = 0.10625063466210267\n",
      "Classifier 25/50: error = 0.40831607765996636, alpha = 0.18546550131636352\n",
      "Classifier 26/50: error = 0.4364944187046872, alpha = 0.12770082467415914\n",
      "Classifier 27/50: error = 0.44107625884219803, alpha = 0.11839763032391351\n",
      "Classifier 28/50: error = 0.43367152412769194, alpha = 0.1334434325933209\n",
      "Classifier 29/50: error = 0.44574985391079447, alpha = 0.10892909151361893\n",
      "Classifier 30/50: error = 0.44694178234360726, alpha = 0.10651746335008064\n",
      "Classifier 31/50: error = 0.45184393377994936, alpha = 0.09661159888705398\n",
      "Classifier 32/50: error = 0.45147903527387545, alpha = 0.09734828129705149\n",
      "Classifier 33/50: error = 0.46136095242793396, alpha = 0.07743248114927542\n",
      "Classifier 34/50: error = 0.449537962645139, alpha = 0.10126884391286767\n",
      "Classifier 35/50: error = 0.4470870217152776, alpha = 0.10622368551053374\n",
      "Classifier 36/50: error = 0.42098750783065375, alpha = 0.15936044603328378\n",
      "Classifier 37/50: error = 0.4583121844050676, alpha = 0.08356963611174038\n",
      "Classifier 38/50: error = 0.4157983454242118, alpha = 0.17002291296542127\n",
      "Classifier 39/50: error = 0.45919146966452834, alpha = 0.0817990149012621\n",
      "Classifier 40/50: error = 0.4566909748261434, alpha = 0.0868356534347161\n",
      "Classifier 41/50: error = 0.4654528892544876, alpha = 0.0692044897043165\n",
      "Classifier 42/50: error = 0.46949638863691034, alpha = 0.06108307939614091\n",
      "Classifier 43/50: error = 0.44705779168373183, alpha = 0.10628280806075678\n",
      "Classifier 44/50: error = 0.46708105437177305, alpha = 0.06593326700476987\n",
      "Classifier 45/50: error = 0.45750551350553736, alpha = 0.08519449308184818\n",
      "Classifier 46/50: error = 0.4588895891901709, alpha = 0.08240685490860755\n",
      "Classifier 47/50: error = 0.4550177419135548, alpha = 0.09020841435995428\n",
      "Classifier 48/50: error = 0.45334923036925506, alpha = 0.09357369765386897\n",
      "Classifier 49/50: error = 0.4705257140069392, alpha = 0.05901699549313057\n",
      "Classifier 50/50: error = 0.47103488027115087, alpha = 0.05799517318449233\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.33184969613968995, alpha = 0.3499155049589789\n",
      "Classifier 2/50: error = 0.3841036216920736, alpha = 0.2360831839555879\n",
      "Classifier 3/50: error = 0.3235685381479054, alpha = 0.36871008675711153\n",
      "Classifier 4/50: error = 0.37676339490007404, alpha = 0.25165441967983476\n",
      "Classifier 5/50: error = 0.3147749184126907, alpha = 0.3889447658407086\n",
      "Classifier 6/50: error = 0.36361141555458276, alpha = 0.2798618003780361\n",
      "Classifier 7/50: error = 0.3412132592896631, alpha = 0.3289460979578556\n",
      "Classifier 8/50: error = 0.3673345247182993, alpha = 0.27183443144759617\n",
      "Classifier 9/50: error = 0.3698834132654164, alpha = 0.266358501880104\n",
      "Classifier 10/50: error = 0.45308627027977266, alpha = 0.09410426257814877\n",
      "Classifier 11/50: error = 0.37531540477905434, alpha = 0.25474006136696664\n",
      "Classifier 12/50: error = 0.3865042108872416, alpha = 0.23101531871497857\n",
      "Classifier 13/50: error = 0.36825723455301285, alpha = 0.2698502967835561\n",
      "Classifier 14/50: error = 0.40697482508820276, alpha = 0.1882427613935199\n",
      "Classifier 15/50: error = 0.4250352954348872, alpha = 0.15106822096436953\n",
      "Classifier 16/50: error = 0.447887045886335, alpha = 0.10460579070613084\n",
      "Classifier 17/50: error = 0.44163501258286186, alpha = 0.11726453512900525\n",
      "Classifier 18/50: error = 0.4424390066622446, alpha = 0.115634642748714\n",
      "Classifier 19/50: error = 0.4209149200941604, alpha = 0.1595093431669013\n",
      "Classifier 20/50: error = 0.4043418650307729, alpha = 0.19370309133180685\n",
      "Classifier 21/50: error = 0.43985101446724617, alpha = 0.12088336390052347\n",
      "Classifier 22/50: error = 0.3886582849043734, alpha = 0.2264777810011415\n",
      "Classifier 23/50: error = 0.44231613689115007, alpha = 0.11588368998235854\n",
      "Classifier 24/50: error = 0.3676986317333117, alpha = 0.2710512308747027\n",
      "Classifier 25/50: error = 0.417066970206851, alpha = 0.16741274952685783\n",
      "Classifier 26/50: error = 0.44017897109427595, alpha = 0.12021787233138538\n",
      "Classifier 27/50: error = 0.3764403010975901, alpha = 0.25234251831234206\n",
      "Classifier 28/50: error = 0.42878595312178513, alpha = 0.1434030759481911\n",
      "Classifier 29/50: error = 0.4499551832274592, alpha = 0.1004258874652656\n",
      "Classifier 30/50: error = 0.4193657964701928, alpha = 0.1626886993943689\n",
      "Classifier 31/50: error = 0.3982906421117631, alpha = 0.2062962695483179\n",
      "Classifier 32/50: error = 0.4123194262007848, alpha = 0.17719260155572625\n",
      "Classifier 33/50: error = 0.46376530015742334, alpha = 0.07259666620666809\n",
      "Classifier 34/50: error = 0.45875831246384713, alpha = 0.08267120112392758\n",
      "Classifier 35/50: error = 0.4307015754670831, alpha = 0.1394946592726284\n",
      "Classifier 36/50: error = 0.44426593208826304, alpha = 0.11193327769137634\n",
      "Classifier 37/50: error = 0.44304317225910705, alpha = 0.1144102543633648\n",
      "Classifier 38/50: error = 0.4542448596158003, alpha = 0.09176701163653803\n",
      "Classifier 39/50: error = 0.43797045304060866, alpha = 0.12470148691626448\n",
      "Classifier 40/50: error = 0.46381526858520383, alpha = 0.07249620246149363\n",
      "Classifier 41/50: error = 0.4545352641335366, alpha = 0.09118132908107085\n",
      "Classifier 42/50: error = 0.4488740649937437, alpha = 0.10261048531580397\n",
      "Classifier 43/50: error = 0.46539437375836523, alpha = 0.06932208303988581\n",
      "Classifier 44/50: error = 0.4553807315102975, alpha = 0.08947655948519131\n",
      "Classifier 45/50: error = 0.42128014208942766, alpha = 0.15876024479368112\n",
      "Classifier 46/50: error = 0.44812742172925046, alpha = 0.10411978382259732\n",
      "Classifier 47/50: error = 0.44817293549217685, alpha = 0.10402776678077158\n",
      "Classifier 48/50: error = 0.44651249757062494, alpha = 0.10738589133790649\n",
      "Classifier 49/50: error = 0.4588982122472486, alpha = 0.08238949143743478\n",
      "Classifier 50/50: error = 0.4299846410194341, alpha = 0.14095690824907406\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3719900359811792, alpha = 0.26184448179357755\n",
      "Classifier 2/50: error = 0.3581220771562308, alpha = 0.29176210411722475\n",
      "Classifier 3/50: error = 0.3746619315677808, alpha = 0.25613415467038664\n",
      "Classifier 4/50: error = 0.3847080042939344, alpha = 0.23480616433474616\n",
      "Classifier 5/50: error = 0.4105388069954934, alpha = 0.18086921351722454\n",
      "Classifier 6/50: error = 0.4012672845058399, alpha = 0.20009376520453073\n",
      "Classifier 7/50: error = 0.36034251477516976, alpha = 0.2869389222890765\n",
      "Classifier 8/50: error = 0.40545677153930715, alpha = 0.19138958242195625\n",
      "Classifier 9/50: error = 0.3669905508589788, alpha = 0.27257462436317736\n",
      "Classifier 10/50: error = 0.4389778047129131, alpha = 0.12265580771279733\n",
      "Classifier 11/50: error = 0.4120361012716667, alpha = 0.17777728963205452\n",
      "Classifier 12/50: error = 0.3878033149542593, alpha = 0.22827766035247934\n",
      "Classifier 13/50: error = 0.4014249876633952, alpha = 0.1997655826271854\n",
      "Classifier 14/50: error = 0.45794247093537754, alpha = 0.08431428552272772\n",
      "Classifier 15/50: error = 0.44761950606053214, alpha = 0.10514677733960827\n",
      "Classifier 16/50: error = 0.40451816560331166, alpha = 0.19333711959235558\n",
      "Classifier 17/50: error = 0.42250038337988827, alpha = 0.15625871213304376\n",
      "Classifier 18/50: error = 0.41717275823926736, alpha = 0.16719519585473772\n",
      "Classifier 19/50: error = 0.4340780720016848, alpha = 0.13261586245481316\n",
      "Classifier 20/50: error = 0.43674824347361296, alpha = 0.12718488530874114\n",
      "Classifier 21/50: error = 0.4052910034333057, alpha = 0.19173343417089467\n",
      "Classifier 22/50: error = 0.44535930553082265, alpha = 0.1097195610832842\n",
      "Classifier 23/50: error = 0.44195988326174573, alpha = 0.116605868719413\n",
      "Classifier 24/50: error = 0.3990596315445448, alpha = 0.20469242538639357\n",
      "Classifier 25/50: error = 0.44585283043653534, alpha = 0.10872068974520518\n",
      "Classifier 26/50: error = 0.4448435434933409, alpha = 0.11076367245030687\n",
      "Classifier 27/50: error = 0.4283024209114631, alpha = 0.14439030332008165\n",
      "Classifier 28/50: error = 0.4564694442050514, alpha = 0.08728208124683348\n",
      "Classifier 29/50: error = 0.4628564513669835, alpha = 0.0744242044384472\n",
      "Classifier 30/50: error = 0.45976775408418696, alpha = 0.08063882626686922\n",
      "Classifier 31/50: error = 0.4559061889652064, alpha = 0.08841730815594465\n",
      "Classifier 32/50: error = 0.4429950361586056, alpha = 0.11450779333128419\n",
      "Classifier 33/50: error = 0.4396531969109979, alpha = 0.12128482796967831\n",
      "Classifier 34/50: error = 0.4371234222509536, alpha = 0.12642239779273645\n",
      "Classifier 35/50: error = 0.44918978478151506, alpha = 0.10197241554892796\n",
      "Classifier 36/50: error = 0.46517664560495153, alpha = 0.06975964858735413\n",
      "Classifier 37/50: error = 0.450359994120482, alpha = 0.09960813962036122\n",
      "Classifier 38/50: error = 0.4588317219914312, alpha = 0.08252337814316703\n",
      "Classifier 39/50: error = 0.43455521196522917, alpha = 0.13164482504865732\n",
      "Classifier 40/50: error = 0.4423662187918782, alpha = 0.11578217623486947\n",
      "Classifier 41/50: error = 0.452206195571107, alpha = 0.09588034305433404\n",
      "Classifier 42/50: error = 0.4486898724770082, alpha = 0.10298277686349049\n",
      "Classifier 43/50: error = 0.46201092298533364, alpha = 0.07612486167537157\n",
      "Classifier 44/50: error = 0.45438788836854943, alpha = 0.09147854602542944\n",
      "Classifier 45/50: error = 0.45746693071538946, alpha = 0.08527222060771474\n",
      "Classifier 46/50: error = 0.4428022246233616, alpha = 0.11489851226694207\n",
      "Classifier 47/50: error = 0.46349469410577315, alpha = 0.07314075716769554\n",
      "Classifier 48/50: error = 0.46800679981001, alpha = 0.06407394115791679\n",
      "Classifier 49/50: error = 0.46480435259359454, alpha = 0.07050790308301354\n",
      "Classifier 50/50: error = 0.4454082178668277, alpha = 0.10962055509813771\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3741019355929336, alpha = 0.25732960299168434\n",
      "Classifier 2/50: error = 0.2862893838533235, alpha = 0.4567372247708348\n",
      "Classifier 3/50: error = 0.2606145146362491, alpha = 0.5213885278196972\n",
      "Classifier 4/50: error = 0.3549236482566915, alpha = 0.298732996724495\n",
      "Classifier 5/50: error = 0.33466976917751434, alpha = 0.3435696114800678\n",
      "Classifier 6/50: error = 0.31376856986738977, alpha = 0.3912796284228622\n",
      "Classifier 7/50: error = 0.30772015846548617, alpha = 0.4053997377077101\n",
      "Classifier 8/50: error = 0.3096108312364458, alpha = 0.4009696627005589\n",
      "Classifier 9/50: error = 0.39775514005259704, alpha = 0.20741375833370432\n",
      "Classifier 10/50: error = 0.39323700458284505, alpha = 0.21686288358411085\n",
      "Classifier 11/50: error = 0.329809581213789, alpha = 0.3545232088518526\n",
      "Classifier 12/50: error = 0.3625031462414618, alpha = 0.2822580940384131\n",
      "Classifier 13/50: error = 0.3965070987476029, alpha = 0.21002016665588003\n",
      "Classifier 14/50: error = 0.39615086990707676, alpha = 0.21076462936007556\n",
      "Classifier 15/50: error = 0.3149130397326253, alpha = 0.38862462147835775\n",
      "Classifier 16/50: error = 0.3762804578726193, alpha = 0.25268302578408836\n",
      "Classifier 17/50: error = 0.3881603262935815, alpha = 0.2275259053291295\n",
      "Classifier 18/50: error = 0.36052138476537143, alpha = 0.2865509529074287\n",
      "Classifier 19/50: error = 0.4269804466021342, alpha = 0.14709081049858294\n",
      "Classifier 20/50: error = 0.3956112522068711, alpha = 0.21189278512353135\n",
      "Classifier 21/50: error = 0.4438994942378205, alpha = 0.11267543557478221\n",
      "Classifier 22/50: error = 0.42634970727604493, alpha = 0.14838002293075458\n",
      "Classifier 23/50: error = 0.43853991977230566, alpha = 0.12354491597429736\n",
      "Classifier 24/50: error = 0.47140274601225496, alpha = 0.05725699577181903\n",
      "Classifier 25/50: error = 0.4464028369836037, alpha = 0.10760775667046349\n",
      "Classifier 26/50: error = 0.4132382809488057, alpha = 0.175297214620042\n",
      "Classifier 27/50: error = 0.4313719219815896, alpha = 0.13812796644140768\n",
      "Classifier 28/50: error = 0.4432120735042171, alpha = 0.11406802413995092\n",
      "Classifier 29/50: error = 0.44356499106957764, alpha = 0.11335302306975763\n",
      "Classifier 30/50: error = 0.4208407333103452, alpha = 0.15966152762746666\n",
      "Classifier 31/50: error = 0.4592204123148038, alpha = 0.08174074169807793\n",
      "Classifier 32/50: error = 0.4506882602965957, alpha = 0.09894511527514181\n",
      "Classifier 33/50: error = 0.4550357871132857, alpha = 0.09017202959436563\n",
      "Classifier 34/50: error = 0.43839599161626286, alpha = 0.12383719884529466\n",
      "Classifier 35/50: error = 0.4363705392716388, alpha = 0.12795265393828453\n",
      "Classifier 36/50: error = 0.4262829149965294, alpha = 0.14851657296677667\n",
      "Classifier 37/50: error = 0.4463374175829023, alpha = 0.10774011824603347\n",
      "Classifier 38/50: error = 0.42260844050836144, alpha = 0.15603728556929475\n",
      "Classifier 39/50: error = 0.45444548215343683, alpha = 0.0913623930671526\n",
      "Classifier 40/50: error = 0.411649760725016, alpha = 0.17857476151955556\n",
      "Classifier 41/50: error = 0.4175350450971965, alpha = 0.1664502695922427\n",
      "Classifier 42/50: error = 0.42837928624589794, alpha = 0.1442333487838499\n",
      "Classifier 43/50: error = 0.4661661743591723, alpha = 0.06777121733412918\n",
      "Classifier 44/50: error = 0.456101278267253, alpha = 0.08802408494299012\n",
      "Classifier 45/50: error = 0.4529615580019986, alpha = 0.09435590841546333\n",
      "Classifier 46/50: error = 0.44308503010475886, alpha = 0.11432543888788096\n",
      "Classifier 47/50: error = 0.446045076069235, alpha = 0.10833165215725175\n",
      "Classifier 48/50: error = 0.40287736140912805, alpha = 0.19674515773896636\n",
      "Classifier 49/50: error = 0.47543030112863194, alpha = 0.04917900711551212\n",
      "Classifier 50/50: error = 0.4445477771110493, alpha = 0.11136253184660572\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3693032165376327, alpha = 0.26760359097605557\n",
      "Classifier 2/50: error = 0.3141102369354646, alpha = 0.3904864610299214\n",
      "Classifier 3/50: error = 0.3493528863785719, alpha = 0.31094243991494086\n",
      "Classifier 4/50: error = 0.2861963603551331, alpha = 0.45696487995013524\n",
      "Classifier 5/50: error = 0.3490308904306242, alpha = 0.3116508803317504\n",
      "Classifier 6/50: error = 0.3154949212107937, alpha = 0.387276739986836\n",
      "Classifier 7/50: error = 0.3573952489324439, alpha = 0.29334376540905094\n",
      "Classifier 8/50: error = 0.3231792764719501, alpha = 0.3695996113836597\n",
      "Classifier 9/50: error = 0.3810544029454064, alpha = 0.24253761273173666\n",
      "Classifier 10/50: error = 0.3663375675337184, alpha = 0.27398057381231905\n",
      "Classifier 11/50: error = 0.3220183487460543, alpha = 0.37225584837978204\n",
      "Classifier 12/50: error = 0.3553737129612754, alpha = 0.2977504002417276\n",
      "Classifier 13/50: error = 0.36100035718158346, alpha = 0.2855124737660954\n",
      "Classifier 14/50: error = 0.3604109331108676, alpha = 0.2867905128517847\n",
      "Classifier 15/50: error = 0.42058343684456445, alpha = 0.16018939446639524\n",
      "Classifier 16/50: error = 0.3961623676410681, alpha = 0.21074059729815328\n",
      "Classifier 17/50: error = 0.45082701965417704, alpha = 0.09866487847415374\n",
      "Classifier 18/50: error = 0.4498147450601435, alpha = 0.10070961413903325\n",
      "Classifier 19/50: error = 0.35844646522107504, alpha = 0.2910566569946513\n",
      "Classifier 20/50: error = 0.3972170760191974, alpha = 0.20853710809899784\n",
      "Classifier 21/50: error = 0.3917367522146533, alpha = 0.22000884888696748\n",
      "Classifier 22/50: error = 0.4102034054486443, alpha = 0.18156228742988031\n",
      "Classifier 23/50: error = 0.3849559684599049, alpha = 0.23428245051796226\n",
      "Classifier 24/50: error = 0.4154952670182751, alpha = 0.17064682755909322\n",
      "Classifier 25/50: error = 0.4284220613745691, alpha = 0.14414600751924372\n",
      "Classifier 26/50: error = 0.414937379136265, alpha = 0.171795635223458\n",
      "Classifier 27/50: error = 0.4781538986858843, alpha = 0.04372003743834402\n",
      "Classifier 28/50: error = 0.4262228592039349, alpha = 0.14863935563334857\n",
      "Classifier 29/50: error = 0.4091478587418771, alpha = 0.1837445992520882\n",
      "Classifier 30/50: error = 0.44919598183095943, alpha = 0.10195989214018027\n",
      "Classifier 31/50: error = 0.4388689267998055, alpha = 0.12287686198373511\n",
      "Classifier 32/50: error = 0.46070827460663666, alpha = 0.07874581311741981\n",
      "Classifier 33/50: error = 0.43588689809270353, alpha = 0.12893598249266564\n",
      "Classifier 34/50: error = 0.43291257990463805, alpha = 0.1349888291082273\n",
      "Classifier 35/50: error = 0.4081861386133161, alpha = 0.18573443478767054\n",
      "Classifier 36/50: error = 0.432539309914749, alpha = 0.1357491330555883\n",
      "Classifier 37/50: error = 0.41738103409275423, alpha = 0.16676692121583694\n",
      "Classifier 38/50: error = 0.4159108952336267, alpha = 0.16979125230331413\n",
      "Classifier 39/50: error = 0.4621160397196107, alpha = 0.07591341094387583\n",
      "Classifier 40/50: error = 0.4238293621588074, alpha = 0.15353646826192505\n",
      "Classifier 41/50: error = 0.4417962263648867, alpha = 0.11693766597371871\n",
      "Classifier 42/50: error = 0.4348885149601265, alpha = 0.13096665974493632\n",
      "Classifier 43/50: error = 0.44308758457031766, alpha = 0.11432026289308898\n",
      "Classifier 44/50: error = 0.4506214931469915, alpha = 0.09907996295925359\n",
      "Classifier 45/50: error = 0.46072823170955157, alpha = 0.07870565102220327\n",
      "Classifier 46/50: error = 0.4746642230815837, alpha = 0.050714988962372184\n",
      "Classifier 47/50: error = 0.42875611158508586, alpha = 0.14346399533568846\n",
      "Classifier 48/50: error = 0.4362440318610712, alpha = 0.12820984216642137\n",
      "Classifier 49/50: error = 0.4493955193604088, alpha = 0.10155667047171771\n",
      "Classifier 50/50: error = 0.4629499533613245, alpha = 0.07423616535244572\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2932228014699598, alpha = 0.4398913724798685\n",
      "Classifier 2/50: error = 0.3921341689713867, alpha = 0.21917506792960367\n",
      "Classifier 3/50: error = 0.38685118224230763, alpha = 0.23028379945022065\n",
      "Classifier 4/50: error = 0.41323856367980893, alpha = 0.1752966316032332\n",
      "Classifier 5/50: error = 0.40223972978039796, alpha = 0.19807076697900777\n",
      "Classifier 6/50: error = 0.41270172521026643, alpha = 0.17640385393654864\n",
      "Classifier 7/50: error = 0.38958111433269266, alpha = 0.22453666156632238\n",
      "Classifier 8/50: error = 0.45288343190956737, alpha = 0.09451355819205362\n",
      "Classifier 9/50: error = 0.3784893410861241, alpha = 0.2479825747291302\n",
      "Classifier 10/50: error = 0.41041041921610294, alpha = 0.18113449374403556\n",
      "Classifier 11/50: error = 0.4089606149329014, alpha = 0.18413190055567613\n",
      "Classifier 12/50: error = 0.4499808176029173, alpha = 0.10037410017839862\n",
      "Classifier 13/50: error = 0.4486233261962364, alpha = 0.10311728778628808\n",
      "Classifier 14/50: error = 0.3861350444024605, alpha = 0.2317938987961481\n",
      "Classifier 15/50: error = 0.4153947071701965, alpha = 0.17085386824716545\n",
      "Classifier 16/50: error = 0.4187000960365693, alpha = 0.16405595351848584\n",
      "Classifier 17/50: error = 0.41192565254819136, alpha = 0.17800525149600696\n",
      "Classifier 18/50: error = 0.43478318197839916, alpha = 0.131180965797488\n",
      "Classifier 19/50: error = 0.4281615927608987, alpha = 0.1446778845380823\n",
      "Classifier 20/50: error = 0.43468497693304664, alpha = 0.13138078044297258\n",
      "Classifier 21/50: error = 0.44116330446610796, alpha = 0.1182210909117501\n",
      "Classifier 22/50: error = 0.42131254310268496, alpha = 0.15869379637040096\n",
      "Classifier 23/50: error = 0.4635494016216737, alpha = 0.073030756651018\n",
      "Classifier 24/50: error = 0.45582009425158304, alpha = 0.08859084986381113\n",
      "Classifier 25/50: error = 0.4152858514713971, alpha = 0.17107800545163457\n",
      "Classifier 26/50: error = 0.43193280194771366, alpha = 0.1369848465001324\n",
      "Classifier 27/50: error = 0.4687371793116114, alpha = 0.06260731343706591\n",
      "Classifier 28/50: error = 0.4541194589715706, alpha = 0.0920199367634215\n",
      "Classifier 29/50: error = 0.46671394323051785, alpha = 0.06667072166007175\n",
      "Classifier 30/50: error = 0.43772871523277657, alpha = 0.12519254980579497\n",
      "Classifier 31/50: error = 0.4395056068721781, alpha = 0.12158438232033528\n",
      "Classifier 32/50: error = 0.45329922058655514, alpha = 0.09367459650354232\n",
      "Classifier 33/50: error = 0.4203582175937114, alpha = 0.16065152466609553\n",
      "Classifier 34/50: error = 0.4459582118070676, alpha = 0.10850743083973366\n",
      "Classifier 35/50: error = 0.44842028437169673, alpha = 0.10352772205182019\n",
      "Classifier 36/50: error = 0.44898400900816904, alpha = 0.10238827901370459\n",
      "Classifier 37/50: error = 0.4559706007272777, alpha = 0.08828747639712829\n",
      "Classifier 38/50: error = 0.4702048581743106, alpha = 0.059660969374023654\n",
      "Classifier 39/50: error = 0.4451287992712366, alpha = 0.11018616957914852\n",
      "Classifier 40/50: error = 0.4410146641196011, alpha = 0.11852255656258431\n",
      "Classifier 41/50: error = 0.46756531947623836, alpha = 0.06496058228845436\n",
      "Classifier 42/50: error = 0.4361987668618119, alpha = 0.12830186952741754\n",
      "Classifier 43/50: error = 0.444727783346927, alpha = 0.11099805086991492\n",
      "Classifier 44/50: error = 0.4367761765303661, alpha = 0.1271281110297572\n",
      "Classifier 45/50: error = 0.45382418524406676, alpha = 0.09261553152818937\n",
      "Classifier 46/50: error = 0.47404880520893955, alpha = 0.05194907093665471\n",
      "Classifier 47/50: error = 0.44025084899134537, alpha = 0.1200720314457966\n",
      "Classifier 48/50: error = 0.4708267691487163, alpha = 0.05841280706622801\n",
      "Classifier 49/50: error = 0.46925150708645835, alpha = 0.06157468693003557\n",
      "Classifier 50/50: error = 0.466493420141451, alpha = 0.06711374428289715\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Classifier 1/50: error = 0.399394856278366, alpha = 0.2039935887284973\n",
      "Classifier 2/50: error = 0.398159279613181, alpha = 0.20657035044067382\n",
      "Classifier 3/50: error = 0.41782823086296295, alpha = 0.16584756148723268\n",
      "Classifier 4/50: error = 0.35169504715930794, alpha = 0.3057983674745003\n",
      "Classifier 5/50: error = 0.3642180761868512, alpha = 0.2785514093460986\n",
      "Classifier 6/50: error = 0.37777772959329736, alpha = 0.24949568550074944\n",
      "Classifier 7/50: error = 0.3616955310649749, alpha = 0.2840063049539\n",
      "Classifier 8/50: error = 0.3998072232207076, alpha = 0.20313420458616108\n",
      "Classifier 9/50: error = 0.4171799748445292, alpha = 0.16718035543776266\n",
      "Classifier 10/50: error = 0.36138820329247845, alpha = 0.2846720084106652\n",
      "Classifier 11/50: error = 0.4448476760553771, alpha = 0.11075530551716732\n",
      "Classifier 12/50: error = 0.4168722568125468, alpha = 0.16781321976039296\n",
      "Classifier 13/50: error = 0.38757721810883206, alpha = 0.22875388125733084\n",
      "Classifier 14/50: error = 0.42275074745992425, alpha = 0.15574569872008734\n",
      "Classifier 15/50: error = 0.36831633567085, alpha = 0.2697232804412087\n",
      "Classifier 16/50: error = 0.4335698263208949, alpha = 0.133650477342149\n",
      "Classifier 17/50: error = 0.3885361534337709, alpha = 0.2267348031175524\n",
      "Classifier 18/50: error = 0.43971051499768754, alpha = 0.12116849884517676\n",
      "Classifier 19/50: error = 0.41572088026209986, alpha = 0.17018236958922625\n",
      "Classifier 20/50: error = 0.4498096995594604, alpha = 0.10071980784455065\n",
      "Classifier 21/50: error = 0.386258557522009, alpha = 0.23153337627143425\n",
      "Classifier 22/50: error = 0.4624785660473294, alpha = 0.07518421205033936\n",
      "Classifier 23/50: error = 0.4453829839465233, alpha = 0.10967163211189856\n",
      "Classifier 24/50: error = 0.4412871393579036, alpha = 0.11796995089974494\n",
      "Classifier 25/50: error = 0.4482351823312998, alpha = 0.10390192263384967\n",
      "Classifier 26/50: error = 0.4283247952390851, alpha = 0.14434461551755662\n",
      "Classifier 27/50: error = 0.430831041605232, alpha = 0.13923066538288784\n",
      "Classifier 28/50: error = 0.4381560179188932, alpha = 0.12432457355105538\n",
      "Classifier 29/50: error = 0.44089907998022904, alpha = 0.11875699392222276\n",
      "Classifier 30/50: error = 0.44229583073770284, alpha = 0.11592485031429248\n",
      "Classifier 31/50: error = 0.4402181733911328, alpha = 0.12013832989558505\n",
      "Classifier 32/50: error = 0.4482959270689659, alpha = 0.10377911843985414\n",
      "Classifier 33/50: error = 0.44471954316656115, alpha = 0.11101473514406732\n",
      "Classifier 34/50: error = 0.44863559810044534, alpha = 0.10309248213616265\n",
      "Classifier 35/50: error = 0.4509980615732727, alpha = 0.09831946543294826\n",
      "Classifier 36/50: error = 0.45723329423712533, alpha = 0.08574291839435823\n",
      "Classifier 37/50: error = 0.4494975536012228, alpha = 0.1013504943309648\n",
      "Classifier 38/50: error = 0.45112685968677124, alpha = 0.09805937768586404\n",
      "Classifier 39/50: error = 0.4659187761467477, alpha = 0.06826830657041842\n",
      "Classifier 40/50: error = 0.46449778415496457, alpha = 0.07112411994749868\n",
      "Classifier 41/50: error = 0.4408878071480951, alpha = 0.11877985911243037\n",
      "Classifier 42/50: error = 0.44102698839831245, alpha = 0.1184975602015044\n",
      "Classifier 43/50: error = 0.467225906768174, alpha = 0.0656423065481805\n",
      "Classifier 44/50: error = 0.4193383989674652, alpha = 0.1627449580437862\n",
      "Classifier 45/50: error = 0.4786916890815369, alpha = 0.04264244974748578\n",
      "Classifier 46/50: error = 0.4576924981845114, alpha = 0.08481781488445027\n",
      "Classifier 47/50: error = 0.4688456022975689, alpha = 0.06238961934793418\n",
      "Classifier 48/50: error = 0.4643091652164073, alpha = 0.07150327958094284\n",
      "Classifier 49/50: error = 0.46308178744813344, alpha = 0.07397104663236537\n",
      "Classifier 50/50: error = 0.45479140721120803, alpha = 0.09066479615996381\n",
      "Multiclass Accuracy: 0.8361\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostMulticlass: # Creamos la clase AdaBoostMulticlass\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.models = [] # Inicializamos los modelos\n",
    "\n",
    "    def fit(self, X, y, verbose=False): # Creamos la función fit\n",
    "        for digit in range(10): # Para cada dígito\n",
    "            X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X, y)\n",
    "            model = AdaBoost(T=self.T, A=self.A) # Creamos el clasificador AdaBoost\n",
    "            model.fit(X_train_balanced, Y_train_binary_balanced, verbose) # Ajustamos el clasificador AdaBoost\n",
    "            self.models.append(model) # Añadimos el clasificador AdaBoost\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        model_preds = np.array([model.predict(X) for model in self.models]) # Realizamos las predicciones\n",
    "        return np.argmax(model_preds, axis=0) # Devolvemos el índice del valor máximo\n",
    "\n",
    "def run_adaboost_multiclass_on_mnist(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_multiclass_on_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    adaboost_multiclass = AdaBoostMulticlass(T=T, A=A) # Creamos el clasificador AdaBoostMulticlass\n",
    "    adaboost_multiclass.fit(X_train, y_train, verboseParam) # Ajustamos el clasificador AdaBoostMulticlass\n",
    "    y_pred = adaboost_multiclass.predict(X_test) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test == y_pred) / len(y_test) # Calculamos la precisión\n",
    "    print(f\"Multiclass Accuracy: {accuracy}\") # Mostramos la precisión\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "accuracy = run_adaboost_multiclass_on_mnist(T=50, A=20, verboseParam=True) # Ejecutamos AdaBoostMulticlass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclase con ADABoosti Binario con Mejoras (Version 1: Solo añadido el parámetro n_componentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False):\n",
    "    print(f\"Running AdaBoost for digit: {digit}\")\n",
    "\n",
    "    # Load MNIST data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Balance the training dataset for the specified digit\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train)\n",
    "\n",
    "    # Flatten the images\n",
    "    X_train_balanced = X_train_balanced.reshape(X_train_balanced.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # Create the AdaBoost classifier (assuming AdaBoost class is properly defined and imported)\n",
    "    adaboost = AdaBoost(T=T, A=A)\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced, verboseParam)  # Train the model\n",
    "    y_pred = adaboost.predict(X_test)  # Predict on the test set\n",
    "\n",
    "    # Convert test labels to binary\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1)\n",
    "\n",
    "    # Calculate accuracy using the correct labels\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary)\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.32654284002396616, alpha = 0.36193161781632127\n",
      "Classifier 2/50: error = 0.3278824223679109, alpha = 0.35888910802399876\n",
      "Classifier 3/50: error = 0.3102920104514464, alpha = 0.39937724049123485\n",
      "Classifier 4/50: error = 0.34543031674796315, alpha = 0.31959355636828796\n",
      "Classifier 5/50: error = 0.3359004284699686, alpha = 0.34080866150254474\n",
      "Classifier 6/50: error = 0.3007772224580255, alpha = 0.42179976710006434\n",
      "Classifier 7/50: error = 0.3291195840877196, alpha = 0.3560848705120504\n",
      "Classifier 8/50: error = 0.37566007272679985, alpha = 0.2540051513004905\n",
      "Classifier 9/50: error = 0.38894746673838576, alpha = 0.22586932348998157\n",
      "Classifier 10/50: error = 0.3522621835287082, alpha = 0.3045551361122384\n",
      "Classifier 11/50: error = 0.42055947686654804, alpha = 0.16023855501713782\n",
      "Classifier 12/50: error = 0.36865403739146096, alpha = 0.26899767634644345\n",
      "Classifier 13/50: error = 0.4004842198509689, alpha = 0.20172396580539753\n",
      "Classifier 14/50: error = 0.4311166730882734, alpha = 0.13864830349454627\n",
      "Classifier 15/50: error = 0.40379841487436446, alpha = 0.1948315297628514\n",
      "Classifier 16/50: error = 0.3921296411159969, alpha = 0.21918456568594108\n",
      "Classifier 17/50: error = 0.4016731671210684, alpha = 0.19924920350731823\n",
      "Classifier 18/50: error = 0.4362299405831749, alpha = 0.12823849062893902\n",
      "Classifier 19/50: error = 0.42362321810608583, alpha = 0.15395857919721745\n",
      "Classifier 20/50: error = 0.40638886996441737, alpha = 0.18945696633453005\n",
      "Classifier 21/50: error = 0.3766718007133424, alpha = 0.25184946572261613\n",
      "Classifier 22/50: error = 0.45826995647845015, alpha = 0.08365468377249302\n",
      "Classifier 23/50: error = 0.438923820996694, alpha = 0.12276540908399329\n",
      "Classifier 24/50: error = 0.43241390248139133, alpha = 0.13600460716725055\n",
      "Classifier 25/50: error = 0.42603542048254195, alpha = 0.14902259835589723\n",
      "Classifier 26/50: error = 0.43674002995048183, alpha = 0.127201579549953\n",
      "Classifier 27/50: error = 0.4246518516612346, alpha = 0.15185283611187658\n",
      "Classifier 28/50: error = 0.43404108237260564, alpha = 0.13269115117640812\n",
      "Classifier 29/50: error = 0.43173168066299916, alpha = 0.13739470731132222\n",
      "Classifier 30/50: error = 0.4282407253732895, alpha = 0.14451628712679943\n",
      "Classifier 31/50: error = 0.43056970729042476, alpha = 0.13976357101064107\n",
      "Classifier 32/50: error = 0.42656344531618773, alpha = 0.1479430940885041\n",
      "Classifier 33/50: error = 0.44181639178417653, alpha = 0.11689678131056827\n",
      "Classifier 34/50: error = 0.4448494309680453, alpha = 0.11075175246278472\n",
      "Classifier 35/50: error = 0.4557560955508455, alpha = 0.08871985593426245\n",
      "Classifier 36/50: error = 0.44149810066691597, alpha = 0.11754215059844611\n",
      "Classifier 37/50: error = 0.45079534323958625, alpha = 0.09872885043417617\n",
      "Classifier 38/50: error = 0.4424368148847175, alpha = 0.11563908518189515\n",
      "Classifier 39/50: error = 0.4486342806446697, alpha = 0.10309514515178973\n",
      "Classifier 40/50: error = 0.44607953109793386, alpha = 0.1082619307447635\n",
      "Classifier 41/50: error = 0.45391575880869967, alpha = 0.09243081208165896\n",
      "Classifier 42/50: error = 0.45218302934077637, alpha = 0.09592710296701012\n",
      "Classifier 43/50: error = 0.45462457184069177, alpha = 0.09100122747952065\n",
      "Classifier 44/50: error = 0.44428922597751264, alpha = 0.11188610401808234\n",
      "Classifier 45/50: error = 0.4575664563963947, alpha = 0.08507172177879398\n",
      "Classifier 46/50: error = 0.4581664124816325, alpha = 0.08386322800630223\n",
      "Classifier 47/50: error = 0.4565819981470669, alpha = 0.08705525857070813\n",
      "Classifier 48/50: error = 0.4546399446314691, alpha = 0.09097022667054237\n",
      "Classifier 49/50: error = 0.46571941594142796, alpha = 0.06866889909014082\n",
      "Classifier 50/50: error = 0.4522682967834311, alpha = 0.09575499682987712\n",
      "Accuracy for digit 4: 0.9146\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=4, T=50, A=50, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n",
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2584212747994935, alpha = 0.5270951122758907\n",
      "Classifier 2/50: error = 0.2338948291435367, alpha = 0.5932239462830551\n",
      "Classifier 3/50: error = 0.2530358589146863, alpha = 0.5412429831463339\n",
      "Classifier 4/50: error = 0.24243996186387784, alpha = 0.5696743462229993\n",
      "Classifier 5/50: error = 0.3391752783385592, alpha = 0.3334858070368075\n",
      "Classifier 6/50: error = 0.2956139657450003, alpha = 0.4341360582070645\n",
      "Classifier 7/50: error = 0.32158124068765226, alpha = 0.37325726590584635\n",
      "Classifier 8/50: error = 0.438662777700055, alpha = 0.12329543819784176\n",
      "Classifier 9/50: error = 0.39040158401278663, alpha = 0.22281224694513813\n",
      "Classifier 10/50: error = 0.39913737263404836, alpha = 0.20453034246953708\n",
      "Classifier 11/50: error = 0.2856434781013628, alpha = 0.45831885738417766\n",
      "Classifier 12/50: error = 0.3412330561570819, alpha = 0.32890206380384396\n",
      "Classifier 13/50: error = 0.3667542277662121, alpha = 0.2730833339188711\n",
      "Classifier 14/50: error = 0.40703241462323536, alpha = 0.188123455124968\n",
      "Classifier 15/50: error = 0.3925981044094293, alpha = 0.21820210990669286\n",
      "Classifier 16/50: error = 0.42669887380538374, alpha = 0.14766627669079402\n",
      "Classifier 17/50: error = 0.3516017139998009, alpha = 0.30600305282089985\n",
      "Classifier 18/50: error = 0.39084014603868994, alpha = 0.22189103821224182\n",
      "Classifier 19/50: error = 0.4257479516268161, alpha = 0.1496104501114625\n",
      "Classifier 20/50: error = 0.42270400628129, alpha = 0.15584146846431168\n",
      "Classifier 21/50: error = 0.3783940690383565, alpha = 0.24818508854120658\n",
      "Classifier 22/50: error = 0.4420330103147788, alpha = 0.11645761952754806\n",
      "Classifier 23/50: error = 0.44195718840320164, alpha = 0.11661133205631016\n",
      "Classifier 24/50: error = 0.4243168478956997, alpha = 0.15253848360116892\n",
      "Classifier 25/50: error = 0.4380840724233459, alpha = 0.12447070273221819\n",
      "Classifier 26/50: error = 0.44887540616770805, alpha = 0.10260777462716357\n",
      "Classifier 27/50: error = 0.4750762306893849, alpha = 0.049888886947865765\n",
      "Classifier 28/50: error = 0.4351275315325669, alpha = 0.13048041101527427\n",
      "Classifier 29/50: error = 0.4054830684489281, alpha = 0.19133503901720939\n",
      "Classifier 30/50: error = 0.4032105879728012, alpha = 0.19605266557244297\n",
      "Classifier 31/50: error = 0.4473456030537064, alpha = 0.10570069436206257\n",
      "Classifier 32/50: error = 0.44963737391491576, alpha = 0.10106797946244603\n",
      "Classifier 33/50: error = 0.43962645481327206, alpha = 0.12133910313902063\n",
      "Classifier 34/50: error = 0.39546321123055006, alpha = 0.2122023808970947\n",
      "Classifier 35/50: error = 0.4493653589847012, alpha = 0.10161761587449819\n",
      "Classifier 36/50: error = 0.4470838224809188, alpha = 0.10623015645257278\n",
      "Classifier 37/50: error = 0.4359672695546922, alpha = 0.12877255588206327\n",
      "Classifier 38/50: error = 0.44417354346039084, alpha = 0.11212038362138568\n",
      "Classifier 39/50: error = 0.4424539957466016, alpha = 0.11560426204722657\n",
      "Classifier 40/50: error = 0.4553997624573092, alpha = 0.08943819218283026\n",
      "Classifier 41/50: error = 0.4562596240184288, alpha = 0.0877049422267531\n",
      "Classifier 42/50: error = 0.4620270688276893, alpha = 0.07609238257891474\n",
      "Classifier 43/50: error = 0.4499063879123779, alpha = 0.10052446661814753\n",
      "Classifier 44/50: error = 0.46250819129921167, alpha = 0.0751246262581161\n",
      "Classifier 45/50: error = 0.4710386995787079, alpha = 0.057987508851940595\n",
      "Classifier 46/50: error = 0.43415929887084537, alpha = 0.1324505384617445\n",
      "Classifier 47/50: error = 0.44892581054214553, alpha = 0.10250590185878836\n",
      "Classifier 48/50: error = 0.45762430810921306, alpha = 0.08495518011472514\n",
      "Classifier 49/50: error = 0.4535564663808652, alpha = 0.09315560217949861\n",
      "Classifier 50/50: error = 0.45530671522384003, alpha = 0.08962578236933806\n",
      "Accuracy for digit 0: 0.9503\n",
      "Running AdaBoost for digit: 1\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Classifier 1/50: error = 0.23488837795742767, alpha = 0.5904556600837811\n",
      "Classifier 2/50: error = 0.36377228858825605, alpha = 0.2795142224377521\n",
      "Classifier 3/50: error = 0.3641814846786158, alpha = 0.2786304207833748\n",
      "Classifier 4/50: error = 0.21586398725082742, alpha = 0.6449669849619093\n",
      "Classifier 5/50: error = 0.2944099947759673, alpha = 0.4370305017686607\n",
      "Classifier 6/50: error = 0.3223930070616839, alpha = 0.3713980715684888\n",
      "Classifier 7/50: error = 0.32364027825851105, alpha = 0.3685462101929708\n",
      "Classifier 8/50: error = 0.3270507652222623, alpha = 0.3607772456149383\n",
      "Classifier 9/50: error = 0.38095446764947716, alpha = 0.24274948364589907\n",
      "Classifier 10/50: error = 0.4210473230822561, alpha = 0.15923775399619852\n",
      "Classifier 11/50: error = 0.3864643841352977, alpha = 0.23109930093627346\n",
      "Classifier 12/50: error = 0.3958710615488188, alpha = 0.2113495466185633\n",
      "Classifier 13/50: error = 0.40250084484033277, alpha = 0.19752783696312964\n",
      "Classifier 14/50: error = 0.36690909353817003, alpha = 0.27274995399087143\n",
      "Classifier 15/50: error = 0.3605701139765769, alpha = 0.28644527370482276\n",
      "Classifier 16/50: error = 0.4225558662223444, alpha = 0.15614501690229468\n",
      "Classifier 17/50: error = 0.3820231533929357, alpha = 0.24048488693636225\n",
      "Classifier 18/50: error = 0.37239637663472086, alpha = 0.26097498925590396\n",
      "Classifier 19/50: error = 0.3509990958238488, alpha = 0.30732523114688814\n",
      "Classifier 20/50: error = 0.3322000358855465, alpha = 0.3491256842090992\n",
      "Classifier 21/50: error = 0.38671458634635114, alpha = 0.23057175545744601\n",
      "Classifier 22/50: error = 0.35792103061895647, alpha = 0.2921994620315893\n",
      "Classifier 23/50: error = 0.37662824467042033, alpha = 0.2519422231031721\n",
      "Classifier 24/50: error = 0.4153858330942841, alpha = 0.17087213960339367\n",
      "Classifier 25/50: error = 0.4061108247985926, alpha = 0.19003331915281693\n",
      "Classifier 26/50: error = 0.4300471866017348, alpha = 0.140829317441415\n",
      "Classifier 27/50: error = 0.44854564464240093, alpha = 0.10327431129547567\n",
      "Classifier 28/50: error = 0.4195665297693467, alpha = 0.1622765402290686\n",
      "Classifier 29/50: error = 0.4109013850138448, alpha = 0.180120175408312\n",
      "Classifier 30/50: error = 0.4343439346433118, alpha = 0.13207476933824727\n",
      "Classifier 31/50: error = 0.4310263753626099, alpha = 0.13883239756941707\n",
      "Classifier 32/50: error = 0.4054885149076518, alpha = 0.19132374245387423\n",
      "Classifier 33/50: error = 0.39487030788602473, alpha = 0.21344271278911775\n",
      "Classifier 34/50: error = 0.4571856527627508, alpha = 0.08583890435553747\n",
      "Classifier 35/50: error = 0.44369641382860714, alpha = 0.11308679377828879\n",
      "Classifier 36/50: error = 0.4306934952203723, alpha = 0.13951113631182566\n",
      "Classifier 37/50: error = 0.3838801436754883, alpha = 0.23655556922140095\n",
      "Classifier 38/50: error = 0.42101049607632834, alpha = 0.15931329236809416\n",
      "Classifier 39/50: error = 0.41792833539844165, alpha = 0.16564180186337985\n",
      "Classifier 40/50: error = 0.4473553091616874, alpha = 0.10568106449211027\n",
      "Classifier 41/50: error = 0.43468317893324204, alpha = 0.1313844388725056\n",
      "Classifier 42/50: error = 0.4233224106477065, alpha = 0.1545746254113941\n",
      "Classifier 43/50: error = 0.4275446678116523, alpha = 0.1459379704596006\n",
      "Classifier 44/50: error = 0.4645609598652268, alpha = 0.07099712942648337\n",
      "Classifier 45/50: error = 0.40753198811280034, alpha = 0.18708872753141842\n",
      "Classifier 46/50: error = 0.47316053701924277, alpha = 0.053730572573633506\n",
      "Classifier 47/50: error = 0.43141162671028455, alpha = 0.1380470331431574\n",
      "Classifier 48/50: error = 0.43129168150727804, alpha = 0.13829153245571965\n",
      "Classifier 49/50: error = 0.46349604286303014, alpha = 0.07313804519742467\n",
      "Classifier 50/50: error = 0.4467121313983585, alpha = 0.10698201913809265\n",
      "Accuracy for digit 1: 0.9622\n",
      "Running AdaBoost for digit: 2\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3580899630748573, alpha = 0.2918319580051959\n",
      "Classifier 2/50: error = 0.3876752033755485, alpha = 0.22854748536224165\n",
      "Classifier 3/50: error = 0.3124546981653885, alpha = 0.39433411398786167\n",
      "Classifier 4/50: error = 0.31502879980870313, alpha = 0.3883563655400444\n",
      "Classifier 5/50: error = 0.3772927841439736, alpha = 0.25052747395790154\n",
      "Classifier 6/50: error = 0.3476045523265942, alpha = 0.3147927015079227\n",
      "Classifier 7/50: error = 0.4465868646637047, alpha = 0.10723543780669441\n",
      "Classifier 8/50: error = 0.3510113537209585, alpha = 0.3072983262585254\n",
      "Classifier 9/50: error = 0.31174182596306954, alpha = 0.3959943279790693\n",
      "Classifier 10/50: error = 0.3525149410359457, alpha = 0.3040013554737989\n",
      "Classifier 11/50: error = 0.41035218543936836, alpha = 0.1812548271333815\n",
      "Classifier 12/50: error = 0.3938698819540064, alpha = 0.2155370366368049\n",
      "Classifier 13/50: error = 0.3993474343190859, alpha = 0.20409243644064892\n",
      "Classifier 14/50: error = 0.3864998398142857, alpha = 0.23102453579062954\n",
      "Classifier 15/50: error = 0.38542220040954844, alpha = 0.2332980846758525\n",
      "Classifier 16/50: error = 0.440347427749167, alpha = 0.11987608029881756\n",
      "Classifier 17/50: error = 0.40587520132974675, alpha = 0.1905218347633543\n",
      "Classifier 18/50: error = 0.42146093807556295, alpha = 0.15838948376029036\n",
      "Classifier 19/50: error = 0.43069719654733407, alpha = 0.1395035886481101\n",
      "Classifier 20/50: error = 0.40082825918898135, alpha = 0.20100760761712405\n",
      "Classifier 21/50: error = 0.4305221566581778, alpha = 0.13986054340310913\n",
      "Classifier 22/50: error = 0.4242978153688889, alpha = 0.15257744147259208\n",
      "Classifier 23/50: error = 0.41467465887814103, alpha = 0.17233678648821715\n",
      "Classifier 24/50: error = 0.44792348779665503, alpha = 0.10453210701623655\n",
      "Classifier 25/50: error = 0.45880405130340435, alpha = 0.08257909751194653\n",
      "Classifier 26/50: error = 0.43288732877978253, alpha = 0.1350402575672325\n",
      "Classifier 27/50: error = 0.4374014246985375, alpha = 0.12585749910940183\n",
      "Classifier 28/50: error = 0.414229311069518, alpha = 0.1732543425738461\n",
      "Classifier 29/50: error = 0.41802554430196803, alpha = 0.16544200735989545\n",
      "Classifier 30/50: error = 0.4183786702548119, alpha = 0.16471633379770345\n",
      "Classifier 31/50: error = 0.4394386997770525, alpha = 0.12172018665276836\n",
      "Classifier 32/50: error = 0.4451476765872853, alpha = 0.1101479548705568\n",
      "Classifier 33/50: error = 0.47243740532790357, alpha = 0.05518112925603424\n",
      "Classifier 34/50: error = 0.4716371168928135, alpha = 0.05678672823918304\n",
      "Classifier 35/50: error = 0.43970497304996464, alpha = 0.12117974628542588\n",
      "Classifier 36/50: error = 0.4554032077576758, alpha = 0.08943124632004928\n",
      "Classifier 37/50: error = 0.4392099662352257, alpha = 0.12218449116100705\n",
      "Classifier 38/50: error = 0.4701412132471295, alpha = 0.05978871381879154\n",
      "Classifier 39/50: error = 0.44728796215283884, alpha = 0.10581727040107079\n",
      "Classifier 40/50: error = 0.44479025171881675, alpha = 0.11087157027032611\n",
      "Classifier 41/50: error = 0.4523380217554547, alpha = 0.09561426624369615\n",
      "Classifier 42/50: error = 0.46292342817718946, alpha = 0.0742895088300885\n",
      "Classifier 43/50: error = 0.45981673908486975, alpha = 0.08054021860382968\n",
      "Classifier 44/50: error = 0.46230712766738524, alpha = 0.07552903956904886\n",
      "Classifier 45/50: error = 0.4712335195950018, alpha = 0.05759656599865878\n",
      "Classifier 46/50: error = 0.4676945992978094, alpha = 0.06470093438732745\n",
      "Classifier 47/50: error = 0.4743625592525411, alpha = 0.05131988833010247\n",
      "Classifier 48/50: error = 0.4519509833056534, alpha = 0.09639549974696689\n",
      "Classifier 49/50: error = 0.46956951791871204, alpha = 0.06093627575570696\n",
      "Classifier 50/50: error = 0.47721708001269936, alpha = 0.045597414603044374\n",
      "Accuracy for digit 2: 0.9113\n",
      "Running AdaBoost for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.36541598694942906, alpha = 0.27596664582366454\n",
      "Classifier 2/50: error = 0.46309791589731286, alpha = 0.07393861298893968\n",
      "Classifier 3/50: error = 0.4144370029473473, alpha = 0.1728263963507353\n",
      "Classifier 4/50: error = 0.3322561233711972, alpha = 0.34899927710343215\n",
      "Classifier 5/50: error = 0.308680499872223, alpha = 0.4031476646544513\n",
      "Classifier 6/50: error = 0.4070080678859642, alpha = 0.18817389278118274\n",
      "Classifier 7/50: error = 0.3508542213747648, alpha = 0.3076432491255713\n",
      "Classifier 8/50: error = 0.3247466322089466, alpha = 0.36602134744381437\n",
      "Classifier 9/50: error = 0.41135824076411664, alpha = 0.17917665652424267\n",
      "Classifier 10/50: error = 0.3986399824962007, alpha = 0.2055675379136564\n",
      "Classifier 11/50: error = 0.42345913760188125, alpha = 0.1542945975979354\n",
      "Classifier 12/50: error = 0.396679261564567, alpha = 0.20966045549465362\n",
      "Classifier 13/50: error = 0.43403682622619333, alpha = 0.13269981423625776\n",
      "Classifier 14/50: error = 0.3811053040709116, alpha = 0.24242970643680592\n",
      "Classifier 15/50: error = 0.42798079640595854, alpha = 0.14504711855472763\n",
      "Classifier 16/50: error = 0.44198745273157086, alpha = 0.11654997701798221\n",
      "Classifier 17/50: error = 0.4212333907778174, alpha = 0.15885612544772582\n",
      "Classifier 18/50: error = 0.4188950027699283, alpha = 0.1636555800552334\n",
      "Classifier 19/50: error = 0.4019905513464851, alpha = 0.1985889849693149\n",
      "Classifier 20/50: error = 0.41373973445223455, alpha = 0.17426335733669854\n",
      "Classifier 21/50: error = 0.42029186748251446, alpha = 0.16078768225604087\n",
      "Classifier 22/50: error = 0.4609926430906738, alpha = 0.07817356784306277\n",
      "Classifier 23/50: error = 0.4337387023258946, alpha = 0.13330667193782816\n",
      "Classifier 24/50: error = 0.4080291808011227, alpha = 0.1860593241405917\n",
      "Classifier 25/50: error = 0.4276042027074325, alpha = 0.14581634882096556\n",
      "Classifier 26/50: error = 0.43087088384771954, alpha = 0.13914942710555425\n",
      "Classifier 27/50: error = 0.4423645429872035, alpha = 0.11578557297671781\n",
      "Classifier 28/50: error = 0.42591930015574786, alpha = 0.14926004318502348\n",
      "Classifier 29/50: error = 0.4224270827237296, alpha = 0.1564089257051137\n",
      "Classifier 30/50: error = 0.428102591134774, alpha = 0.14479837716633065\n",
      "Classifier 31/50: error = 0.46846036071513475, alpha = 0.06316314291861189\n",
      "Classifier 32/50: error = 0.4420570865501754, alpha = 0.1164088113132392\n",
      "Classifier 33/50: error = 0.45372174847021823, alpha = 0.09282217138331766\n",
      "Classifier 34/50: error = 0.4158748605840501, alpha = 0.16986542026139922\n",
      "Classifier 35/50: error = 0.4165672985981046, alpha = 0.16844053955603466\n",
      "Classifier 36/50: error = 0.4411452707798599, alpha = 0.11825766488020856\n",
      "Classifier 37/50: error = 0.454567210573783, alpha = 0.09111690389649367\n",
      "Classifier 38/50: error = 0.4544122196102658, alpha = 0.09142947540098853\n",
      "Classifier 39/50: error = 0.4489210090463339, alpha = 0.10251560611698621\n",
      "Classifier 40/50: error = 0.4673088116070043, alpha = 0.06547578320053829\n",
      "Classifier 41/50: error = 0.4614647669292731, alpha = 0.07722360812537903\n",
      "Classifier 42/50: error = 0.4617752128861486, alpha = 0.0765990361502422\n",
      "Classifier 43/50: error = 0.4616776090729497, alpha = 0.07679539433220661\n",
      "Classifier 44/50: error = 0.4419455627065324, alpha = 0.11663490112704296\n",
      "Classifier 45/50: error = 0.4554574754898749, alpha = 0.0893218415451554\n",
      "Classifier 46/50: error = 0.4599686803719429, alpha = 0.08023436806909331\n",
      "Classifier 47/50: error = 0.4727601191567211, alpha = 0.054533757316308315\n",
      "Classifier 48/50: error = 0.447934958466132, alpha = 0.10450891413979788\n",
      "Classifier 49/50: error = 0.47063644383661496, alpha = 0.05879476650020346\n",
      "Classifier 50/50: error = 0.46947383707972556, alpha = 0.0611283511311231\n",
      "Accuracy for digit 3: 0.8811\n",
      "Running AdaBoost for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3754172729607122, alpha = 0.25452282734076154\n",
      "Classifier 2/50: error = 0.3254241212835153, alpha = 0.3644774226342497\n",
      "Classifier 3/50: error = 0.2789501393698056, alpha = 0.4748376177933295\n",
      "Classifier 4/50: error = 0.3293821249049772, alpha = 0.3554904691232416\n",
      "Classifier 5/50: error = 0.39413464538270837, alpha = 0.21498259186248136\n",
      "Classifier 6/50: error = 0.37575569405632264, alpha = 0.25380131260786587\n",
      "Classifier 7/50: error = 0.37670128182390233, alpha = 0.25178668486616274\n",
      "Classifier 8/50: error = 0.3896595626414452, alpha = 0.2243717269110106\n",
      "Classifier 9/50: error = 0.3427554577753379, alpha = 0.3255194588381902\n",
      "Classifier 10/50: error = 0.3677225980591171, alpha = 0.2709996902823785\n",
      "Classifier 11/50: error = 0.4286482402684607, alpha = 0.14368421582734156\n",
      "Classifier 12/50: error = 0.39335436151447206, alpha = 0.21661697001775648\n",
      "Classifier 13/50: error = 0.4208566303903159, alpha = 0.15962891623501335\n",
      "Classifier 14/50: error = 0.41312084806688265, alpha = 0.17553938201092018\n",
      "Classifier 15/50: error = 0.4295101144357042, alpha = 0.14192507477087174\n",
      "Classifier 16/50: error = 0.4502024199102346, alpha = 0.09992643529819867\n",
      "Classifier 17/50: error = 0.418458616557287, alpha = 0.16455206809931758\n",
      "Classifier 18/50: error = 0.41348997870733506, alpha = 0.1747782377998796\n",
      "Classifier 19/50: error = 0.41518481626129744, alpha = 0.17128605526467508\n",
      "Classifier 20/50: error = 0.4290152305249637, alpha = 0.14293505756577154\n",
      "Classifier 21/50: error = 0.3850203934020575, alpha = 0.2341464021979219\n",
      "Classifier 22/50: error = 0.41475105788786726, alpha = 0.1721794094925258\n",
      "Classifier 23/50: error = 0.44313767744632715, alpha = 0.11421876325960895\n",
      "Classifier 24/50: error = 0.3270094048627197, alpha = 0.36087121166107483\n",
      "Classifier 25/50: error = 0.44490597985869873, alpha = 0.11063726319374458\n",
      "Classifier 26/50: error = 0.42617427210516595, alpha = 0.1487386940786026\n",
      "Classifier 27/50: error = 0.4510913315138445, alpha = 0.09813111998004985\n",
      "Classifier 28/50: error = 0.4405087989279487, alpha = 0.1195486905628581\n",
      "Classifier 29/50: error = 0.41791685828460645, alpha = 0.16566539176337397\n",
      "Classifier 30/50: error = 0.40532155853768426, alpha = 0.1916700505434247\n",
      "Classifier 31/50: error = 0.42310975667460593, alpha = 0.15501020568890325\n",
      "Classifier 32/50: error = 0.43255773607317016, alpha = 0.13571159763894583\n",
      "Classifier 33/50: error = 0.4398998623998003, alpha = 0.12078423464096813\n",
      "Classifier 34/50: error = 0.4303644805245831, alpha = 0.14018211897324157\n",
      "Classifier 35/50: error = 0.4425201048704815, alpha = 0.11547027096902336\n",
      "Classifier 36/50: error = 0.467366115079761, alpha = 0.06536068508924484\n",
      "Classifier 37/50: error = 0.43483737513725373, alpha = 0.1310707051816252\n",
      "Classifier 38/50: error = 0.45542023555940814, alpha = 0.08939691771982758\n",
      "Classifier 39/50: error = 0.4471386942206227, alpha = 0.10611917117253798\n",
      "Classifier 40/50: error = 0.4515566186133464, alpha = 0.09719164188080036\n",
      "Classifier 41/50: error = 0.4371963243820004, alpha = 0.12627425351356747\n",
      "Classifier 42/50: error = 0.43342118512903394, alpha = 0.1339531137879021\n",
      "Classifier 43/50: error = 0.4385471903710084, alpha = 0.12353015172483701\n",
      "Classifier 44/50: error = 0.4194694743436631, alpha = 0.16247581415041346\n",
      "Classifier 45/50: error = 0.44984973892963986, alpha = 0.10063891465304431\n",
      "Classifier 46/50: error = 0.45501839100896213, alpha = 0.09020710557650455\n",
      "Classifier 47/50: error = 0.45402889299775007, alpha = 0.0922026098743892\n",
      "Classifier 48/50: error = 0.45547865772911245, alpha = 0.08927913832886956\n",
      "Classifier 49/50: error = 0.46073162445024335, alpha = 0.07869882342441416\n",
      "Classifier 50/50: error = 0.43699828213238506, alpha = 0.12667670765887895\n",
      "Accuracy for digit 4: 0.9132\n",
      "Running AdaBoost for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1/50: error = 0.40658732355383354, alpha = 0.18904567296969088\n",
      "Classifier 2/50: error = 0.3535234502220418, alpha = 0.30179355222455206\n",
      "Classifier 3/50: error = 0.3920888553533009, alpha = 0.2192701207992344\n",
      "Classifier 4/50: error = 0.35277143799399835, alpha = 0.30343956675318284\n",
      "Classifier 5/50: error = 0.4002103958797607, alpha = 0.20229426765729375\n",
      "Classifier 6/50: error = 0.4063912695086229, alpha = 0.18945199292165166\n",
      "Classifier 7/50: error = 0.38699706629528197, alpha = 0.22997630463799462\n",
      "Classifier 8/50: error = 0.3969103348972369, alpha = 0.20917774271852513\n",
      "Classifier 9/50: error = 0.3766520908843486, alpha = 0.2518914394438665\n",
      "Classifier 10/50: error = 0.4009629955944085, alpha = 0.20072711518025996\n",
      "Classifier 11/50: error = 0.4219124507168386, alpha = 0.1574637483648609\n",
      "Classifier 12/50: error = 0.4318282411913046, alpha = 0.13719792293685557\n",
      "Classifier 13/50: error = 0.40917243429560046, alpha = 0.18369377041085402\n",
      "Classifier 14/50: error = 0.45070173172954603, alpha = 0.0989179078460086\n",
      "Classifier 15/50: error = 0.40692835203728905, alpha = 0.18833904188399916\n",
      "Classifier 16/50: error = 0.4627803367769654, alpha = 0.07457728010996331\n",
      "Classifier 17/50: error = 0.4400521958570309, alpha = 0.12047511281223283\n",
      "Classifier 18/50: error = 0.42400753400607694, alpha = 0.15317167836005305\n",
      "Classifier 19/50: error = 0.4497401863163545, alpha = 0.1008602514384922\n",
      "Classifier 20/50: error = 0.42717926659737815, alpha = 0.14668452914612728\n",
      "Classifier 21/50: error = 0.41984414987225516, alpha = 0.16170660190781116\n",
      "Classifier 22/50: error = 0.44143736908768527, alpha = 0.11766530142885984\n",
      "Classifier 23/50: error = 0.4192737256815896, alpha = 0.16287776367739853\n",
      "Classifier 24/50: error = 0.4344402612786481, alpha = 0.1318787409307583\n",
      "Classifier 25/50: error = 0.4256169470130622, alpha = 0.14987837852038818\n",
      "Classifier 26/50: error = 0.4589568826794248, alpha = 0.08227135340285849\n",
      "Classifier 27/50: error = 0.4629408849340251, alpha = 0.07425440236779074\n",
      "Classifier 28/50: error = 0.44272670203785314, alpha = 0.11505156295733705\n",
      "Classifier 29/50: error = 0.4484935519554769, alpha = 0.10337961296471433\n",
      "Classifier 30/50: error = 0.4679418342408136, alpha = 0.06420440754493106\n",
      "Classifier 31/50: error = 0.42631739718652195, alpha = 0.14844607694307563\n",
      "Classifier 32/50: error = 0.43462471702787997, alpha = 0.13150339449608317\n",
      "Classifier 33/50: error = 0.46646879037745614, alpha = 0.06716322618578584\n",
      "Classifier 34/50: error = 0.4600978551510272, alpha = 0.07997435720681753\n",
      "Classifier 35/50: error = 0.46207033072179526, alpha = 0.07600535742091061\n",
      "Classifier 36/50: error = 0.43474231153780885, alpha = 0.13126412230150034\n",
      "Classifier 37/50: error = 0.4443974473842158, alpha = 0.11166694568819603\n",
      "Classifier 38/50: error = 0.4454778980571209, alpha = 0.10947951552276507\n",
      "Classifier 39/50: error = 0.4421152029785145, alpha = 0.1162909978643208\n",
      "Classifier 40/50: error = 0.4583720439940544, alpha = 0.0834490800918856\n",
      "Classifier 41/50: error = 0.45858660747898694, alpha = 0.08301697336539669\n",
      "Classifier 42/50: error = 0.44897359098717715, alpha = 0.10240933429701005\n",
      "Classifier 43/50: error = 0.45983769632559635, alpha = 0.0804980317892042\n",
      "Classifier 44/50: error = 0.46022004385899395, alpha = 0.07972841826499542\n",
      "Classifier 45/50: error = 0.46661880827202684, alpha = 0.06686184101176972\n",
      "Classifier 46/50: error = 0.46160382333002387, alpha = 0.0769438395279247\n",
      "Classifier 47/50: error = 0.459900749232125, alpha = 0.08037110834569604\n",
      "Classifier 48/50: error = 0.4726141204050164, alpha = 0.054826628742398784\n",
      "Classifier 49/50: error = 0.4582674989636236, alpha = 0.08365963328039912\n",
      "Classifier 50/50: error = 0.4294416456252089, alpha = 0.14206479201152258\n",
      "Accuracy for digit 5: 0.8584\n",
      "Running AdaBoost for digit: 6\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2731806271659201, alpha = 0.4892723882707189\n",
      "Classifier 2/50: error = 0.2496786301782021, alpha = 0.5501634979720457\n",
      "Classifier 3/50: error = 0.4054183470095548, alpha = 0.1914692822007747\n",
      "Classifier 4/50: error = 0.3655929310221461, alpha = 0.27558515440269116\n",
      "Classifier 5/50: error = 0.28941183789093655, alpha = 0.4491211529651399\n",
      "Classifier 6/50: error = 0.34373244178432527, alpha = 0.3233524997775002\n",
      "Classifier 7/50: error = 0.3842401655298212, alpha = 0.2357946100758624\n",
      "Classifier 8/50: error = 0.309221917439646, alpha = 0.40187970971213954\n",
      "Classifier 9/50: error = 0.35252561316015496, alpha = 0.30397797729905807\n",
      "Classifier 10/50: error = 0.39653589817157164, alpha = 0.20995999038601815\n",
      "Classifier 11/50: error = 0.433925913038818, alpha = 0.1329255765149982\n",
      "Classifier 12/50: error = 0.4052255475626856, alpha = 0.1918692212070118\n",
      "Classifier 13/50: error = 0.35066808381565007, alpha = 0.3080519335062141\n",
      "Classifier 14/50: error = 0.38216611160794467, alpha = 0.2401821354944238\n",
      "Classifier 15/50: error = 0.33729310364989595, alpha = 0.33769025461264784\n",
      "Classifier 16/50: error = 0.4461050024358707, alpha = 0.10821038893791579\n",
      "Classifier 17/50: error = 0.4407945474245185, alpha = 0.11896902672993971\n",
      "Classifier 18/50: error = 0.42060467094986476, alpha = 0.16014582744242614\n",
      "Classifier 19/50: error = 0.4341438173657116, alpha = 0.13248204797518973\n",
      "Classifier 20/50: error = 0.33454831454202405, alpha = 0.3438423644796041\n",
      "Classifier 21/50: error = 0.4346782096006703, alpha = 0.13139455010037976\n",
      "Classifier 22/50: error = 0.4022178806513641, alpha = 0.1981162025490663\n",
      "Classifier 23/50: error = 0.42304159149448967, alpha = 0.15514984113770744\n",
      "Classifier 24/50: error = 0.38384972605731915, alpha = 0.23661987364360804\n",
      "Classifier 25/50: error = 0.43955894447421984, alpha = 0.12147612379377912\n",
      "Classifier 26/50: error = 0.4515207316039884, alpha = 0.09726409653859223\n",
      "Classifier 27/50: error = 0.45183165497218525, alpha = 0.09663638649169619\n",
      "Classifier 28/50: error = 0.44167125801132473, alpha = 0.11719104350527729\n",
      "Classifier 29/50: error = 0.4373856940204668, alpha = 0.12588946158167016\n",
      "Classifier 30/50: error = 0.42088503512367703, alpha = 0.15957064738410115\n",
      "Classifier 31/50: error = 0.4476512157810424, alpha = 0.1050826545860005\n",
      "Classifier 32/50: error = 0.42939962264482107, alpha = 0.14215054670208027\n",
      "Classifier 33/50: error = 0.4379465894332859, alpha = 0.12474996045768519\n",
      "Classifier 34/50: error = 0.43187574907068527, alpha = 0.13710110869843434\n",
      "Classifier 35/50: error = 0.3876196288059488, alpha = 0.22866454513172393\n",
      "Classifier 36/50: error = 0.46647773977757645, alpha = 0.0671452465460709\n",
      "Classifier 37/50: error = 0.44007246639718245, alpha = 0.12043398065891565\n",
      "Classifier 38/50: error = 0.42495605990827556, alpha = 0.15123034011936923\n",
      "Classifier 39/50: error = 0.42856997515943007, alpha = 0.14384400358081137\n",
      "Classifier 40/50: error = 0.43085743331884807, alpha = 0.13917685251122253\n",
      "Classifier 41/50: error = 0.40475319557500633, alpha = 0.19284931455302462\n",
      "Classifier 42/50: error = 0.4235377118835003, alpha = 0.1541336820061456\n",
      "Classifier 43/50: error = 0.4550640408464076, alpha = 0.09011506171135535\n",
      "Classifier 44/50: error = 0.43297383017604685, alpha = 0.13486408485843207\n",
      "Classifier 45/50: error = 0.4197991937029725, alpha = 0.16179888729665642\n",
      "Classifier 46/50: error = 0.4230271346141319, alpha = 0.15517945663274924\n",
      "Classifier 47/50: error = 0.44726486947643546, alpha = 0.10586397506798709\n",
      "Classifier 48/50: error = 0.422511589863875, alpha = 0.1562357475256151\n",
      "Classifier 49/50: error = 0.43968228859261504, alpha = 0.12122578494441529\n",
      "Classifier 50/50: error = 0.4551653866644377, alpha = 0.08991072336177781\n",
      "Accuracy for digit 6: 0.9437\n",
      "Running AdaBoost for digit: 7\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Classifier 1/50: error = 0.30185968552957165, alpha = 0.41922890700670434\n",
      "Classifier 2/50: error = 0.29105128858633955, alpha = 0.44514184165103693\n",
      "Classifier 3/50: error = 0.285635116287186, alpha = 0.45833934705609997\n",
      "Classifier 4/50: error = 0.3847717272891624, alpha = 0.23467156583441448\n",
      "Classifier 5/50: error = 0.3287052586571729, alpha = 0.357023409162681\n",
      "Classifier 6/50: error = 0.3237394094767029, alpha = 0.36831979505116863\n",
      "Classifier 7/50: error = 0.2863601926644609, alpha = 0.45656396499512436\n",
      "Classifier 8/50: error = 0.4025036787916454, alpha = 0.19752194503014853\n",
      "Classifier 9/50: error = 0.35256439241800885, alpha = 0.3038930308268118\n",
      "Classifier 10/50: error = 0.40669800258868505, alpha = 0.18881631907889143\n",
      "Classifier 11/50: error = 0.41612692324841705, alpha = 0.1693466535600607\n",
      "Classifier 12/50: error = 0.3632278455568439, alpha = 0.28069079753313153\n",
      "Classifier 13/50: error = 0.4126829857062956, alpha = 0.17644251163707372\n",
      "Classifier 14/50: error = 0.35432372895495556, alpha = 0.3000436329090136\n",
      "Classifier 15/50: error = 0.4109969365932147, alpha = 0.17992281183537887\n",
      "Classifier 16/50: error = 0.4278810079319946, alpha = 0.1452509298500205\n",
      "Classifier 17/50: error = 0.376029694462356, alpha = 0.25321733247327005\n",
      "Classifier 18/50: error = 0.428834048120708, alpha = 0.14330489563263846\n",
      "Classifier 19/50: error = 0.40522116634066574, alpha = 0.19187831022348598\n",
      "Classifier 20/50: error = 0.34539564843707626, alpha = 0.31967022128047173\n",
      "Classifier 21/50: error = 0.37843781511824637, alpha = 0.2480920977688832\n",
      "Classifier 22/50: error = 0.4389538121847907, alpha = 0.12270451859480205\n",
      "Classifier 23/50: error = 0.42935518872691136, alpha = 0.1422412235596906\n",
      "Classifier 24/50: error = 0.392393623103784, alpha = 0.21863089462386928\n",
      "Classifier 25/50: error = 0.43698664120217556, alpha = 0.1267003651962454\n",
      "Classifier 26/50: error = 0.4099828110766439, alpha = 0.1820182177765786\n",
      "Classifier 27/50: error = 0.4400398915770175, alpha = 0.12050008035325108\n",
      "Classifier 28/50: error = 0.3916114114451241, alpha = 0.22027187641868778\n",
      "Classifier 29/50: error = 0.43806313426791876, alpha = 0.12451323141237253\n",
      "Classifier 30/50: error = 0.4269477404450366, alpha = 0.14715764894297664\n",
      "Classifier 31/50: error = 0.3992417124134857, alpha = 0.2043128204224976\n",
      "Classifier 32/50: error = 0.39825453156258267, alpha = 0.20637160924816833\n",
      "Classifier 33/50: error = 0.44239842492225645, alpha = 0.11571689712051358\n",
      "Classifier 34/50: error = 0.4577028437986226, alpha = 0.08479697448170895\n",
      "Classifier 35/50: error = 0.4500858242187976, alpha = 0.10016196845506319\n",
      "Classifier 36/50: error = 0.45790058154264845, alpha = 0.08439866189474642\n",
      "Classifier 37/50: error = 0.4378337422899616, alpha = 0.12497919190715934\n",
      "Classifier 38/50: error = 0.4431495828685622, alpha = 0.11419464049361414\n",
      "Classifier 39/50: error = 0.4530915140981676, alpha = 0.09409368180294295\n",
      "Classifier 40/50: error = 0.4202968747596485, alpha = 0.1607774065767353\n",
      "Classifier 41/50: error = 0.44049976669711755, alpha = 0.11956701447204104\n",
      "Classifier 42/50: error = 0.45508169524787334, alpha = 0.09007946551223552\n",
      "Classifier 43/50: error = 0.42283597598968226, alpha = 0.15557107810038628\n",
      "Classifier 44/50: error = 0.4378613989460741, alpha = 0.1249230104967486\n",
      "Classifier 45/50: error = 0.4424666235110071, alpha = 0.11557866756392329\n",
      "Classifier 46/50: error = 0.44586319989136913, alpha = 0.10869970477802736\n",
      "Classifier 47/50: error = 0.4573082467829286, alpha = 0.08559191046923283\n",
      "Classifier 48/50: error = 0.4417803066067181, alpha = 0.11696994298635655\n",
      "Classifier 49/50: error = 0.4513429396072599, alpha = 0.09762306759523624\n",
      "Classifier 50/50: error = 0.456309064549009, alpha = 0.08760529947298099\n",
      "Accuracy for digit 7: 0.9304\n",
      "Running AdaBoost for digit: 8\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3569780360652935, alpha = 0.29425231289956627\n",
      "Classifier 2/50: error = 0.41275291184806007, alpha = 0.17629826374114577\n",
      "Classifier 3/50: error = 0.40409672093857185, alpha = 0.194212056395488\n",
      "Classifier 4/50: error = 0.4283539979035462, alpha = 0.1442849853267675\n",
      "Classifier 5/50: error = 0.4419221627952371, alpha = 0.11668234075389425\n",
      "Classifier 6/50: error = 0.31244709277690697, alpha = 0.3943518153184258\n",
      "Classifier 7/50: error = 0.38504025075640763, alpha = 0.23410447047042557\n",
      "Classifier 8/50: error = 0.4228031282767447, alpha = 0.15563837707827674\n",
      "Classifier 9/50: error = 0.44461644563175595, alpha = 0.11122348669516187\n",
      "Classifier 10/50: error = 0.3451919769415412, alpha = 0.3201206903998353\n",
      "Classifier 11/50: error = 0.3953473764600115, alpha = 0.21244465228632686\n",
      "Classifier 12/50: error = 0.4415693635663094, alpha = 0.11739764899755148\n",
      "Classifier 13/50: error = 0.4037064052838723, alpha = 0.1950226300497944\n",
      "Classifier 14/50: error = 0.41474473186621674, alpha = 0.1721924403652611\n",
      "Classifier 15/50: error = 0.4056645040265277, alpha = 0.1909587473240338\n",
      "Classifier 16/50: error = 0.4017423340903225, alpha = 0.19910530868627965\n",
      "Classifier 17/50: error = 0.38603124731413907, alpha = 0.2320128587926963\n",
      "Classifier 18/50: error = 0.4450108680723028, alpha = 0.11042491344775716\n",
      "Classifier 19/50: error = 0.4142311643645159, alpha = 0.17325052360768414\n",
      "Classifier 20/50: error = 0.4370571319457093, alpha = 0.12655711098503616\n",
      "Classifier 21/50: error = 0.4383696608156929, alpha = 0.12389067253556536\n",
      "Classifier 22/50: error = 0.43544525580254195, alpha = 0.12983413644622763\n",
      "Classifier 23/50: error = 0.4361385601215376, alpha = 0.12842427799163597\n",
      "Classifier 24/50: error = 0.464279125412039, alpha = 0.07156366714404933\n",
      "Classifier 25/50: error = 0.4255812863966082, alpha = 0.14995131470378908\n",
      "Classifier 26/50: error = 0.4650656271118704, alpha = 0.06998277132098031\n",
      "Classifier 27/50: error = 0.42719310774846403, alpha = 0.1466562470541709\n",
      "Classifier 28/50: error = 0.44292627831809384, alpha = 0.11464712223136075\n",
      "Classifier 29/50: error = 0.44699700924652885, alpha = 0.1064057529157072\n",
      "Classifier 30/50: error = 0.43721536562231267, alpha = 0.12623556075435696\n",
      "Classifier 31/50: error = 0.42842140709270693, alpha = 0.144147343461491\n",
      "Classifier 32/50: error = 0.45694802755945607, alpha = 0.08631768483166034\n",
      "Classifier 33/50: error = 0.447334928290803, alpha = 0.10572228335711117\n",
      "Classifier 34/50: error = 0.4677853969985085, alpha = 0.06451857987445941\n",
      "Classifier 35/50: error = 0.4669898292347595, alpha = 0.06611651378861501\n",
      "Classifier 36/50: error = 0.45869541195784735, alpha = 0.0827978652109265\n",
      "Classifier 37/50: error = 0.4500551613562528, alpha = 0.10022391186890461\n",
      "Classifier 38/50: error = 0.45988991136251733, alpha = 0.0803929244395797\n",
      "Classifier 39/50: error = 0.4651168879705515, alpha = 0.06987974741545608\n",
      "Classifier 40/50: error = 0.4369439944100778, alpha = 0.12678703628611454\n",
      "Classifier 41/50: error = 0.46937616114370095, alpha = 0.06132443622922769\n",
      "Classifier 42/50: error = 0.44793570446314823, alpha = 0.10450740579086068\n",
      "Classifier 43/50: error = 0.45152380168096096, alpha = 0.09725789811724067\n",
      "Classifier 44/50: error = 0.46160372022971935, alpha = 0.07694404695172867\n",
      "Classifier 45/50: error = 0.45679221268391573, alpha = 0.08663165072581407\n",
      "Classifier 46/50: error = 0.4477213422930734, alpha = 0.10494084923247236\n",
      "Classifier 47/50: error = 0.4638753237325166, alpha = 0.0723754608510698\n",
      "Classifier 48/50: error = 0.46377419757877536, alpha = 0.07257877743855394\n",
      "Classifier 49/50: error = 0.4712242015742615, alpha = 0.057615263951185824\n",
      "Classifier 50/50: error = 0.4503279804024225, alpha = 0.09967280484170524\n",
      "Accuracy for digit 8: 0.8628\n",
      "Running AdaBoost for digit: 9\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Classifier 1/50: error = 0.3640107581106067, alpha = 0.27899911268120686\n",
      "Classifier 2/50: error = 0.3390135477928, alpha = 0.3338466365105494\n",
      "Classifier 3/50: error = 0.35373239227389625, alpha = 0.3013364990467222\n",
      "Classifier 4/50: error = 0.41824563604992465, alpha = 0.164989698762225\n",
      "Classifier 5/50: error = 0.3131658478536973, alpha = 0.3926799692673436\n",
      "Classifier 6/50: error = 0.41150534679968304, alpha = 0.17887291382219733\n",
      "Classifier 7/50: error = 0.4152475504828177, alpha = 0.1711568724031097\n",
      "Classifier 8/50: error = 0.45965279824515054, alpha = 0.0808702405297161\n",
      "Classifier 9/50: error = 0.31415740511673296, alpha = 0.39037699860848746\n",
      "Classifier 10/50: error = 0.3939951372297498, alpha = 0.2152747215781884\n",
      "Classifier 11/50: error = 0.40302554567693855, alpha = 0.1964371868018115\n",
      "Classifier 12/50: error = 0.45163650964902047, alpha = 0.09703034823436349\n",
      "Classifier 13/50: error = 0.41342521874453253, alpha = 0.17491175776331291\n",
      "Classifier 14/50: error = 0.43357620272280006, alpha = 0.13363749540484207\n",
      "Classifier 15/50: error = 0.42029064637261937, alpha = 0.1607901881607291\n",
      "Classifier 16/50: error = 0.4215316280537964, alpha = 0.15824453051262252\n",
      "Classifier 17/50: error = 0.39258337260447673, alpha = 0.21823299895336357\n",
      "Classifier 18/50: error = 0.41830934679826226, alpha = 0.16485877986205752\n",
      "Classifier 19/50: error = 0.4095661963495886, alpha = 0.18287949260081152\n",
      "Classifier 20/50: error = 0.4501618374790013, alpha = 0.10000841398618404\n",
      "Classifier 21/50: error = 0.4450856605501743, alpha = 0.11027349959822803\n",
      "Classifier 22/50: error = 0.42777409882033, alpha = 0.14546929786977328\n",
      "Classifier 23/50: error = 0.4251716926053427, alpha = 0.1507891651829512\n",
      "Classifier 24/50: error = 0.4610485276397476, alpha = 0.07806111530563102\n",
      "Classifier 25/50: error = 0.43460875291463175, alpha = 0.1315358781900191\n",
      "Classifier 26/50: error = 0.4565957387650669, alpha = 0.08702756860483143\n",
      "Classifier 27/50: error = 0.4474616529353965, alpha = 0.10546599755698931\n",
      "Classifier 28/50: error = 0.4578967556118556, alpha = 0.08440636839610668\n",
      "Classifier 29/50: error = 0.44848062055908966, alpha = 0.10340575321853354\n",
      "Classifier 30/50: error = 0.4644543132840697, alpha = 0.07121150278078243\n",
      "Classifier 31/50: error = 0.4475920479176126, alpha = 0.105202303335221\n",
      "Classifier 32/50: error = 0.44566925869391305, alpha = 0.10909220502487946\n",
      "Classifier 33/50: error = 0.4294207601378852, alpha = 0.1421074119680521\n",
      "Classifier 34/50: error = 0.43516875195671667, alpha = 0.1303965595288505\n",
      "Classifier 35/50: error = 0.46037046592289443, alpha = 0.0794256650131325\n",
      "Classifier 36/50: error = 0.44099503518942185, alpha = 0.11856236867567521\n",
      "Classifier 37/50: error = 0.4387573321742759, alpha = 0.12310344438250459\n",
      "Classifier 38/50: error = 0.45225174855873124, alpha = 0.09578839777638821\n",
      "Classifier 39/50: error = 0.45677993317136134, alpha = 0.08665639458193143\n",
      "Classifier 40/50: error = 0.4549927293189021, alpha = 0.09025884796641997\n",
      "Classifier 41/50: error = 0.4808082608212687, alpha = 0.0384023450465974\n",
      "Classifier 42/50: error = 0.4705387571884141, alpha = 0.058990818206085927\n",
      "Classifier 43/50: error = 0.46064954831503513, alpha = 0.07886399661746915\n",
      "Classifier 44/50: error = 0.4517588719107326, alpha = 0.09678331831476744\n",
      "Classifier 45/50: error = 0.4454636562418317, alpha = 0.10950834200819451\n",
      "Classifier 46/50: error = 0.4581916114173874, alpha = 0.08381247506824353\n",
      "Classifier 47/50: error = 0.453125402125682, alpha = 0.09402530434995861\n",
      "Classifier 48/50: error = 0.46376006372413825, alpha = 0.07260719437319828\n",
      "Classifier 49/50: error = 0.43001173358717315, alpha = 0.1409016397958043\n",
      "Classifier 50/50: error = 0.4576211768847316, alpha = 0.08496148787439232\n",
      "Accuracy for digit 9: 0.869\n",
      "Accuracies for all digits: {0: 0.9503, 1: 0.9622, 2: 0.9113, 3: 0.8811, 4: 0.9132, 5: 0.8584, 6: 0.9437, 7: 0.9304, 8: 0.8628, 9: 0.869}\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=50, A=20, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Classifier 1/50: error = 0.4041367665681721, alpha = 0.19412890738154484\n",
      "Classifier 2/50: error = 0.3923853820853689, alpha = 0.21864817719050703\n",
      "Classifier 3/50: error = 0.3316592696588827, alpha = 0.3503449864978734\n",
      "Classifier 4/50: error = 0.42129669262611685, alpha = 0.15872630256212605\n",
      "Classifier 5/50: error = 0.4676664848826072, alpha = 0.06475739913816118\n",
      "Classifier 6/50: error = 0.3009617329307832, alpha = 0.4213611812245938\n",
      "Classifier 7/50: error = 0.339161624307877, alpha = 0.33351626666949763\n",
      "Classifier 8/50: error = 0.1565972245519484, alpha = 0.8418837856699998\n",
      "Classifier 9/50: error = 0.3723021759921057, alpha = 0.26117652650366924\n",
      "Classifier 10/50: error = 0.4351921779980539, alpha = 0.13034890657691928\n",
      "Classifier 11/50: error = 0.23390254936659405, alpha = 0.5932024042911099\n",
      "Classifier 12/50: error = 0.40274457439436284, alpha = 0.1970211599140868\n",
      "Classifier 13/50: error = 0.4157525331767733, alpha = 0.17011721324070667\n",
      "Classifier 14/50: error = 0.41140497729783276, alpha = 0.1790801519711057\n",
      "Classifier 15/50: error = 0.43465444539698844, alpha = 0.13144290410178983\n",
      "Classifier 16/50: error = 0.47544510424373604, alpha = 0.049149329265955576\n",
      "Classifier 17/50: error = 0.3972957574032425, alpha = 0.20837280775241274\n",
      "Classifier 18/50: error = 0.4335572020268328, alpha = 0.133676179711423\n",
      "Classifier 19/50: error = 0.3958145078870593, alpha = 0.2114677848799708\n",
      "Classifier 20/50: error = 0.4265121162209985, alpha = 0.1480480172329107\n",
      "Classifier 21/50: error = 0.42032413107916844, alpha = 0.1607214731352161\n",
      "Classifier 22/50: error = 0.37928259201738834, alpha = 0.24629718301646902\n",
      "Classifier 23/50: error = 0.43367729350876677, alpha = 0.1334316871544832\n",
      "Classifier 24/50: error = 0.3885571887892276, alpha = 0.22669053272700754\n",
      "Classifier 25/50: error = 0.371131385978153, alpha = 0.26368310701277053\n",
      "Classifier 26/50: error = 0.4253586690338243, alpha = 0.15040666686007761\n",
      "Classifier 27/50: error = 0.41967586174089916, alpha = 0.162052075439514\n",
      "Classifier 28/50: error = 0.36440866315733667, alpha = 0.27813993285138455\n",
      "Classifier 29/50: error = 0.4091552115901531, alpha = 0.1837293914899117\n",
      "Classifier 30/50: error = 0.4431797345671973, alpha = 0.11413354771507461\n",
      "Classifier 31/50: error = 0.3290509469320504, alpha = 0.356240307193638\n",
      "Classifier 32/50: error = 0.38695521486855977, alpha = 0.23006451481740528\n",
      "Classifier 33/50: error = 0.392892205322275, alpha = 0.21758553681458323\n",
      "Classifier 34/50: error = 0.39455001072023654, alpha = 0.21411303163401094\n",
      "Classifier 35/50: error = 0.4033624668633975, alpha = 0.19573710127106875\n",
      "Classifier 36/50: error = 0.4175409139907226, alpha = 0.16643820361124245\n",
      "Classifier 37/50: error = 0.4519126459181818, alpha = 0.09647288977620018\n",
      "Classifier 38/50: error = 0.3710334160013009, alpha = 0.26389300047235453\n",
      "Classifier 39/50: error = 0.3563822738615645, alpha = 0.29555049953021284\n",
      "Classifier 40/50: error = 0.4231176187519391, alpha = 0.15499410071572078\n",
      "Classifier 41/50: error = 0.39975798873118484, alpha = 0.20323679504507447\n",
      "Classifier 42/50: error = 0.40969426936447395, alpha = 0.18261469653481696\n",
      "Classifier 43/50: error = 0.43211072022132435, alpha = 0.13662230874153444\n",
      "Classifier 44/50: error = 0.41590301164636423, alpha = 0.1698074784600225\n",
      "Classifier 45/50: error = 0.4280824727817085, alpha = 0.14483946365341827\n",
      "Classifier 46/50: error = 0.4475713668322061, alpha = 0.10524412515753596\n",
      "Classifier 47/50: error = 0.4568975989178443, alpha = 0.08641929633405898\n",
      "Classifier 48/50: error = 0.4219179218426877, alpha = 0.15745253257171818\n",
      "Classifier 49/50: error = 0.4011532310985546, alpha = 0.20033113851717696\n",
      "Classifier 50/50: error = 0.4348275906438404, alpha = 0.13109061233567676\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2630720166135132, alpha = 0.5150311745776066\n",
      "Classifier 2/50: error = 0.35400568434507584, alpha = 0.30073886692180163\n",
      "Classifier 3/50: error = 0.3943801376831134, alpha = 0.21446861931490846\n",
      "Classifier 4/50: error = 0.30015781653420115, alpha = 0.4232732328957671\n",
      "Classifier 5/50: error = 0.39302320184470707, alpha = 0.2173109592714949\n",
      "Classifier 6/50: error = 0.41258153142757675, alpha = 0.1766518106139813\n",
      "Classifier 7/50: error = 0.3161351311446017, alpha = 0.3857952926991718\n",
      "Classifier 8/50: error = 0.2818884888738997, alpha = 0.46755665076934955\n",
      "Classifier 9/50: error = 0.40782568135250086, alpha = 0.18648060855965362\n",
      "Classifier 10/50: error = 0.10084484033627916, alpha = 1.0939362543822106\n",
      "Classifier 11/50: error = 0.3858740190110238, alpha = 0.23234457359716335\n",
      "Classifier 12/50: error = 0.38319335314491054, alpha = 0.23800794907754907\n",
      "Classifier 13/50: error = 0.42057880047154905, alpha = 0.16019890721366162\n",
      "Classifier 14/50: error = 0.417260515766785, alpha = 0.16701473391503946\n",
      "Classifier 15/50: error = 0.4178245431403095, alpha = 0.16585514167344886\n",
      "Classifier 16/50: error = 0.43506095085821517, alpha = 0.1306158547302831\n",
      "Classifier 17/50: error = 0.405489387113686, alpha = 0.19132193340568396\n",
      "Classifier 18/50: error = 0.4259526461256239, alpha = 0.14919185506700705\n",
      "Classifier 19/50: error = 0.4252491237687487, alpha = 0.1506307586794611\n",
      "Classifier 20/50: error = 0.43399684747020434, alpha = 0.13278118889703833\n",
      "Classifier 21/50: error = 0.42575132896628964, alpha = 0.14960354311625604\n",
      "Classifier 22/50: error = 0.3923818997322003, alpha = 0.2186554802101246\n",
      "Classifier 23/50: error = 0.40224920881547716, alpha = 0.198051055443794\n",
      "Classifier 24/50: error = 0.4137967763750069, alpha = 0.17414577619820562\n",
      "Classifier 25/50: error = 0.42069258530692033, alpha = 0.1599654557861387\n",
      "Classifier 26/50: error = 0.39982114283403425, alpha = 0.2031052008964138\n",
      "Classifier 27/50: error = 0.4355949425571934, alpha = 0.1295296999639297\n",
      "Classifier 28/50: error = 0.4224282844609276, alpha = 0.15640646295261748\n",
      "Classifier 29/50: error = 0.43904835697047057, alpha = 0.12251257219424004\n",
      "Classifier 30/50: error = 0.41718892143113545, alpha = 0.1671619575429149\n",
      "Classifier 31/50: error = 0.4601547944086092, alpha = 0.07985974982990224\n",
      "Classifier 32/50: error = 0.44386606388292116, alpha = 0.11274314924457134\n",
      "Classifier 33/50: error = 0.4000895692922958, alpha = 0.20254595828186048\n",
      "Classifier 34/50: error = 0.3821683542863622, alpha = 0.24017738638012093\n",
      "Classifier 35/50: error = 0.4517901388724437, alpha = 0.0967201971885453\n",
      "Classifier 36/50: error = 0.467541033886412, alpha = 0.06500935887535646\n",
      "Classifier 37/50: error = 0.4374611149916048, alpha = 0.1257362193569265\n",
      "Classifier 38/50: error = 0.3843691449781595, alpha = 0.2355220583055758\n",
      "Classifier 39/50: error = 0.4314183509611037, alpha = 0.1380333267472529\n",
      "Classifier 40/50: error = 0.45451962178992567, alpha = 0.09121287468239209\n",
      "Classifier 41/50: error = 0.4347343435403948, alpha = 0.13128033449278628\n",
      "Classifier 42/50: error = 0.44522753979254187, alpha = 0.1099862855839999\n",
      "Classifier 43/50: error = 0.45551909442819527, alpha = 0.08919761918491775\n",
      "Classifier 44/50: error = 0.39145920019269864, alpha = 0.22059133198113962\n",
      "Classifier 45/50: error = 0.45870415596030334, alpha = 0.08278025706863187\n",
      "Classifier 46/50: error = 0.41587677798013456, alpha = 0.16986147375341237\n",
      "Classifier 47/50: error = 0.31586741742082824, alpha = 0.3864145862139568\n",
      "Classifier 48/50: error = 0.3983796124230876, alpha = 0.20611065506840184\n",
      "Classifier 49/50: error = 0.431839528198227, alpha = 0.13717492140558\n",
      "Classifier 50/50: error = 0.41494487806347546, alpha = 0.17178019039498704\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Classifier 1/50: error = 0.27139979859013086, alpha = 0.4937660777456382\n",
      "Classifier 2/50: error = 0.39243221320282196, alpha = 0.21854996750665093\n",
      "Classifier 3/50: error = 0.39908231297938046, alpha = 0.2046451356158315\n",
      "Classifier 4/50: error = 0.39744990409006853, alpha = 0.20805095492762604\n",
      "Classifier 5/50: error = 0.4046327245535491, alpha = 0.19309934098968295\n",
      "Classifier 6/50: error = 0.3791929344994467, alpha = 0.24648760621328744\n",
      "Classifier 7/50: error = 0.45307864113648666, alpha = 0.09411965640775483\n",
      "Classifier 8/50: error = 0.4120672442817721, alpha = 0.17771301497761088\n",
      "Classifier 9/50: error = 0.31321592734459003, alpha = 0.3925635607537369\n",
      "Classifier 10/50: error = 0.39017853327002167, alpha = 0.22328091237693287\n",
      "Classifier 11/50: error = 0.4252385062805878, alpha = 0.1506524791950725\n",
      "Classifier 12/50: error = 0.39304348622348473, alpha = 0.2172684447313426\n",
      "Classifier 13/50: error = 0.43439302706059235, alpha = 0.1319748631209733\n",
      "Classifier 14/50: error = 0.4504982771708588, alpha = 0.09932882814228781\n",
      "Classifier 15/50: error = 0.4207605315287845, alpha = 0.15982605931725008\n",
      "Classifier 16/50: error = 0.4110810512059221, alpha = 0.1797490830076497\n",
      "Classifier 17/50: error = 0.397782205161398, alpha = 0.207357266475243\n",
      "Classifier 18/50: error = 0.3718728268951871, alpha = 0.2620953590807392\n",
      "Classifier 19/50: error = 0.35406984820290077, alpha = 0.30059858433898223\n",
      "Classifier 20/50: error = 0.39017126757243964, alpha = 0.22329618039615562\n",
      "Classifier 21/50: error = 0.42104542516373844, alpha = 0.15924164690166356\n",
      "Classifier 22/50: error = 0.43647735702067925, alpha = 0.1277355076932556\n",
      "Classifier 23/50: error = 0.4311768740779142, alpha = 0.1385255741911072\n",
      "Classifier 24/50: error = 0.4366410688541018, alpha = 0.1274027266025437\n",
      "Classifier 25/50: error = 0.45986853556394647, alpha = 0.0804359530855566\n",
      "Classifier 26/50: error = 0.42970572946097274, alpha = 0.14152593366694285\n",
      "Classifier 27/50: error = 0.43312179621706587, alpha = 0.1345627497201763\n",
      "Classifier 28/50: error = 0.40830007814311253, alpha = 0.18549861391463868\n",
      "Classifier 29/50: error = 0.41472861449803, alpha = 0.17222564054081052\n",
      "Classifier 30/50: error = 0.44356875419284275, alpha = 0.11334539971084946\n",
      "Classifier 31/50: error = 0.44563806067771694, alpha = 0.10915534702308698\n",
      "Classifier 32/50: error = 0.4596108633192844, alpha = 0.08095466066198703\n",
      "Classifier 33/50: error = 0.46384539538281144, alpha = 0.07243563190177228\n",
      "Classifier 34/50: error = 0.40964946107436206, alpha = 0.18270733657929508\n",
      "Classifier 35/50: error = 0.41961107063549025, alpha = 0.16218509332833925\n",
      "Classifier 36/50: error = 0.4447958104433827, alpha = 0.11086031561299393\n",
      "Classifier 37/50: error = 0.38704541326082986, alpha = 0.22987440820069752\n",
      "Classifier 38/50: error = 0.4058274134153472, alpha = 0.19062092388679572\n",
      "Classifier 39/50: error = 0.4343648059808742, alpha = 0.13203229450770182\n",
      "Classifier 40/50: error = 0.4349504191141693, alpha = 0.13084071773945338\n",
      "Classifier 41/50: error = 0.4482876472970935, alpha = 0.10379585700179786\n",
      "Classifier 42/50: error = 0.44101625993242055, alpha = 0.11851931989317968\n",
      "Classifier 43/50: error = 0.43683399325141103, alpha = 0.12701060042745763\n",
      "Classifier 44/50: error = 0.42308764672701793, alpha = 0.15505549695990556\n",
      "Classifier 45/50: error = 0.4545645467110983, alpha = 0.09112227597935174\n",
      "Classifier 46/50: error = 0.42732433214329096, alpha = 0.146388123378299\n",
      "Classifier 47/50: error = 0.45021465608786626, alpha = 0.09990171782572758\n",
      "Classifier 48/50: error = 0.436379107738566, alpha = 0.12793523494472664\n",
      "Classifier 49/50: error = 0.4305503118849271, alpha = 0.13980312471969347\n",
      "Classifier 50/50: error = 0.4586607514115102, alpha = 0.08286766302293204\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.4338499184339315, alpha = 0.13308027027170158\n",
      "Classifier 2/50: error = 0.4303924391190719, alpha = 0.14012509619031027\n",
      "Classifier 3/50: error = 0.4090499541747997, alpha = 0.18394710149831264\n",
      "Classifier 4/50: error = 0.4496870015628588, alpha = 0.10096770785546262\n",
      "Classifier 5/50: error = 0.3978253705315264, alpha = 0.20726717192114186\n",
      "Classifier 6/50: error = 0.4028161760064852, alpha = 0.19687232993068593\n",
      "Classifier 7/50: error = 0.33463946100036757, alpha = 0.3436376705255776\n",
      "Classifier 8/50: error = 0.3949816726498959, alpha = 0.2132096925773992\n",
      "Classifier 9/50: error = 0.4619756274535767, alpha = 0.07619586298764429\n",
      "Classifier 10/50: error = 0.45981905046719584, alpha = 0.0805355657894124\n",
      "Classifier 11/50: error = 0.4611342493698831, alpha = 0.07788862734060648\n",
      "Classifier 12/50: error = 0.4542304685598505, alpha = 0.09179603688614486\n",
      "Classifier 13/50: error = 0.30214396402888544, alpha = 0.4185546116202407\n",
      "Classifier 14/50: error = 0.41569748545365615, alpha = 0.17023052785276255\n",
      "Classifier 15/50: error = 0.4547189461284793, alpha = 0.09081091480004388\n",
      "Classifier 16/50: error = 0.41325414693358514, alpha = 0.1752644977119747\n",
      "Classifier 17/50: error = 0.3722972822015629, alpha = 0.2611869970741689\n",
      "Classifier 18/50: error = 0.41778174246811206, alpha = 0.16594312067624284\n",
      "Classifier 19/50: error = 0.4414239386718046, alpha = 0.11769253595848628\n",
      "Classifier 20/50: error = 0.36210294791554787, alpha = 0.28312417596678824\n",
      "Classifier 21/50: error = 0.3094042979297853, alpha = 0.401452866033917\n",
      "Classifier 22/50: error = 0.4643729356330364, alpha = 0.07137508672756332\n",
      "Classifier 23/50: error = 0.45158922282324876, alpha = 0.09712581596598038\n",
      "Classifier 24/50: error = 0.4578267845159698, alpha = 0.08454731164053696\n",
      "Classifier 25/50: error = 0.4543683508145937, alpha = 0.09151794918131344\n",
      "Classifier 26/50: error = 0.4410916993352272, alpha = 0.11836631453726139\n",
      "Classifier 27/50: error = 0.4513012918609065, alpha = 0.09770716012926353\n",
      "Classifier 28/50: error = 0.45757927431624734, alpha = 0.08504590001602208\n",
      "Classifier 29/50: error = 0.4262388044076877, alpha = 0.14860675560270045\n",
      "Classifier 30/50: error = 0.47717591413206095, alpha = 0.04567991797127373\n",
      "Classifier 31/50: error = 0.4495664119933328, alpha = 0.10121136003478044\n",
      "Classifier 32/50: error = 0.4478836372565542, alpha = 0.10461268284011493\n",
      "Classifier 33/50: error = 0.4587574735587472, alpha = 0.08267289042755016\n",
      "Classifier 34/50: error = 0.4752263492307841, alpha = 0.04958790649121094\n",
      "Classifier 35/50: error = 0.45832282855810824, alpha = 0.08354819882248186\n",
      "Classifier 36/50: error = 0.4427111524612562, alpha = 0.11508307569903878\n",
      "Classifier 37/50: error = 0.41341647814213645, alpha = 0.17492977932290313\n",
      "Classifier 38/50: error = 0.45768052546334537, alpha = 0.08484193305448307\n",
      "Classifier 39/50: error = 0.39020828795123685, alpha = 0.22321838744001896\n",
      "Classifier 40/50: error = 0.34601708958222455, alpha = 0.3182965272129452\n",
      "Classifier 41/50: error = 0.450743044043151, alpha = 0.098834472803163\n",
      "Classifier 42/50: error = 0.44772455848107373, alpha = 0.10493434576341303\n",
      "Classifier 43/50: error = 0.4533230249269318, alpha = 0.09362656905198653\n",
      "Classifier 44/50: error = 0.3984657938269234, alpha = 0.20593087230393187\n",
      "Classifier 45/50: error = 0.45808810715834136, alpha = 0.08402094476902053\n",
      "Classifier 46/50: error = 0.44136827618076235, alpha = 0.11780541158824791\n",
      "Classifier 47/50: error = 0.4504424140514023, alpha = 0.09944166158731026\n",
      "Classifier 48/50: error = 0.46383095654713546, alpha = 0.07246466141810391\n",
      "Classifier 49/50: error = 0.47394367186686426, alpha = 0.0521599078896365\n",
      "Classifier 50/50: error = 0.48106354631835435, alpha = 0.03789103071349621\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.45502011469656756, alpha = 0.09020363007380615\n",
      "Classifier 2/50: error = 0.40753087159059953, alpha = 0.18709103965446439\n",
      "Classifier 3/50: error = 0.41047614528951937, alpha = 0.18099868461991997\n",
      "Classifier 4/50: error = 0.396930935241417, alpha = 0.20913471321508714\n",
      "Classifier 5/50: error = 0.4392247104476271, alpha = 0.12215456041582953\n",
      "Classifier 6/50: error = 0.1828569062403213, alpha = 0.7485551553003805\n",
      "Classifier 7/50: error = 0.36602223947267937, alpha = 0.2746598902659985\n",
      "Classifier 8/50: error = 0.27252967457696053, alpha = 0.49091285124160916\n",
      "Classifier 9/50: error = 0.3742729937551956, alpha = 0.25696436132205097\n",
      "Classifier 10/50: error = 0.3743363177654132, alpha = 0.25682916954933727\n",
      "Classifier 11/50: error = 0.38915887921816217, alpha = 0.2254246018551329\n",
      "Classifier 12/50: error = 0.39641482967295016, alpha = 0.2102129725798024\n",
      "Classifier 13/50: error = 0.439574841893156, alpha = 0.12144385759051127\n",
      "Classifier 14/50: error = 0.4160255391182802, alpha = 0.16955529997392\n",
      "Classifier 15/50: error = 0.39721513763519967, alpha = 0.20854115592028927\n",
      "Classifier 16/50: error = 0.41789900155788196, alpha = 0.16570209459417817\n",
      "Classifier 17/50: error = 0.44709096106893254, alpha = 0.10621571757590852\n",
      "Classifier 18/50: error = 0.44233738324970695, alpha = 0.11584062428429863\n",
      "Classifier 19/50: error = 0.42910876185550934, alpha = 0.14274415220805875\n",
      "Classifier 20/50: error = 0.46009938347001267, alpha = 0.07997128097794284\n",
      "Classifier 21/50: error = 0.43852528718131445, alpha = 0.12357463022668301\n",
      "Classifier 22/50: error = 0.42025661952160726, alpha = 0.16086001728031327\n",
      "Classifier 23/50: error = 0.42105906388037406, alpha = 0.15921367202700937\n",
      "Classifier 24/50: error = 0.44053182094983945, alpha = 0.119501985581278\n",
      "Classifier 25/50: error = 0.4266366824273925, alpha = 0.14779339379417766\n",
      "Classifier 26/50: error = 0.42919776668500775, alpha = 0.14256249541796556\n",
      "Classifier 27/50: error = 0.4391818335081281, alpha = 0.12224160118997494\n",
      "Classifier 28/50: error = 0.431632261657964, alpha = 0.13759732813373537\n",
      "Classifier 29/50: error = 0.4408593948065975, alpha = 0.11883748968547174\n",
      "Classifier 30/50: error = 0.47531155080894516, alpha = 0.049417085420871115\n",
      "Classifier 31/50: error = 0.4501599505321876, alpha = 0.1000122257525529\n",
      "Classifier 32/50: error = 0.4492450940246878, alpha = 0.10186064408689323\n",
      "Classifier 33/50: error = 0.3755636675883371, alpha = 0.2542106817831657\n",
      "Classifier 34/50: error = 0.4488666067942082, alpha = 0.1026255593435311\n",
      "Classifier 35/50: error = 0.4526854959655445, alpha = 0.09491299192686041\n",
      "Classifier 36/50: error = 0.4201797062481687, alpha = 0.16101786266023632\n",
      "Classifier 37/50: error = 0.4391214581803736, alpha = 0.12236416705699299\n",
      "Classifier 38/50: error = 0.4438827889178809, alpha = 0.11270927231449412\n",
      "Classifier 39/50: error = 0.4564245334184167, alpha = 0.0873725895481852\n",
      "Classifier 40/50: error = 0.46732355915284907, alpha = 0.0654461615376508\n",
      "Classifier 41/50: error = 0.4631318744822832, alpha = 0.07387032418834008\n",
      "Classifier 42/50: error = 0.4421796582389135, alpha = 0.1161603381125249\n",
      "Classifier 43/50: error = 0.430597686921411, alpha = 0.13970651196451153\n",
      "Classifier 44/50: error = 0.44877535076980846, alpha = 0.10281000384905502\n",
      "Classifier 45/50: error = 0.43895926038123145, alpha = 0.12269345733138168\n",
      "Classifier 46/50: error = 0.46976555508140483, alpha = 0.060542753170335126\n",
      "Classifier 47/50: error = 0.4656898645701518, alpha = 0.06872828120695994\n",
      "Classifier 48/50: error = 0.405838121005927, alpha = 0.19059872117844034\n",
      "Classifier 49/50: error = 0.46077901635319957, alpha = 0.07860345207567114\n",
      "Classifier 50/50: error = 0.45474838057505884, alpha = 0.0907515594203978\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1/50: error = 0.35409170587692607, alpha = 0.3005507991409799\n",
      "Classifier 2/50: error = 0.3765814107277552, alpha = 0.25204196586902416\n",
      "Classifier 3/50: error = 0.43547187518267394, alpha = 0.12977999556972003\n",
      "Classifier 4/50: error = 0.4276082938992428, alpha = 0.14580799123372548\n",
      "Classifier 5/50: error = 0.44790947115060487, alpha = 0.10456044782803275\n",
      "Classifier 6/50: error = 0.3697410287897369, alpha = 0.2666639809170901\n",
      "Classifier 7/50: error = 0.45256757025634853, alpha = 0.09515097975870183\n",
      "Classifier 8/50: error = 0.38913713549591966, alpha = 0.2254703373191992\n",
      "Classifier 9/50: error = 0.3815163038534156, alpha = 0.2415586225393292\n",
      "Classifier 10/50: error = 0.37615010564556817, alpha = 0.2529607522702776\n",
      "Classifier 11/50: error = 0.39040428683048645, alpha = 0.2228065684819196\n",
      "Classifier 12/50: error = 0.43707193808838796, alpha = 0.12652702198487906\n",
      "Classifier 13/50: error = 0.4016325743048621, alpha = 0.19933365651088913\n",
      "Classifier 14/50: error = 0.41583884961764994, alpha = 0.16993954132309538\n",
      "Classifier 15/50: error = 0.4457491124301305, alpha = 0.10893059214104443\n",
      "Classifier 16/50: error = 0.44114838553807545, alpha = 0.11825134784259116\n",
      "Classifier 17/50: error = 0.463660622826058, alpha = 0.07280712937896582\n",
      "Classifier 18/50: error = 0.47513692479928205, alpha = 0.04976719709166161\n",
      "Classifier 19/50: error = 0.33780501964008636, alpha = 0.3365455903146815\n",
      "Classifier 20/50: error = 0.38778812667798457, alpha = 0.22830964776559368\n",
      "Classifier 21/50: error = 0.42307460206485814, alpha = 0.15508221868192967\n",
      "Classifier 22/50: error = 0.4605519922321273, alpha = 0.07906032784318312\n",
      "Classifier 23/50: error = 0.4315387716278005, alpha = 0.1377878756570077\n",
      "Classifier 24/50: error = 0.4328207755690773, alpha = 0.1351758085578548\n",
      "Classifier 25/50: error = 0.4580614420615082, alpha = 0.08407465257681278\n",
      "Classifier 26/50: error = 0.46799270816463434, alpha = 0.06410224036399455\n",
      "Classifier 27/50: error = 0.43759518301404954, alpha = 0.12546383107633854\n",
      "Classifier 28/50: error = 0.43457382410368217, alpha = 0.13160695211408083\n",
      "Classifier 29/50: error = 0.4121556723730276, alpha = 0.17753052014536969\n",
      "Classifier 30/50: error = 0.44825805166002686, alpha = 0.10385568864155675\n",
      "Classifier 31/50: error = 0.43440781499131365, alpha = 0.1319447692470914\n",
      "Classifier 32/50: error = 0.4618555903810506, alpha = 0.07643733809595646\n",
      "Classifier 33/50: error = 0.4587220171605262, alpha = 0.082744289425082\n",
      "Classifier 34/50: error = 0.45251012103652366, alpha = 0.09526692287430207\n",
      "Classifier 35/50: error = 0.45145830204431137, alpha = 0.09739014213383894\n",
      "Classifier 36/50: error = 0.4202579950323064, alpha = 0.16085719445878988\n",
      "Classifier 37/50: error = 0.3994494932282237, alpha = 0.20387970675451758\n",
      "Classifier 38/50: error = 0.46161074787060663, alpha = 0.07692990830826167\n",
      "Classifier 39/50: error = 0.45310616226457245, alpha = 0.0940641254070772\n",
      "Classifier 40/50: error = 0.46979627738675855, alpha = 0.060481083293615595\n",
      "Classifier 41/50: error = 0.41160975556528223, alpha = 0.17865735172572692\n",
      "Classifier 42/50: error = 0.4328025897823409, alpha = 0.13521284897306285\n",
      "Classifier 43/50: error = 0.4546578550093887, alpha = 0.09093410877679826\n",
      "Classifier 44/50: error = 0.4547225112411459, alpha = 0.09080372561730539\n",
      "Classifier 45/50: error = 0.4374692600052583, alpha = 0.12571967046505247\n",
      "Classifier 46/50: error = 0.4366891846488201, alpha = 0.1273049257768243\n",
      "Classifier 47/50: error = 0.38792309063967245, alpha = 0.22802542171871176\n",
      "Classifier 48/50: error = 0.45041856959748683, alpha = 0.09948982386018618\n",
      "Classifier 49/50: error = 0.44744741277035627, alpha = 0.1054947959394101\n",
      "Classifier 50/50: error = 0.47472807614557466, alpha = 0.0505869549212156\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Classifier 1/50: error = 0.40765784802637145, alpha = 0.18682810623538637\n",
      "Classifier 2/50: error = 0.24166096245332713, alpha = 0.5717974010898469\n",
      "Classifier 3/50: error = 0.4071773448314012, alpha = 0.1878232315861347\n",
      "Classifier 4/50: error = 0.3762167367552326, alpha = 0.25281878426290616\n",
      "Classifier 5/50: error = 0.3924604985862046, alpha = 0.21849065215040342\n",
      "Classifier 6/50: error = 0.40929814182933555, alpha = 0.18343378816467135\n",
      "Classifier 7/50: error = 0.36584680873307224, alpha = 0.2750379306650312\n",
      "Classifier 8/50: error = 0.373731300390004, alpha = 0.25812121022508067\n",
      "Classifier 9/50: error = 0.19672578520206452, alpha = 0.7034426687024292\n",
      "Classifier 10/50: error = 0.2655155515204152, alpha = 0.5087477043916775\n",
      "Classifier 11/50: error = 0.2400097918543621, alpha = 0.5763129134952582\n",
      "Classifier 12/50: error = 0.42550716884731377, alpha = 0.15010291140618784\n",
      "Classifier 13/50: error = 0.3683757578105816, alpha = 0.26959558274580875\n",
      "Classifier 14/50: error = 0.4257697299917522, alpha = 0.149565911437497\n",
      "Classifier 15/50: error = 0.4154208195438832, alpha = 0.17080010460222747\n",
      "Classifier 16/50: error = 0.37731454813328913, alpha = 0.25048115687764844\n",
      "Classifier 17/50: error = 0.38519002559218213, alpha = 0.233788224886897\n",
      "Classifier 18/50: error = 0.4321775766809189, alpha = 0.13648608693435982\n",
      "Classifier 19/50: error = 0.40676055848794745, alpha = 0.1886866967191178\n",
      "Classifier 20/50: error = 0.45649928278685004, alpha = 0.0872219486120465\n",
      "Classifier 21/50: error = 0.37616834817815936, alpha = 0.252921882707111\n",
      "Classifier 22/50: error = 0.4181735344235673, alpha = 0.16513786679738746\n",
      "Classifier 23/50: error = 0.44479358874361596, alpha = 0.110864813848267\n",
      "Classifier 24/50: error = 0.41216781127385604, alpha = 0.17750546921596094\n",
      "Classifier 25/50: error = 0.42793014049834976, alpha = 0.14515057836727566\n",
      "Classifier 26/50: error = 0.4306588898041329, alpha = 0.13958170367205727\n",
      "Classifier 27/50: error = 0.42271141635085696, alpha = 0.1558262855072947\n",
      "Classifier 28/50: error = 0.39188061554923526, alpha = 0.21970698868497246\n",
      "Classifier 29/50: error = 0.4413197707537905, alpha = 0.11790377614455608\n",
      "Classifier 30/50: error = 0.46379748948584265, alpha = 0.07253194796352613\n",
      "Classifier 31/50: error = 0.4167561164651863, alpha = 0.16805211290335464\n",
      "Classifier 32/50: error = 0.37928748829866077, alpha = 0.2462867843311322\n",
      "Classifier 33/50: error = 0.39442526159750013, alpha = 0.214374158222939\n",
      "Classifier 34/50: error = 0.4465763573696502, alpha = 0.10725669502677451\n",
      "Classifier 35/50: error = 0.4487697286486305, alpha = 0.10282136737445713\n",
      "Classifier 36/50: error = 0.39690410125590914, alpha = 0.2091907635490638\n",
      "Classifier 37/50: error = 0.4552009350325935, alpha = 0.08983905079397127\n",
      "Classifier 38/50: error = 0.44240853259140334, alpha = 0.11569640992872166\n",
      "Classifier 39/50: error = 0.44845223842387283, alpha = 0.10346312696080107\n",
      "Classifier 40/50: error = 0.4325662467714244, alpha = 0.13569426085972774\n",
      "Classifier 41/50: error = 0.4100855017771905, alpha = 0.18180596459032158\n",
      "Classifier 42/50: error = 0.4574413313246123, alpha = 0.08532379280310777\n",
      "Classifier 43/50: error = 0.45553521930592566, alpha = 0.08916511225625466\n",
      "Classifier 44/50: error = 0.4347488785988536, alpha = 0.13125076059549512\n",
      "Classifier 45/50: error = 0.4517084089279357, alpha = 0.09688519361209903\n",
      "Classifier 46/50: error = 0.41172671712071296, alpha = 0.1784158926762846\n",
      "Classifier 47/50: error = 0.38942499325731417, alpha = 0.22486493609005617\n",
      "Classifier 48/50: error = 0.41800536629938057, alpha = 0.16548347834835234\n",
      "Classifier 49/50: error = 0.4275324424097728, alpha = 0.14596294581178615\n",
      "Classifier 50/50: error = 0.4661608836803023, alpha = 0.06778184737348318\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Classifier 1/50: error = 0.4571793439221008, alpha = 0.08585161525032356\n",
      "Classifier 2/50: error = 0.36777329358824185, alpha = 0.2708906720438777\n",
      "Classifier 3/50: error = 0.4096368027931619, alpha = 0.1827335078280553\n",
      "Classifier 4/50: error = 0.418894465335322, alpha = 0.16365668397095978\n",
      "Classifier 5/50: error = 0.3290121823431176, alpha = 0.3563281013348451\n",
      "Classifier 6/50: error = 0.3354519625696789, alpha = 0.3418142009672755\n",
      "Classifier 7/50: error = 0.4105574118739139, alpha = 0.18083077342516182\n",
      "Classifier 8/50: error = 0.2520103419731524, alpha = 0.5439595126015684\n",
      "Classifier 9/50: error = 0.2915477523690014, alpha = 0.4439394245078011\n",
      "Classifier 10/50: error = 0.29173325718763043, alpha = 0.44349044796704656\n",
      "Classifier 11/50: error = 0.358538103309879, alpha = 0.29085742259206265\n",
      "Classifier 12/50: error = 0.3432255510384384, alpha = 0.3244764203376249\n",
      "Classifier 13/50: error = 0.42498448458143484, alpha = 0.1511721811588674\n",
      "Classifier 14/50: error = 0.4043218494851015, alpha = 0.1937446436352823\n",
      "Classifier 15/50: error = 0.3965417270293903, alpha = 0.2099478111955794\n",
      "Classifier 16/50: error = 0.30538776991536554, alpha = 0.4108857007426402\n",
      "Classifier 17/50: error = 0.38796255523909956, alpha = 0.22794231847283802\n",
      "Classifier 18/50: error = 0.3848302741858266, alpha = 0.23454790787614377\n",
      "Classifier 19/50: error = 0.453631255688939, alpha = 0.09300472387946931\n",
      "Classifier 20/50: error = 0.37673555011099297, alpha = 0.2517137120251513\n",
      "Classifier 21/50: error = 0.4214276567530808, alpha = 0.15845773102019756\n",
      "Classifier 22/50: error = 0.4471427186693431, alpha = 0.10611103130047259\n",
      "Classifier 23/50: error = 0.4560345908616287, alpha = 0.08815849743301993\n",
      "Classifier 24/50: error = 0.4332266406814289, alpha = 0.13434924704786147\n",
      "Classifier 25/50: error = 0.4429881436038098, alpha = 0.11452176000533548\n",
      "Classifier 26/50: error = 0.4124453893034507, alpha = 0.17693269428004907\n",
      "Classifier 27/50: error = 0.4084736096178384, alpha = 0.18513949465182108\n",
      "Classifier 28/50: error = 0.3643170869434592, alpha = 0.27833763405583956\n",
      "Classifier 29/50: error = 0.37691536515844526, alpha = 0.25133084663980954\n",
      "Classifier 30/50: error = 0.4291972209151427, alpha = 0.1425636092930968\n",
      "Classifier 31/50: error = 0.42613735370319594, alpha = 0.14881417731197155\n",
      "Classifier 32/50: error = 0.4265390042022259, alpha = 0.14799305440461086\n",
      "Classifier 33/50: error = 0.43575592115886574, alpha = 0.1292023244971455\n",
      "Classifier 34/50: error = 0.39118856112742495, alpha = 0.22115944911537946\n",
      "Classifier 35/50: error = 0.4572523802549499, alpha = 0.08570446516176308\n",
      "Classifier 36/50: error = 0.4287134431150369, alpha = 0.14355110183813724\n",
      "Classifier 37/50: error = 0.40589859751755863, alpha = 0.19047332368370248\n",
      "Classifier 38/50: error = 0.42458516463682483, alpha = 0.15198931219082146\n",
      "Classifier 39/50: error = 0.45374484232701684, alpha = 0.09277558477506447\n",
      "Classifier 40/50: error = 0.41851170219991374, alpha = 0.16444299787162445\n",
      "Classifier 41/50: error = 0.4514542677957286, alpha = 0.09739828740810637\n",
      "Classifier 42/50: error = 0.46371426676302185, alpha = 0.07269927262457032\n",
      "Classifier 43/50: error = 0.4524295460107002, alpha = 0.0954295424325364\n",
      "Classifier 44/50: error = 0.4433458786046026, alpha = 0.11379692513784438\n",
      "Classifier 45/50: error = 0.4633916315837132, alpha = 0.07334798999828451\n",
      "Classifier 46/50: error = 0.4630026280976258, alpha = 0.07413023506184184\n",
      "Classifier 47/50: error = 0.46390488151291054, alpha = 0.07231603534473684\n",
      "Classifier 48/50: error = 0.472431419935601, alpha = 0.055193136536121686\n",
      "Classifier 49/50: error = 0.44989439218164506, alpha = 0.1005487013939392\n",
      "Classifier 50/50: error = 0.42112158404645395, alpha = 0.15908543777075357\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3820186308862491, alpha = 0.24049446523369344\n",
      "Classifier 2/50: error = 0.42101427888439213, alpha = 0.1593055331106058\n",
      "Classifier 3/50: error = 0.2981240071764125, alpha = 0.4281236042090254\n",
      "Classifier 4/50: error = 0.41434392136206255, alpha = 0.1730181819017054\n",
      "Classifier 5/50: error = 0.4552706864074365, alpha = 0.08969842084857818\n",
      "Classifier 6/50: error = 0.35733303058848154, alpha = 0.2934792258627417\n",
      "Classifier 7/50: error = 0.4209238958847489, alpha = 0.15949093100626333\n",
      "Classifier 8/50: error = 0.41160644581572947, alpha = 0.17866418477477478\n",
      "Classifier 9/50: error = 0.41774521090062466, alpha = 0.16601821522257856\n",
      "Classifier 10/50: error = 0.4522322762508466, alpha = 0.09582770096701329\n",
      "Classifier 11/50: error = 0.40513839826252995, alpha = 0.1920500217258855\n",
      "Classifier 12/50: error = 0.38380883623897843, alpha = 0.23670631986738072\n",
      "Classifier 13/50: error = 0.40396305106972674, alpha = 0.19448962194149683\n",
      "Classifier 14/50: error = 0.4097569395392462, alpha = 0.1824851326799311\n",
      "Classifier 15/50: error = 0.3856522595258848, alpha = 0.23281251946390943\n",
      "Classifier 16/50: error = 0.42494150242711304, alpha = 0.1512601261838176\n",
      "Classifier 17/50: error = 0.4343958856236031, alpha = 0.13196904584265673\n",
      "Classifier 18/50: error = 0.44530679284743035, alpha = 0.10982585710234931\n",
      "Classifier 19/50: error = 0.43851290886464156, alpha = 0.12359976691685733\n",
      "Classifier 20/50: error = 0.4500596078921262, alpha = 0.10021492917630259\n",
      "Classifier 21/50: error = 0.41820722424340306, alpha = 0.1650686337007377\n",
      "Classifier 22/50: error = 0.46297369505642755, alpha = 0.07418841996871585\n",
      "Classifier 23/50: error = 0.433631932677086, alpha = 0.13352403476767807\n",
      "Classifier 24/50: error = 0.46653807497963373, alpha = 0.06702403226777996\n",
      "Classifier 25/50: error = 0.43792432159606065, alpha = 0.12479519307773411\n",
      "Classifier 26/50: error = 0.43178057506609935, alpha = 0.1372950622263002\n",
      "Classifier 27/50: error = 0.4596221732048604, alpha = 0.08093189236636131\n",
      "Classifier 28/50: error = 0.4196734751080913, alpha = 0.16205697516023523\n",
      "Classifier 29/50: error = 0.4136830573515626, alpha = 0.17438019118823317\n",
      "Classifier 30/50: error = 0.40602304803175526, alpha = 0.19021529529503708\n",
      "Classifier 31/50: error = 0.4488025875059729, alpha = 0.10275495287667867\n",
      "Classifier 32/50: error = 0.43129731498449775, alpha = 0.13828004866686308\n",
      "Classifier 33/50: error = 0.4837615719864984, alpha = 0.03248828153189788\n",
      "Classifier 34/50: error = 0.3353222708164497, alpha = 0.3421051167197047\n",
      "Classifier 35/50: error = 0.4537189889279756, alpha = 0.09282773815958262\n",
      "Classifier 36/50: error = 0.44683848247832214, alpha = 0.10672642066995923\n",
      "Classifier 37/50: error = 0.4007114455703906, alpha = 0.20125081389695612\n",
      "Classifier 38/50: error = 0.4396689387517545, alpha = 0.12125287901078584\n",
      "Classifier 39/50: error = 0.42702806629728857, alpha = 0.14699349701590897\n",
      "Classifier 40/50: error = 0.45737337405233225, alpha = 0.085460700825635\n",
      "Classifier 41/50: error = 0.46503923918304, alpha = 0.07003580627186444\n",
      "Classifier 42/50: error = 0.4415968101764335, alpha = 0.11734199610739317\n",
      "Classifier 43/50: error = 0.4469799063985127, alpha = 0.10644034748564402\n",
      "Classifier 44/50: error = 0.4493994931609864, alpha = 0.1015486406252674\n",
      "Classifier 45/50: error = 0.4722671861826834, alpha = 0.05552261166026541\n",
      "Classifier 46/50: error = 0.4627962693794029, alpha = 0.07454523742574119\n",
      "Classifier 47/50: error = 0.4572717590059545, alpha = 0.08566542240860894\n",
      "Classifier 48/50: error = 0.4562420651878717, alpha = 0.08774033082132994\n",
      "Classifier 49/50: error = 0.4696091620746311, alpha = 0.060856693048869794\n",
      "Classifier 50/50: error = 0.47664791414836016, alpha = 0.04673817449329871\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Classifier 1/50: error = 0.4037653387123885, alpha = 0.19490022602687201\n",
      "Classifier 2/50: error = 0.4336737640150761, alpha = 0.13343887257466228\n",
      "Classifier 3/50: error = 0.38531072335126804, alpha = 0.23353340819605217\n",
      "Classifier 4/50: error = 0.2054697982977896, alpha = 0.6762259710744792\n",
      "Classifier 5/50: error = 0.3857635016126604, alpha = 0.2325777694225006\n",
      "Classifier 6/50: error = 0.4455033255376081, alpha = 0.10942804887587351\n",
      "Classifier 7/50: error = 0.3994725126594555, alpha = 0.20383172799769977\n",
      "Classifier 8/50: error = 0.4308940558471569, alpha = 0.1391021802701193\n",
      "Classifier 9/50: error = 0.41738678637248666, alpha = 0.16675509374667696\n",
      "Classifier 10/50: error = 0.42580109662966614, alpha = 0.14950176493600414\n",
      "Classifier 11/50: error = 0.4364303653929179, alpha = 0.1278310339313335\n",
      "Classifier 12/50: error = 0.4704459747628702, alpha = 0.05917703160204048\n",
      "Classifier 13/50: error = 0.4608700166609576, alpha = 0.0784203272796195\n",
      "Classifier 14/50: error = 0.4457295011667697, alpha = 0.10897028209200178\n",
      "Classifier 15/50: error = 0.444776589741136, alpha = 0.1108992315685625\n",
      "Classifier 16/50: error = 0.44134829642484097, alpha = 0.11784592842548602\n",
      "Classifier 17/50: error = 0.4714356983491636, alpha = 0.05719087505227922\n",
      "Classifier 18/50: error = 0.4447535697124798, alpha = 0.1109458404209977\n",
      "Classifier 19/50: error = 0.4380202381122945, alpha = 0.12460036163706557\n",
      "Classifier 20/50: error = 0.46734721557503867, alpha = 0.06539864590092148\n",
      "Classifier 21/50: error = 0.4535154836904338, alpha = 0.09323828154820635\n",
      "Classifier 22/50: error = 0.46047107035026813, alpha = 0.07922318742049962\n",
      "Classifier 23/50: error = 0.42219222673662127, alpha = 0.1568902585852097\n",
      "Classifier 24/50: error = 0.41770675057171847, alpha = 0.16609727654940848\n",
      "Classifier 25/50: error = 0.4340011793195184, alpha = 0.13277237156054764\n",
      "Classifier 26/50: error = 0.4404547269184844, alpha = 0.11965838897349168\n",
      "Classifier 27/50: error = 0.45358179262376835, alpha = 0.09310450910261113\n",
      "Classifier 28/50: error = 0.43746376777345025, alpha = 0.1257308294750841\n",
      "Classifier 29/50: error = 0.40642745555568716, alpha = 0.18937699307048955\n",
      "Classifier 30/50: error = 0.4167715382594893, alpha = 0.1680203901816413\n",
      "Classifier 31/50: error = 0.4344070116176413, alpha = 0.13194640413003508\n",
      "Classifier 32/50: error = 0.4682788341784416, alpha = 0.06352765474661709\n",
      "Classifier 33/50: error = 0.43588352576312306, alpha = 0.12894283990708502\n",
      "Classifier 34/50: error = 0.46717923971493003, alpha = 0.06573604397733115\n",
      "Classifier 35/50: error = 0.46268386898146974, alpha = 0.0747712935596687\n",
      "Classifier 36/50: error = 0.4590903819602582, alpha = 0.08200254948506902\n",
      "Classifier 37/50: error = 0.41607700977802853, alpha = 0.16944937260916265\n",
      "Classifier 38/50: error = 0.45308507713855106, alpha = 0.09410667005557995\n",
      "Classifier 39/50: error = 0.4239938982635807, alpha = 0.1531995948146538\n",
      "Classifier 40/50: error = 0.4599335592507772, alpha = 0.08030506387104822\n",
      "Classifier 41/50: error = 0.45933676595197803, alpha = 0.08150648058436992\n",
      "Classifier 42/50: error = 0.42455780770236107, alpha = 0.15204530022667112\n",
      "Classifier 43/50: error = 0.4610273661190146, alpha = 0.07810369690967743\n",
      "Classifier 44/50: error = 0.4471424727819523, alpha = 0.1061115286332566\n",
      "Classifier 45/50: error = 0.44584137079571196, alpha = 0.10874388106440964\n",
      "Classifier 46/50: error = 0.4450304446857425, alpha = 0.11038528102875436\n",
      "Classifier 47/50: error = 0.4676967873188799, alpha = 0.06469654000182279\n",
      "Classifier 48/50: error = 0.4356667458905491, alpha = 0.1293836731132512\n",
      "Classifier 49/50: error = 0.4658172276932523, alpha = 0.06847235433519235\n",
      "Classifier 50/50: error = 0.4350743931844593, alpha = 0.13058850889496573\n",
      "Multiclass Accuracy: 0.8457\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostMulticlass: # Creamos la clase AdaBoostMulticlass\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.models = [] # Inicializamos los modelos\n",
    "\n",
    "    def fit(self, X, y, verbose=False): # Creamos la función fit\n",
    "        for digit in range(10): # Para cada dígito\n",
    "            X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X, y)\n",
    "            model = AdaBoost(T=self.T, A=self.A) # Creamos el clasificador AdaBoost\n",
    "            model.fit(X_train_balanced, Y_train_binary_balanced, verbose) # Ajustamos el clasificador AdaBoost\n",
    "            self.models.append(model) # Añadimos el clasificador AdaBoost\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        model_preds = np.array([model.predict(X) for model in self.models]) # Realizamos las predicciones\n",
    "        return np.argmax(model_preds, axis=0) # Devolvemos el índice del valor máximo\n",
    "\n",
    "def run_adaboost_multiclass_on_mnist(T=5, A=20, verboseParam=False, n_components=50): # Creamos la función run_adaboost_multiclass_on_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Redimensionamos los datos\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Redimensionamos los datos\n",
    "\n",
    "    # Apply PCA\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components)\n",
    "\n",
    "    adaboost_multiclass = AdaBoostMulticlass(T=T, A=A) # Creamos el clasificador AdaBoostMulticlass\n",
    "    adaboost_multiclass.fit(X_train_reduced, y_train, verboseParam) # Ajustamos el clasificador AdaBoostMulticlass\n",
    "    y_pred = adaboost_multiclass.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test == y_pred) / len(y_test) # Calculamos la precisión\n",
    "    print(f\"Multiclass Accuracy: {accuracy}\") # Mostramos la precisión\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "\n",
    "accuracy = run_adaboost_multiclass_on_mnist(T=50, A=20, verboseParam=True) # Ejecutamos AdaBoostMulticlass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclase con ADABoosti Binario con Mejoras (Version 1: Añadidos los parámetros n_componentes y k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "def select_best_features(X_train, y_train, X_test, k=200): # Creamos la función select_best_features\n",
    "    selector = SelectKBest(f_classif, k=k) # Creamos el objeto SelectKBest\n",
    "    X_train_best = selector.fit_transform(X_train, y_train) # Seleccionamos las mejores características de los datos de entrenamiento\n",
    "    X_test_best = selector.transform(X_test) # Seleccionamos las mejores características de los datos de prueba\n",
    "    return X_train_best, X_test_best # Devolvemos los datos con las mejores características\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False, n_components=50, k=200): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Redimensionamos los datos para reducir la dimensión de las imagénes a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Redimensionamos los datos para reducir la dimensión de las imagénes a 1D\n",
    "\n",
    "    X_train_best, X_test_best = select_best_features(X_train, y_train, X_test, k=k) # Seleccionamos las mejores características\n",
    "\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train_best, X_test_best, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A) # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_reduced, y_train_binary, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Classifier 1/5: error = 0.09578333333333311, alpha = 1.1224901549858723\n",
      "Classifier 2/5: error = 0.423811546699126, alpha = 0.15357294594849832\n",
      "Classifier 3/5: error = 0.44594286075021583, alpha = 0.10853849596112296\n",
      "Classifier 4/5: error = 0.43261482746443425, alpha = 0.13559530076075985\n",
      "Classifier 5/5: error = 0.47357464282143635, alpha = 0.05290000448255655\n",
      "Accuracy for digit 3: 0.9003\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=5, A=20, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n",
      "Classifier 1/5: error = 0.06493333333333336, alpha = 1.3336283620106117\n",
      "Classifier 2/5: error = 0.40729086733956765, alpha = 0.18758809260424628\n",
      "Classifier 3/5: error = 0.45367364809500965, alpha = 0.09291920425006518\n",
      "Classifier 4/5: error = 0.36758716202838615, alpha = 0.2712909696472824\n",
      "Classifier 5/5: error = 0.45306292554869954, alpha = 0.09415136693421174\n",
      "Accuracy for digit 0: 0.934\n",
      "Running AdaBoost for digit: 1\n",
      "Classifier 1/5: error = 0.11243333333333305, alpha = 1.033061590861527\n",
      "Classifier 2/5: error = 0.4453157110817808, alpha = 0.10980780466445449\n",
      "Classifier 3/5: error = 0.1714455120615499, alpha = 0.7877085495093323\n",
      "Classifier 4/5: error = 0.2813040656471276, alpha = 0.46900109941991674\n",
      "Classifier 5/5: error = 0.4102413735063475, alpha = 0.18148382156901965\n",
      "Accuracy for digit 1: 0.8865\n",
      "Running AdaBoost for digit: 2\n",
      "Classifier 1/5: error = 0.09931666666666639, alpha = 1.1024201675356724\n",
      "Classifier 2/5: error = 0.40659765491248473, alpha = 0.18902426304933448\n",
      "Classifier 3/5: error = 0.37921790125569543, alpha = 0.24643457767282734\n",
      "Classifier 4/5: error = 0.3869060627791908, alpha = 0.23016811708369078\n",
      "Classifier 5/5: error = 0.43002896293659254, alpha = 0.14086649261262346\n",
      "Accuracy for digit 2: 0.8965\n",
      "Running AdaBoost for digit: 3\n",
      "Classifier 1/5: error = 0.10219999999999974, alpha = 1.08650782385557\n",
      "Classifier 2/5: error = 0.3392567786858214, alpha = 0.33330400715356173\n",
      "Classifier 3/5: error = 0.4018037495315872, alpha = 0.19897754694905645\n",
      "Classifier 4/5: error = 0.45816307580283977, alpha = 0.08386994841179553\n",
      "Classifier 5/5: error = 0.4404138790016704, alpha = 0.11974126095019975\n",
      "Accuracy for digit 3: 0.899\n",
      "Running AdaBoost for digit: 4\n",
      "Classifier 1/5: error = 0.09738333333333335, alpha = 1.1133214355743424\n",
      "Classifier 2/5: error = 0.4223246683662537, alpha = 0.15661881313152382\n",
      "Classifier 3/5: error = 0.23469215245170938, alpha = 0.5910017509445644\n",
      "Classifier 4/5: error = 0.42255997075733776, alpha = 0.15613660606383142\n",
      "Classifier 5/5: error = 0.4685230618983178, alpha = 0.06303724058271912\n",
      "Accuracy for digit 4: 0.9018\n",
      "Running AdaBoost for digit: 5\n",
      "Classifier 1/5: error = 0.09033333333333338, alpha = 1.154785849415778\n",
      "Classifier 2/5: error = 0.40890621963185436, alpha = 0.18424442383523368\n",
      "Classifier 3/5: error = 0.43706179582771343, alpha = 0.12654763303298114\n",
      "Classifier 4/5: error = 0.44099009882411255, alpha = 0.11857238085094904\n",
      "Classifier 5/5: error = 0.3699171721141806, alpha = 0.26628608101979395\n",
      "Accuracy for digit 5: 0.9108\n",
      "Running AdaBoost for digit: 6\n",
      "Classifier 1/5: error = 0.09865000000000004, alpha = 1.106157703038341\n",
      "Classifier 2/5: error = 0.44660665243817943, alpha = 0.10719540558518503\n",
      "Classifier 3/5: error = 0.4046580016358953, alpha = 0.1930468787727354\n",
      "Classifier 4/5: error = 0.3547516543645769, alpha = 0.299108647530328\n",
      "Classifier 5/5: error = 0.3839636209743459, alpha = 0.23637910374876583\n",
      "Accuracy for digit 6: 0.9042\n",
      "Running AdaBoost for digit: 7\n",
      "Classifier 1/5: error = 0.10443333333333304, alpha = 1.0744538774497296\n",
      "Classifier 2/5: error = 0.37481838141768886, alpha = 0.25580030235815276\n",
      "Classifier 3/5: error = 0.4424931577455386, alpha = 0.11552488735401109\n",
      "Classifier 4/5: error = 0.3778648765692736, alpha = 0.24931032348281731\n",
      "Classifier 5/5: error = 0.4434659988972084, alpha = 0.11355356675307499\n",
      "Accuracy for digit 7: 0.8972\n",
      "Running AdaBoost for digit: 8\n",
      "Classifier 1/5: error = 0.09753333333333303, alpha = 1.1124687771255912\n",
      "Classifier 2/5: error = 0.46229559280251675, alpha = 0.0755522411942153\n",
      "Classifier 3/5: error = 0.3548757528680084, alpha = 0.29883759651876357\n",
      "Classifier 4/5: error = 0.396799965914886, alpha = 0.2094082918019669\n",
      "Classifier 5/5: error = 0.43698193336451585, alpha = 0.1267099328433053\n",
      "Accuracy for digit 8: 0.9026\n",
      "Running AdaBoost for digit: 9\n",
      "Classifier 1/5: error = 0.09919999999999973, alpha = 1.103072621098775\n",
      "Classifier 2/5: error = 0.4691282481951643, alpha = 0.061822144323068706\n",
      "Classifier 3/5: error = 0.4574559232965292, alpha = 0.08529439595326087\n",
      "Classifier 4/5: error = 0.40427478418970286, alpha = 0.19384235386469156\n",
      "Classifier 5/5: error = 0.46933335255988473, alpha = 0.06141037623255637\n",
      "Accuracy for digit 9: 0.8991\n",
      "Accuracies for all digits: {0: 0.934, 1: 0.8865, 2: 0.8965, 3: 0.899, 4: 0.9018, 5: 0.9108, 6: 0.9042, 7: 0.8972, 8: 0.9026, 9: 0.8991}\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=5, A=20, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  0   1   2   3   4   5   6   7   8   9  10  11  16  17  18  19  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  52  53  54  55  56  57  82  83\n",
      "  84  85 111 112 140 141 168 476 560 644 645 671 672 673 699 700 701 727\n",
      " 728 729 730 754 755 756 757 758 759 780 781 782 783] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Accuracy: 0.8016\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostMulticlass: # Creamos la clase AdaBoostMulticlass\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A# Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.models = [] # Inicializamos los modelos\n",
    "\n",
    "    def fit(self, X, y, verbose=False): # Creamos la función fit\n",
    "        for digit in range(10): # Para cada dígito\n",
    "            y_binary = np.where(y == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "            model = AdaBoost(T=self.T, A=self.A) # Creamos el clasificador AdaBoost\n",
    "            model.fit(X, y_binary, verbose) # Ajustamos el clasificador AdaBoost\n",
    "            self.models.append(model) # Añadimos el clasificador AdaBoost\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        model_preds = np.array([model.predict(X) for model in self.models]) # Realizamos las predicciones\n",
    "        return np.argmax(model_preds, axis=0) # Devolvemos el índice del valor máximo\n",
    "\n",
    "def run_adaboost_multiclass_on_mnist(T=5, A=20, verboseParam=False, n_components=50, k=200): # Creamos la función run_adaboost_multiclass_on_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Redimensionamos los datos\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Redimensionamos los datos\n",
    "\n",
    "    X_train_best, X_test_best = select_best_features(X_train, y_train, X_test, k=k) # Seleccionamos las mejores características\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train_best, X_test_best, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    adaboost_multiclass = AdaBoostMulticlass(T=T, A=A) # Creamos el clasificador AdaBoostMulticlass\n",
    "    adaboost_multiclass.fit(X_train_reduced, y_train, verboseParam) # Ajustamos el clasificador AdaBoostMulticlass\n",
    "    y_pred = adaboost_multiclass.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test == y_pred) / len(y_test) # Calculamos la precisión\n",
    "    print(f\"Multiclass Accuracy: {accuracy}\") # Mostramos la precisión\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "\n",
    "accuracy = run_adaboost_multiclass_on_mnist(T=50, A=20, verboseParam=False) # Ejecutamos AdaBoostMulticlass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 1E: Implementación de una versión de AdaboostBinario.fit que pare automáticamente el entrenamiento \n",
    "cuando detecte sobreentrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada clasificador débil\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada pixel\n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "    \n",
    "    def compute_optimal_number_of_weak_classificators(self, X_verification, y_verification, X_test, y_test, \n",
    "                                                      iterNumber = 100, numberOfTries = 10):\n",
    "        bestAccuracy = 0\n",
    "        newAccuracy = 0\n",
    "        optimalNumberOfWeakClassificators = 5\n",
    "        \n",
    "        while round(newAccuracy, 6) >= round(bestAccuracy, 6):\n",
    "            optimalNumberOfWeakClassificators += 1\n",
    "            bestAccuracy = newAccuracy\n",
    "            newAccuracy = 0\n",
    "            for i in range(iterNumber):\n",
    "                adaboost = AdaBoost(T=optimalNumberOfWeakClassificators, A=numberOfTries)\n",
    "                adaboost.fit(X_verification, y_verification)\n",
    "                y_pred = adaboost.predict(X_test)\n",
    "                partialAccuracy = np.sum(y_test == np.sign(y_pred)) / len(y_test)\n",
    "                newAccuracy += partialAccuracy\n",
    "            newAccuracy /= iterNumber\n",
    "        self.T = optimalNumberOfWeakClassificators\n",
    "        #return optimalNumberOfWeakClassificators\n",
    "\n",
    "    \n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False, n_components=50): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Aplanamos los datos de prueba reduciendo la dimensión a 1D   \n",
    "\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) \n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A)  # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_reduced, y_train_binary, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "\n",
    "\n",
    "def run_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    digit, A=20, verboseParam=False, n_components=50, \n",
    "    split_proportion = 0.90, iterNumber = 100, numberOfTries = 10):\n",
    "    \n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "    num_samples = len(X_train)\n",
    "    split_point = int(split_proportion * num_samples)\n",
    "    \n",
    "    X_True_Train = X_train[:split_point]\n",
    "    y_True_Train = y_train[:split_point]\n",
    "\n",
    "    X_Verification = X_train[split_point:]\n",
    "    y_verification = y_train[split_point:]\n",
    "\n",
    "    X_True_Train = X_True_Train.reshape(X_True_Train.shape[0], -1) \n",
    "    X_Verification = X_Verification.reshape(X_Verification.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Aplanamos los datos de prueba reduciendo la dimensión a 1D   \n",
    "\n",
    "    X_true_train_reduced, X_test_train_reduced = apply_pca(X_True_Train, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "    X_verification_reduced, X_test_verification_reduced = apply_pca(X_Verification, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "    \n",
    "    y_true_train_binary = np.where(y_True_Train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_verification_binary = np.where(y_verification == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) \n",
    "    \n",
    "    adaboost = AdaBoost(T=9, A=A)  # Creamos el clasificador AdaBoost\n",
    "    \n",
    "    adaboost.compute_optimal_number_of_weak_classificators(\n",
    "        X_verification_reduced, y_verification_binary, X_test_verification_reduced, y_test_binary,\n",
    "        iterNumber = iterNumber, numberOfTries = numberOfTries)\n",
    "    \n",
    "    print(\"The optimal number of weak classificators is: \", adaboost.T)\n",
    "    \n",
    "    adaboost.fit(X_true_train_reduced, y_true_train_binary, verboseParam)\n",
    "    y_pred = adaboost.predict(X_test_train_reduced)\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\")\n",
    "    \n",
    "    return accuracy \n",
    "\n",
    "def run_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    A=20, verboseParam=False, n_components=50, \n",
    "    split_proportion = 0.90, iterNumber = 100, numberOfTries = 10):\n",
    "    \n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators(\n",
    "            digit, A, verboseParam, n_components, split_proportion, iterNumber, numberOfTries)\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Classifier 1/5: error = 0.10220000000000003, alpha = 1.0865078238555685\n",
      "Classifier 2/5: error = 0.45907977648290865, alpha = 0.08202390342839501\n",
      "Classifier 3/5: error = 0.45479312565178, alpha = 0.09066133095088648\n",
      "Classifier 4/5: error = 0.3101342671000804, alpha = 0.3997458325454976\n",
      "Classifier 5/5: error = 0.3633968464888695, alpha = 0.28032549513346844\n",
      "Accuracy for digit 3: 0.899\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=5, A=20, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=5, A=20, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "The optimal number of weak classificators is:  8\n",
      "Classifier 1/8: error = 0.10196296296296303, alpha = 1.0878008356802709\n",
      "Classifier 2/8: error = 0.46049299123156895, alpha = 0.07917907007095394\n",
      "Classifier 3/8: error = 0.44313703792545045, alpha = 0.11422005906052599\n",
      "Classifier 4/8: error = 0.37680582081984193, alpha = 0.2515640817889374\n",
      "Classifier 5/8: error = 0.39856886431587335, alpha = 0.2057158745059474\n",
      "Classifier 6/8: error = 0.43957064799317835, alpha = 0.12145236971698917\n",
      "Classifier 7/8: error = 0.4304948700702459, alpha = 0.13991619149231785\n",
      "Classifier 8/8: error = 0.42123258426575716, alpha = 0.15885777952092933\n",
      "Accuracy for digit 3: 0.899\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = run_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    digit=3, A=20, verboseParam=True, \n",
    "    n_components=50, split_proportion=0.90,\n",
    "    iterNumber = 100, numberOfTries=10)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[209], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_accuracies \u001b[38;5;241m=\u001b[39m  \u001b[43mrun_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverboseParam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_proportion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterNumber\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumberOfTries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracies for all digits:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_accuracies) \u001b[38;5;66;03m# Imprimimos las precisiones\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[208], line 175\u001b[0m, in \u001b[0;36mrun_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators\u001b[1;34m(A, verboseParam, n_components, split_proportion, iterNumber, numberOfTries)\u001b[0m\n\u001b[0;32m    173\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# Inicializamos las precisiones\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m digit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \u001b[38;5;66;03m# Para cada dígito\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mrun_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdigit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverboseParam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_proportion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterNumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumberOfTries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     accuracies[digit] \u001b[38;5;241m=\u001b[39m accuracy \u001b[38;5;66;03m# Guardamos la precisión\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracies\n",
      "Cell \u001b[1;32mIn[208], line 156\u001b[0m, in \u001b[0;36mrun_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators\u001b[1;34m(digit, A, verboseParam, n_components, split_proportion, iterNumber, numberOfTries)\u001b[0m\n\u001b[0;32m    152\u001b[0m y_test_binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_test \u001b[38;5;241m==\u001b[39m digit, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m    154\u001b[0m adaboost \u001b[38;5;241m=\u001b[39m AdaBoost(T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, A\u001b[38;5;241m=\u001b[39mA)  \u001b[38;5;66;03m# Creamos el clasificador AdaBoost\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[43madaboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_optimal_number_of_weak_classificators\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_verification_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_verification_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_verification_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_binary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterNumber\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43miterNumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumberOfTries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumberOfTries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimal number of weak classificators is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, adaboost\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    162\u001b[0m adaboost\u001b[38;5;241m.\u001b[39mfit(X_true_train_reduced, y_true_train_binary, verboseParam)\n",
      "Cell \u001b[1;32mIn[208], line 80\u001b[0m, in \u001b[0;36mAdaBoost.compute_optimal_number_of_weak_classificators\u001b[1;34m(self, X_verification, y_verification, X_test, y_test, iterNumber, numberOfTries)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterNumber):\n\u001b[0;32m     79\u001b[0m     adaboost \u001b[38;5;241m=\u001b[39m AdaBoost(T\u001b[38;5;241m=\u001b[39moptimalNumberOfWeakClassificators, A\u001b[38;5;241m=\u001b[39mnumberOfTries)\n\u001b[1;32m---> 80\u001b[0m     \u001b[43madaboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_verification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_verification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m adaboost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     82\u001b[0m     partialAccuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_test \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39msign(y_pred)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_test)\n",
      "Cell \u001b[1;32mIn[208], line 41\u001b[0m, in \u001b[0;36mAdaBoost.fit\u001b[1;34m(self, X, Y, verbose)\u001b[0m\n\u001b[0;32m     39\u001b[0m clf\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;28mmin\u001b[39m(X[:, clf\u001b[38;5;241m.\u001b[39mfeature_index]), \u001b[38;5;28mmax\u001b[39m(X[:, clf\u001b[38;5;241m.\u001b[39mfeature_index])) \u001b[38;5;66;03m# Elegimos un umbral aleatorio\u001b[39;00m\n\u001b[0;32m     40\u001b[0m clf\u001b[38;5;241m.\u001b[39mpolarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Elegimos una polaridad\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Realizamos las predicciones\u001b[39;00m\n\u001b[0;32m     42\u001b[0m error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(w[Y \u001b[38;5;241m!=\u001b[39m predictions]) \u001b[38;5;66;03m# Calculamos el error\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m: \u001b[38;5;66;03m# Si el error es mayor que 0.5\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_accuracies =  run_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    A=100, verboseParam=True, \n",
    "    n_components=50, split_proportion=0.75,\n",
    "    iterNumber = 500, numberOfTries=20)\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSIÓN BUENA BUENÍSIMA TAREA 1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, X_Verification, Y_Verification, iter_number = 100, \n",
    "            verbose=False, round1 = 3, round2 = 3, \n",
    "            bestAccuracyBreak = 0.999 ,practicalAccuracyBreak=666): # Creamos la función fit\n",
    "        bestAccuracy = 0\n",
    "        newAccuracy = 0\n",
    "        practicalRepeatAccuracy = 0\n",
    "        t = 0\n",
    "        \n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        while (True):\n",
    "            if newAccuracy >= bestAccuracyBreak:\n",
    "                print(\"Se para el entrenamiento debido a que la precisión ha alcanzado el valor práctico buscado\")\n",
    "                print(f\"El número óptimo de clasificadores débiles es: {len(self.clfs)} \")\n",
    "                print(f\"La precisión obtenida en el entrenamiento ha sido de: {round(newAccuracy, round2)}\")\n",
    "                break\n",
    "            \n",
    "            if practicalRepeatAccuracy == practicalAccuracyBreak:\n",
    "                print(\"Se para el entrenamiento debido a que la precisión se ha quedado estancada en un intervalo\")\n",
    "                print(\"Se para el entrenamiento debido a que la precisión ha alcanzado el valor práctico buscado\")\n",
    "                print(f\"El número óptimo de clasificadores débiles es: {len(self.clfs)} \")\n",
    "                print(f\"La precisión obtenida en el entrenamiento ha sido de: {round(newAccuracy, round2)}\")\n",
    "                break\n",
    "            \n",
    "            if round(newAccuracy, round1) == round(bestAccuracy, round1):\n",
    "                practicalRepeatAccuracy += 1\n",
    "            else:\n",
    "                practicalRepeatAccuracy = 0\n",
    "                \n",
    "            bestAccuracy = newAccuracy\n",
    "            newAccuracy = 0\n",
    "            t += 1\n",
    "            \n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): \n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "            \n",
    "            for i in range(iter_number):\n",
    "                y_pred_verification = self.predict(X_Verification)\n",
    "                partialAccuracy = np.sum(Y_Verification == np.sign(y_pred_verification)) / len(Y_Verification)\n",
    "                newAccuracy += partialAccuracy\n",
    "            \n",
    "            newAccuracy /= iter_number\n",
    "            if t == 6:\n",
    "                print(\"\")\n",
    "            #if newAccuracy < bestAccuracy:\n",
    "            if round(newAccuracy, round2) < round(bestAccuracy, round2) and self.T > 10:\n",
    "                self.clfs.pop() # Borramos el último clasificador débil\n",
    "                t = t - 1\n",
    "                print(\"Se ha detectado sobreentrenamiento. El número óptimo de clasificadores débiles es: \", t)\n",
    "                break\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {len(self.clfs)}: error = {min_error}, alpha = {best_clf.alpha}, newAccuracy = {newAccuracy}, bestAccuracy = {bestAccuracy}') # Mostramos el error y el alpha\n",
    "        self.T = t\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False, n_components=50): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Aplanamos los datos de prueba reduciendo la dimensión a 1D   \n",
    "\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) \n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A)  # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_reduced, y_train_binary, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "def run_adaboost_for_one_digit_detecting_overfitting(digit, A=20, \n",
    "                                                     verboseParam=False, n_components=50,\n",
    "                                                     split_proportion=0.90, iter_number=100,\n",
    "                                                     round1=3, round2=3,\n",
    "                                                     bestAccuracyBreak=0.999, practicalAccuracyBreak=0.95):  # Adjust practicalAccuracyBreak default\n",
    "    print(f\"Running AdaBoost for digit: {digit}\")  # Display the digit\n",
    "\n",
    "    # Load MNIST data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Balance the dataset for the specified digit\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train)\n",
    "\n",
    "    # Split the balanced dataset into training and verification sets\n",
    "    num_samples = len(X_train_balanced)\n",
    "    split_point = int(split_proportion * num_samples)\n",
    "    \n",
    "    X_True_Train = X_train_balanced[:split_point]\n",
    "    y_True_Train = Y_train_binary_balanced[:split_point]\n",
    "\n",
    "    X_Verification = X_train_balanced[split_point:]\n",
    "    y_verification_binary = Y_train_binary_balanced[split_point:]\n",
    "\n",
    "    # Reshape the datasets to flatten the images\n",
    "    X_True_Train = X_True_Train.reshape(X_True_Train.shape[0], -1)\n",
    "    X_Verification = X_Verification.reshape(X_Verification.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)  # Flatten test data\n",
    "    \n",
    "    # Initialize AdaBoost\n",
    "    adaboost = AdaBoost(T=0, A=A)  # Assuming AdaBoost is defined correctly\n",
    "    \n",
    "    # Train AdaBoost with added parameters for iterative training and validation\n",
    "    adaboost.fit(X_True_Train, y_True_Train, \n",
    "                 X_Verification, y_verification_binary, \n",
    "                 iter_number, verboseParam,\n",
    "                 round1=round1, round2=round2,\n",
    "                 bestAccuracyBreak=bestAccuracyBreak, practicalAccuracyBreak=practicalAccuracyBreak)\n",
    "    \n",
    "    # Predict on test data and calculate accuracy\n",
    "    y_pred = adaboost.predict(X_test)\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1)  # Convert test labels to binary\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary)\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1: error = 0.3113544101365482, alpha = 0.3968974546771549, newAccuracy = 0.6985239852398523, bestAccuracy = 0\n",
      "Classifier 2: error = 0.36757621005774654, alpha = 0.27131452577886744, newAccuracy = 0.6985239852398523, bestAccuracy = 0.6985239852398523\n",
      "Classifier 3: error = 0.3284689935590015, alpha = 0.3575588720568289, newAccuracy = 0.7531365313653139, bestAccuracy = 0.6985239852398523\n",
      "Classifier 4: error = 0.33439230327539404, alpha = 0.3441927942276345, newAccuracy = 0.7712177121771218, bestAccuracy = 0.7531365313653139\n",
      "Classifier 5: error = 0.3560250545799557, alpha = 0.2963293570927493, newAccuracy = 0.793726937269372, bestAccuracy = 0.7712177121771218\n",
      "\n",
      "Classifier 6: error = 0.4086826259012525, alpha = 0.1847070030882008, newAccuracy = 0.7933579335793363, bestAccuracy = 0.793726937269372\n",
      "Classifier 7: error = 0.4096153065291699, alpha = 0.1827779523496694, newAccuracy = 0.8018450184501846, bestAccuracy = 0.7933579335793363\n",
      "Classifier 8: error = 0.4106421001315417, alpha = 0.18065580317812072, newAccuracy = 0.8184501845018444, bestAccuracy = 0.8018450184501846\n",
      "Classifier 9: error = 0.3934965382014125, alpha = 0.21631908287220508, newAccuracy = 0.8092250922509221, bestAccuracy = 0.8184501845018444\n",
      "Classifier 10: error = 0.415498781596508, alpha = 0.17063959172542795, newAccuracy = 0.8081180811808126, bestAccuracy = 0.8092250922509221\n",
      "Classifier 11: error = 0.4031015670462138, alpha = 0.1962792060898821, newAccuracy = 0.8184501845018444, bestAccuracy = 0.8081180811808126\n",
      "Classifier 12: error = 0.4201117934609887, alpha = 0.1611572433919583, newAccuracy = 0.8202952029520296, bestAccuracy = 0.8184501845018444\n",
      "Classifier 13: error = 0.3917025132943629, alpha = 0.2200806962655684, newAccuracy = 0.8284132841328422, bestAccuracy = 0.8202952029520296\n",
      "Classifier 14: error = 0.4159988126278842, alpha = 0.16961030496557125, newAccuracy = 0.8350553505535053, bestAccuracy = 0.8284132841328422\n",
      "Classifier 15: error = 0.4380130546596249, alpha = 0.12461495277528621, newAccuracy = 0.8298892988929886, bestAccuracy = 0.8350553505535053\n",
      "Classifier 16: error = 0.4153247268222311, alpha = 0.17099795796532913, newAccuracy = 0.8409594095940963, bestAccuracy = 0.8298892988929886\n",
      "Classifier 17: error = 0.39192115568985053, alpha = 0.21962193274675798, newAccuracy = 0.8365313653136538, bestAccuracy = 0.8409594095940963\n",
      "Classifier 18: error = 0.42420369021315385, alpha = 0.15277011396393964, newAccuracy = 0.8394833948339483, bestAccuracy = 0.8365313653136538\n",
      "Classifier 19: error = 0.4185890304143629, alpha = 0.16428412558006142, newAccuracy = 0.8394833948339483, bestAccuracy = 0.8394833948339483\n",
      "Classifier 20: error = 0.43868337611388414, alpha = 0.12325361214040027, newAccuracy = 0.8464944649446486, bestAccuracy = 0.8394833948339483\n",
      "Classifier 21: error = 0.4267893624829455, alpha = 0.14748132931075567, newAccuracy = 0.8405904059040593, bestAccuracy = 0.8464944649446486\n",
      "Classifier 22: error = 0.44771273082892715, alpha = 0.10495826255864599, newAccuracy = 0.8461254612546131, bestAccuracy = 0.8405904059040593\n",
      "Classifier 23: error = 0.43923124478076203, alpha = 0.12214129579187179, newAccuracy = 0.8476014760147595, bestAccuracy = 0.8461254612546131\n",
      "Classifier 24: error = 0.4496907593571885, alpha = 0.1009601153943891, newAccuracy = 0.8494464944649444, bestAccuracy = 0.8476014760147595\n",
      "Classifier 25: error = 0.4323380969057318, alpha = 0.1361590432673456, newAccuracy = 0.8542435424354246, bestAccuracy = 0.8494464944649444\n",
      "Classifier 26: error = 0.4173110714907434, alpha = 0.16691077753866068, newAccuracy = 0.8575645756457558, bestAccuracy = 0.8542435424354246\n",
      "Classifier 27: error = 0.4401113273505721, alpha = 0.12035512673117328, newAccuracy = 0.8627306273062726, bestAccuracy = 0.8575645756457558\n",
      "Classifier 28: error = 0.43432177578442843, alpha = 0.13211986489555436, newAccuracy = 0.8557195571955709, bestAccuracy = 0.8627306273062726\n",
      "Classifier 29: error = 0.4337998603547161, alpha = 0.13318217138939026, newAccuracy = 0.8571955719557203, bestAccuracy = 0.8557195571955709\n",
      "Classifier 30: error = 0.4482920938585786, alpha = 0.10378686773184578, newAccuracy = 0.8601476014760151, bestAccuracy = 0.8571955719557203\n",
      "Classifier 31: error = 0.4527414730967323, alpha = 0.09480002730221024, newAccuracy = 0.8557195571955709, bestAccuracy = 0.8601476014760151\n",
      "Classifier 32: error = 0.4518021178914985, alpha = 0.09669601438450483, newAccuracy = 0.8616236162361617, bestAccuracy = 0.8557195571955709\n",
      "Classifier 33: error = 0.45747717688953704, alpha = 0.08525157892708468, newAccuracy = 0.862361623616237, bestAccuracy = 0.8616236162361617\n",
      "Classifier 34: error = 0.4375341965419757, alpha = 0.12558773602990597, newAccuracy = 0.8682656826568264, bestAccuracy = 0.862361623616237\n",
      "Classifier 35: error = 0.4415656498759488, alpha = 0.11740517922174393, newAccuracy = 0.8678966789667892, bestAccuracy = 0.8682656826568264\n",
      "Classifier 36: error = 0.44614775119405037, alpha = 0.10812388718205701, newAccuracy = 0.8704797047970486, bestAccuracy = 0.8678966789667892\n",
      "Classifier 37: error = 0.45453817309405503, alpha = 0.09117546265856694, newAccuracy = 0.8653136531365319, bestAccuracy = 0.8704797047970486\n",
      "Classifier 38: error = 0.44138400165666747, alpha = 0.1177735222528435, newAccuracy = 0.8686346863468637, bestAccuracy = 0.8653136531365319\n",
      "Classifier 39: error = 0.4638807118875653, alpha = 0.07236462800227415, newAccuracy = 0.8697416974169745, bestAccuracy = 0.8686346863468637\n",
      "Classifier 40: error = 0.4681079512558056, alpha = 0.06387080921819178, newAccuracy = 0.8675276752767538, bestAccuracy = 0.8697416974169745\n",
      "Classifier 41: error = 0.459786434504704, alpha = 0.08060122206919844, newAccuracy = 0.8682656826568264, bestAccuracy = 0.8675276752767538\n",
      "Classifier 42: error = 0.43430528803598556, alpha = 0.13215341950637652, newAccuracy = 0.8719557195571949, bestAccuracy = 0.8682656826568264\n",
      "Classifier 43: error = 0.45929833883380655, alpha = 0.08158384700548976, newAccuracy = 0.8738007380073803, bestAccuracy = 0.8719557195571949\n",
      "Classifier 44: error = 0.44396351144881463, alpha = 0.11254577065444797, newAccuracy = 0.8715867158671593, bestAccuracy = 0.8738007380073803\n",
      "Classifier 45: error = 0.441724825193037, alpha = 0.11708243241394956, newAccuracy = 0.8752767527675267, bestAccuracy = 0.8715867158671593\n",
      "Classifier 46: error = 0.4542797531707145, alpha = 0.09169663563818267, newAccuracy = 0.8738007380073803, bestAccuracy = 0.8752767527675267\n",
      "Classifier 47: error = 0.4551013340144081, alpha = 0.09003986854695609, newAccuracy = 0.8745387453874541, bestAccuracy = 0.8738007380073803\n",
      "Classifier 48: error = 0.46169739725258696, alpha = 0.07675558423232619, newAccuracy = 0.873062730627306, bestAccuracy = 0.8745387453874541\n",
      "Classifier 49: error = 0.44076921873422203, alpha = 0.11902040479796326, newAccuracy = 0.8763837638376378, bestAccuracy = 0.873062730627306\n",
      "Classifier 50: error = 0.461710164297483, alpha = 0.07672989946537918, newAccuracy = 0.8782287822878224, bestAccuracy = 0.8763837638376378\n",
      "Classifier 51: error = 0.4537368934091072, alpha = 0.09279161986530776, newAccuracy = 0.8771217712177116, bestAccuracy = 0.8782287822878224\n",
      "Classifier 52: error = 0.4621146365300877, alpha = 0.07591623352734592, newAccuracy = 0.8745387453874541, bestAccuracy = 0.8771217712177116\n",
      "Classifier 53: error = 0.45982599247073186, alpha = 0.080521591551821, newAccuracy = 0.8782287822878224, bestAccuracy = 0.8745387453874541\n",
      "Classifier 54: error = 0.45913054567332084, alpha = 0.08192168122682929, newAccuracy = 0.8804428044280435, bestAccuracy = 0.8782287822878224\n",
      "Classifier 55: error = 0.44698466890746946, alpha = 0.1064307141552736, newAccuracy = 0.8760147601476024, bestAccuracy = 0.8804428044280435\n",
      "Classifier 56: error = 0.4486083215340374, alpha = 0.10314761743221003, newAccuracy = 0.8774907749077486, bestAccuracy = 0.8760147601476024\n",
      "Classifier 57: error = 0.45839946491568717, alpha = 0.08339385571033496, newAccuracy = 0.8811808118081191, bestAccuracy = 0.8774907749077486\n",
      "Classifier 58: error = 0.46622636578410714, alpha = 0.06765028171540294, newAccuracy = 0.8800738007380079, bestAccuracy = 0.8811808118081191\n",
      "Classifier 59: error = 0.4612825504632221, alpha = 0.07759022904677793, newAccuracy = 0.8800738007380079, bestAccuracy = 0.8800738007380079\n",
      "Classifier 60: error = 0.46169023420498156, alpha = 0.07676999491036071, newAccuracy = 0.8808118081180817, bestAccuracy = 0.8800738007380079\n",
      "Classifier 61: error = 0.4621980653166765, alpha = 0.07574841459177029, newAccuracy = 0.8789667896678971, bestAccuracy = 0.8808118081180817\n",
      "Classifier 62: error = 0.46004503003059805, alpha = 0.0800806855202944, newAccuracy = 0.8808118081180817, bestAccuracy = 0.8789667896678971\n",
      "Classifier 63: error = 0.4620977714389294, alpha = 0.07595015856691394, newAccuracy = 0.8826568265682654, bestAccuracy = 0.8808118081180817\n",
      "Classifier 64: error = 0.45634295581489304, alpha = 0.08753699580679791, newAccuracy = 0.8808118081180817, bestAccuracy = 0.8826568265682654\n",
      "Classifier 65: error = 0.4563238399127053, alpha = 0.08757552144966077, newAccuracy = 0.8841328413284135, bestAccuracy = 0.8808118081180817\n",
      "Classifier 66: error = 0.4611448036442456, alpha = 0.07786739050984036, newAccuracy = 0.8815498154981546, bestAccuracy = 0.8841328413284135\n",
      "Classifier 67: error = 0.46992847066844445, alpha = 0.06021573268182989, newAccuracy = 0.8859778597785984, bestAccuracy = 0.8815498154981546\n",
      "Classifier 68: error = 0.4602480553424293, alpha = 0.07967203867901525, newAccuracy = 0.8848708487084876, bestAccuracy = 0.8859778597785984\n",
      "Classifier 69: error = 0.46365734815170123, alpha = 0.07281371350951545, newAccuracy = 0.8830258302583027, bestAccuracy = 0.8848708487084876\n",
      "Classifier 70: error = 0.4714876225808425, alpha = 0.05708668717105947, newAccuracy = 0.8841328413284135, bestAccuracy = 0.8830258302583027\n",
      "Classifier 71: error = 0.46910503267400916, alpha = 0.06186875318413, newAccuracy = 0.8837638376383764, bestAccuracy = 0.8841328413284135\n",
      "Classifier 72: error = 0.45177441052339484, alpha = 0.09675194917362732, newAccuracy = 0.8845018450184503, bestAccuracy = 0.8837638376383764\n",
      "Classifier 73: error = 0.45635572573376115, alpha = 0.0875112598217353, newAccuracy = 0.8859778597785984, bestAccuracy = 0.8845018450184503\n",
      "Classifier 74: error = 0.46670564566588896, alpha = 0.06668739068228767, newAccuracy = 0.8826568265682654, bestAccuracy = 0.8859778597785984\n",
      "Classifier 75: error = 0.47255472703993406, alpha = 0.05494577367659909, newAccuracy = 0.8852398523985247, bestAccuracy = 0.8826568265682654\n",
      "Classifier 76: error = 0.45177327122034083, alpha = 0.09675424917677854, newAccuracy = 0.8837638376383764, bestAccuracy = 0.8852398523985247\n",
      "Classifier 77: error = 0.4576495169228813, alpha = 0.08490439794633642, newAccuracy = 0.8845018450184503, bestAccuracy = 0.8837638376383764\n",
      "Classifier 78: error = 0.47031319961938717, alpha = 0.059443517114898196, newAccuracy = 0.887453874538745, bestAccuracy = 0.8845018450184503\n",
      "Classifier 79: error = 0.4596007743207833, alpha = 0.08097497122112043, newAccuracy = 0.8848708487084876, bestAccuracy = 0.887453874538745\n",
      "Classifier 80: error = 0.46892953887944233, alpha = 0.06222109366365147, newAccuracy = 0.8878228782287821, bestAccuracy = 0.8848708487084876\n",
      "Classifier 81: error = 0.4690387489153871, alpha = 0.062001829878496215, newAccuracy = 0.8856088560885602, bestAccuracy = 0.8878228782287821\n",
      "Classifier 82: error = 0.47122529917256123, alpha = 0.05761306145980329, newAccuracy = 0.887453874538745, bestAccuracy = 0.8856088560885602\n",
      "Classifier 83: error = 0.4636613599991527, alpha = 0.07280564720378793, newAccuracy = 0.8859778597785984, bestAccuracy = 0.887453874538745\n",
      "Classifier 84: error = 0.473938193900027, alpha = 0.05217089366396678, newAccuracy = 0.8867158671586709, bestAccuracy = 0.8859778597785984\n",
      "Classifier 85: error = 0.46052506925604364, alpha = 0.07911451129487446, newAccuracy = 0.8870848708487096, bestAccuracy = 0.8867158671586709\n",
      "Classifier 86: error = 0.46392539575177205, alpha = 0.0722747920526127, newAccuracy = 0.8863468634686339, bestAccuracy = 0.8870848708487096\n",
      "Classifier 87: error = 0.46426898783230386, alpha = 0.07158404634693036, newAccuracy = 0.887453874538745, bestAccuracy = 0.8863468634686339\n",
      "Classifier 88: error = 0.47326887277312457, alpha = 0.05351327746979684, newAccuracy = 0.887453874538745, bestAccuracy = 0.887453874538745\n",
      "Classifier 89: error = 0.46026975609817483, alpha = 0.07962836123886635, newAccuracy = 0.8885608856088558, bestAccuracy = 0.887453874538745\n",
      "Classifier 90: error = 0.4616647639566891, alpha = 0.07682123642297364, newAccuracy = 0.8863468634686339, bestAccuracy = 0.8885608856088558\n",
      "Classifier 91: error = 0.47493038929952225, alpha = 0.05018130057011744, newAccuracy = 0.8881918819188194, bestAccuracy = 0.8863468634686339\n",
      "Classifier 92: error = 0.4591169546142888, alpha = 0.08194904623815484, newAccuracy = 0.8900369003690044, bestAccuracy = 0.8881918819188194\n",
      "Classifier 93: error = 0.4722908953837477, alpha = 0.05547504705374922, newAccuracy = 0.8889298892988932, bestAccuracy = 0.8900369003690044\n",
      "Classifier 94: error = 0.47248694663990315, alpha = 0.05508174516753314, newAccuracy = 0.8926199261992618, bestAccuracy = 0.8889298892988932\n",
      "Classifier 95: error = 0.46240248316643306, alpha = 0.07533724133473628, newAccuracy = 0.8926199261992618, bestAccuracy = 0.8926199261992618\n",
      "Classifier 96: error = 0.4705073017694119, alpha = 0.05905394845752773, newAccuracy = 0.8889298892988932, bestAccuracy = 0.8926199261992618\n",
      "Classifier 97: error = 0.4669570422871514, alpha = 0.06618237503751036, newAccuracy = 0.8929889298892989, bestAccuracy = 0.8889298892988932\n",
      "Classifier 98: error = 0.4654249888093074, alpha = 0.06926055848389373, newAccuracy = 0.8915129151291505, bestAccuracy = 0.8929889298892989\n",
      "Classifier 99: error = 0.46733893117818903, alpha = 0.06541528567823433, newAccuracy = 0.8937269372693726, bestAccuracy = 0.8915129151291505\n",
      "Classifier 100: error = 0.46419809690968195, alpha = 0.07172655741553682, newAccuracy = 0.8911439114391152, bestAccuracy = 0.8937269372693726\n",
      "Classifier 101: error = 0.4556497694032403, alpha = 0.08893419052548654, newAccuracy = 0.8948339483394824, bestAccuracy = 0.8911439114391152\n",
      "Classifier 102: error = 0.47040181706080597, alpha = 0.05926565710540406, newAccuracy = 0.8926199261992618, bestAccuracy = 0.8948339483394824\n",
      "Classifier 103: error = 0.4670213555100159, alpha = 0.06605318547344187, newAccuracy = 0.8911439114391152, bestAccuracy = 0.8926199261992618\n",
      "Classifier 104: error = 0.46185618453167615, alpha = 0.07643614283843429, newAccuracy = 0.8918819188191877, bestAccuracy = 0.8911439114391152\n",
      "Classifier 105: error = 0.4764419966890726, alpha = 0.047150917682369405, newAccuracy = 0.8926199261992618, bestAccuracy = 0.8918819188191877\n",
      "Classifier 106: error = 0.4734340096757668, alpha = 0.053182062818913446, newAccuracy = 0.8907749077490769, bestAccuracy = 0.8926199261992618\n",
      "Classifier 107: error = 0.46833343878716016, alpha = 0.06341800495579995, newAccuracy = 0.8937269372693726, bestAccuracy = 0.8907749077490769\n",
      "Classifier 108: error = 0.47371882493720063, alpha = 0.05261083694785369, newAccuracy = 0.8915129151291505, bestAccuracy = 0.8937269372693726\n",
      "Classifier 109: error = 0.4707935793790354, alpha = 0.05847941361294837, newAccuracy = 0.8948339483394824, bestAccuracy = 0.8915129151291505\n",
      "Classifier 110: error = 0.46667365625021806, alpha = 0.06675165473791597, newAccuracy = 0.8933579335793362, bestAccuracy = 0.8948339483394824\n",
      "Classifier 111: error = 0.4723595725781695, alpha = 0.05533727057673623, newAccuracy = 0.8959409594095936, bestAccuracy = 0.8933579335793362\n",
      "Classifier 112: error = 0.47184974782548983, alpha = 0.05636010384236676, newAccuracy = 0.8915129151291505, bestAccuracy = 0.8959409594095936\n",
      "Classifier 113: error = 0.47256681970895775, alpha = 0.05492151528088817, newAccuracy = 0.8948339483394824, bestAccuracy = 0.8915129151291505\n",
      "Classifier 114: error = 0.4657614873381988, alpha = 0.0685843593939681, newAccuracy = 0.8948339483394824, bestAccuracy = 0.8948339483394824\n",
      "Classifier 115: error = 0.472109221231526, alpha = 0.05583952208097292, newAccuracy = 0.8970479704797044, bestAccuracy = 0.8948339483394824\n",
      "Classifier 116: error = 0.46954278236194225, alpha = 0.060989945840998795, newAccuracy = 0.8952029520295207, bestAccuracy = 0.8970479704797044\n",
      "Classifier 117: error = 0.4705569475439837, alpha = 0.05895431082470466, newAccuracy = 0.8955719557195582, bestAccuracy = 0.8952029520295207\n",
      "Classifier 118: error = 0.4771843719371327, alpha = 0.04566296705264956, newAccuracy = 0.8937269372693726, bestAccuracy = 0.8955719557195582\n",
      "Classifier 119: error = 0.4709419830102969, alpha = 0.058181595331238664, newAccuracy = 0.8955719557195582, bestAccuracy = 0.8937269372693726\n",
      "Classifier 120: error = 0.4748026563138865, alpha = 0.05043741368082497, newAccuracy = 0.8937269372693726, bestAccuracy = 0.8955719557195582\n",
      "Classifier 121: error = 0.4686022856892127, alpha = 0.06287816413612818, newAccuracy = 0.8970479704797044, bestAccuracy = 0.8937269372693726\n",
      "Classifier 122: error = 0.47225736101003246, alpha = 0.05554232266669611, newAccuracy = 0.8940959409594099, bestAccuracy = 0.8970479704797044\n",
      "Classifier 123: error = 0.47462673351746393, alpha = 0.05079016138954388, newAccuracy = 0.8955719557195582, bestAccuracy = 0.8940959409594099\n",
      "Classifier 124: error = 0.4619225814445447, alpha = 0.07630257301505719, newAccuracy = 0.894464944649447, bestAccuracy = 0.8955719557195582\n",
      "Classifier 125: error = 0.4683896491879228, alpha = 0.06330513221538155, newAccuracy = 0.896309963099632, bestAccuracy = 0.894464944649447\n",
      "Classifier 126: error = 0.47408226647814433, alpha = 0.05188196786508744, newAccuracy = 0.8970479704797044, bestAccuracy = 0.896309963099632\n",
      "Classifier 127: error = 0.4778314610603501, alpha = 0.0443661644463072, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8970479704797044\n",
      "Classifier 128: error = 0.4789939052008704, alpha = 0.042036933307779835, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8977859778597781\n",
      "Classifier 129: error = 0.4722971609537828, alpha = 0.05546247731851668, newAccuracy = 0.896309963099632, bestAccuracy = 0.8977859778597781\n",
      "Classifier 130: error = 0.47147932172178075, alpha = 0.05710334306696919, newAccuracy = 0.898523985239853, bestAccuracy = 0.896309963099632\n",
      "Classifier 131: error = 0.47023735649001885, alpha = 0.059595741370401814, newAccuracy = 0.8959409594095936, bestAccuracy = 0.898523985239853\n",
      "Classifier 132: error = 0.4702285363220672, alpha = 0.05961344445150806, newAccuracy = 0.8992619926199267, bestAccuracy = 0.8959409594095936\n",
      "Classifier 133: error = 0.4726312086424478, alpha = 0.054792349495821915, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8992619926199267\n",
      "Classifier 134: error = 0.47647834826070135, alpha = 0.04707805303504639, newAccuracy = 0.8981549815498155, bestAccuracy = 0.8977859778597781\n",
      "Classifier 135: error = 0.4686875351734535, alpha = 0.0627069920215246, newAccuracy = 0.8955719557195582, bestAccuracy = 0.8981549815498155\n",
      "Classifier 136: error = 0.4716190552040528, alpha = 0.056822968304711285, newAccuracy = 0.8970479704797044, bestAccuracy = 0.8955719557195582\n",
      "Classifier 137: error = 0.474066564042822, alpha = 0.051913457396712645, newAccuracy = 0.8966789667896673, bestAccuracy = 0.8970479704797044\n",
      "Classifier 138: error = 0.474265484069065, alpha = 0.05151455243136762, newAccuracy = 0.8974169741697418, bestAccuracy = 0.8966789667896673\n",
      "Classifier 139: error = 0.47036315466258677, alpha = 0.05934325417357231, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8974169741697418\n",
      "Classifier 140: error = 0.47630855484218615, alpha = 0.047418398531462826, newAccuracy = 0.8974169741697418, bestAccuracy = 0.8977859778597781\n",
      "Classifier 141: error = 0.4748933550512384, alpha = 0.050255556016421274, newAccuracy = 0.8974169741697418, bestAccuracy = 0.8974169741697418\n",
      "Classifier 142: error = 0.47751656319988367, alpha = 0.04499721838280697, newAccuracy = 0.8981549815498155, bestAccuracy = 0.8974169741697418\n",
      "Classifier 143: error = 0.4730311560191305, alpha = 0.05399008595315564, newAccuracy = 0.8996309963099638, bestAccuracy = 0.8981549815498155\n",
      "Classifier 144: error = 0.4623008639904993, alpha = 0.07554163853477162, newAccuracy = 0.898523985239853, bestAccuracy = 0.8996309963099638\n",
      "Classifier 145: error = 0.4706828515581555, alpha = 0.05870163034971571, newAccuracy = 0.8996309963099638, bestAccuracy = 0.898523985239853\n",
      "Classifier 146: error = 0.47535237738537306, alpha = 0.0493352330353521, newAccuracy = 0.898523985239853, bestAccuracy = 0.8996309963099638\n",
      "Classifier 147: error = 0.47175480552577664, alpha = 0.05655059428698393, newAccuracy = 0.8981549815498155, bestAccuracy = 0.898523985239853\n",
      "Classifier 148: error = 0.47467967038544734, alpha = 0.05068401487386351, newAccuracy = 0.8996309963099638, bestAccuracy = 0.8981549815498155\n",
      "Classifier 149: error = 0.4715589399678757, alpha = 0.0569435882275863, newAccuracy = 0.8981549815498155, bestAccuracy = 0.8996309963099638\n",
      "Classifier 150: error = 0.47524714892527054, alpha = 0.049546204813262745, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8981549815498155\n",
      "Classifier 151: error = 0.47519374823168115, alpha = 0.04965326916176974, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8977859778597781\n",
      "Classifier 152: error = 0.477505958857231, alpha = 0.04501847005968847, newAccuracy = 0.898523985239853, bestAccuracy = 0.8977859778597781\n",
      "Classifier 153: error = 0.47210041191985763, alpha = 0.055857195714640706, newAccuracy = 0.8959409594095936, bestAccuracy = 0.898523985239853\n",
      "Classifier 154: error = 0.4775466850529221, alpha = 0.044936852779757924, newAccuracy = 0.8970479704797044, bestAccuracy = 0.8959409594095936\n",
      "Classifier 155: error = 0.4737847076269206, alpha = 0.05247870743528567, newAccuracy = 0.8959409594095936, bestAccuracy = 0.8970479704797044\n",
      "Classifier 156: error = 0.4744403032638625, alpha = 0.05116399167915231, newAccuracy = 0.8988929889298893, bestAccuracy = 0.8959409594095936\n",
      "Classifier 157: error = 0.47178645622364246, alpha = 0.05648709046715811, newAccuracy = 0.896309963099632, bestAccuracy = 0.8988929889298893\n",
      "Classifier 158: error = 0.4665583311917736, alpha = 0.06698333769087872, newAccuracy = 0.898523985239853, bestAccuracy = 0.896309963099632\n",
      "Classifier 159: error = 0.47490157041125614, alpha = 0.05023908377729461, newAccuracy = 0.8970479704797044, bestAccuracy = 0.898523985239853\n",
      "Classifier 160: error = 0.4782909562423785, alpha = 0.04344540132505735, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8970479704797044\n",
      "Classifier 161: error = 0.46860788787720176, alpha = 0.06286691541135392, newAccuracy = 0.898523985239853, bestAccuracy = 0.8977859778597781\n",
      "Classifier 162: error = 0.4711915590992239, alpha = 0.057680766102778454, newAccuracy = 0.8981549815498155, bestAccuracy = 0.898523985239853\n",
      "Classifier 163: error = 0.4669247635921624, alpha = 0.066247215886766, newAccuracy = 0.8988929889298893, bestAccuracy = 0.8981549815498155\n",
      "Classifier 164: error = 0.47245371912546985, alpha = 0.0551484022694343, newAccuracy = 0.8988929889298893, bestAccuracy = 0.8988929889298893\n",
      "Classifier 165: error = 0.4702017776506481, alpha = 0.05966715237948168, newAccuracy = 0.8966789667896673, bestAccuracy = 0.8988929889298893\n",
      "Classifier 166: error = 0.47924328685561246, alpha = 0.04153729855894837, newAccuracy = 0.8977859778597781, bestAccuracy = 0.8966789667896673\n",
      "Classifier 167: error = 0.4724470136537016, alpha = 0.05516185405162734, newAccuracy = 0.8974169741697418, bestAccuracy = 0.8977859778597781\n",
      "Classifier 168: error = 0.47411825788479456, alpha = 0.05180979138888288, newAccuracy = 0.8988929889298893, bestAccuracy = 0.8974169741697418\n",
      "Classifier 169: error = 0.47532087613168406, alpha = 0.049398389209789825, newAccuracy = 0.8981549815498155, bestAccuracy = 0.8988929889298893\n",
      "Classifier 170: error = 0.47638912315407034, alpha = 0.047256900554088634, newAccuracy = 0.8999999999999991, bestAccuracy = 0.8981549815498155\n",
      "Classifier 171: error = 0.47935688918480934, alpha = 0.04130970381464384, newAccuracy = 0.8999999999999991, bestAccuracy = 0.8999999999999991\n",
      "Classifier 172: error = 0.480726058149344, alpha = 0.038566994006807166, newAccuracy = 0.898523985239853, bestAccuracy = 0.8999999999999991\n",
      "Classifier 173: error = 0.47416775311837683, alpha = 0.051710535477935166, newAccuracy = 0.8988929889298893, bestAccuracy = 0.898523985239853\n",
      "Classifier 174: error = 0.4763398990195257, alpha = 0.04735556930303204, newAccuracy = 0.9029520295202949, bestAccuracy = 0.8988929889298893\n",
      "Classifier 175: error = 0.475584843523045, alpha = 0.048869178886448386, newAccuracy = 0.8996309963099638, bestAccuracy = 0.9029520295202949\n",
      "Classifier 176: error = 0.47757041733926764, alpha = 0.04488929239789096, newAccuracy = 0.8988929889298893, bestAccuracy = 0.8996309963099638\n",
      "Classifier 177: error = 0.4760203828201742, alpha = 0.04799605533402675, newAccuracy = 0.8996309963099638, bestAccuracy = 0.8988929889298893\n",
      "Classifier 178: error = 0.4757922547607675, alpha = 0.04845337336323342, newAccuracy = 0.8999999999999991, bestAccuracy = 0.8996309963099638\n",
      "Classifier 179: error = 0.47531395904602564, alpha = 0.0494122571762219, newAccuracy = 0.8999999999999991, bestAccuracy = 0.8999999999999991\n",
      "Classifier 180: error = 0.4715508159459936, alpha = 0.056959889028888926, newAccuracy = 0.9007380073800728, bestAccuracy = 0.8999999999999991\n",
      "Classifier 181: error = 0.47808359462917815, alpha = 0.0438609153552896, newAccuracy = 0.9014760147601487, bestAccuracy = 0.9007380073800728\n",
      "Classifier 182: error = 0.48184442915803427, alpha = 0.036327113048299636, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9014760147601487\n",
      "Classifier 183: error = 0.47244376547849787, alpha = 0.05516837019172189, newAccuracy = 0.901845018450184, bestAccuracy = 0.9011070110701099\n",
      "Classifier 184: error = 0.48161695411358907, alpha = 0.036782671347450245, newAccuracy = 0.9003690036900375, bestAccuracy = 0.901845018450184\n",
      "Classifier 185: error = 0.46883712153116475, alpha = 0.06240664700668003, newAccuracy = 0.9007380073800728, bestAccuracy = 0.9003690036900375\n",
      "Classifier 186: error = 0.4793232166426954, alpha = 0.04137716407605856, newAccuracy = 0.9007380073800728, bestAccuracy = 0.9007380073800728\n",
      "Classifier 187: error = 0.4793732503469539, alpha = 0.04127692566256441, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9007380073800728\n",
      "Classifier 188: error = 0.47286048529524893, alpha = 0.05433242968892525, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9011070110701099\n",
      "Classifier 189: error = 0.475820635663589, alpha = 0.04839647834858454, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9011070110701099\n",
      "Classifier 190: error = 0.4715382115259752, alpha = 0.05698517978210046, newAccuracy = 0.9029520295202949, bestAccuracy = 0.9011070110701099\n",
      "Classifier 191: error = 0.4769922255542074, alpha = 0.04604806845304491, newAccuracy = 0.8988929889298893, bestAccuracy = 0.9029520295202949\n",
      "Classifier 192: error = 0.4759898412576574, alpha = 0.04805727945936629, newAccuracy = 0.9014760147601487, bestAccuracy = 0.8988929889298893\n",
      "Classifier 193: error = 0.46851713173269993, alpha = 0.06304914811466165, newAccuracy = 0.8992619926199267, bestAccuracy = 0.9014760147601487\n",
      "Classifier 194: error = 0.475122219124686, alpha = 0.049796681389506936, newAccuracy = 0.901845018450184, bestAccuracy = 0.8992619926199267\n",
      "Classifier 195: error = 0.48129550235369634, alpha = 0.037426460412175094, newAccuracy = 0.9007380073800728, bestAccuracy = 0.901845018450184\n",
      "Classifier 196: error = 0.4689766693695052, alpha = 0.06212646784046179, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9007380073800728\n",
      "Classifier 197: error = 0.4737298857307842, alpha = 0.052588654099693585, newAccuracy = 0.9014760147601487, bestAccuracy = 0.9011070110701099\n",
      "Classifier 198: error = 0.47346929085424283, alpha = 0.05311130096591996, newAccuracy = 0.8999999999999991, bestAccuracy = 0.9014760147601487\n",
      "Classifier 199: error = 0.47668491321788403, alpha = 0.04666401484750091, newAccuracy = 0.901845018450184, bestAccuracy = 0.8999999999999991\n",
      "Classifier 200: error = 0.4745922339929596, alpha = 0.05085933882734257, newAccuracy = 0.901845018450184, bestAccuracy = 0.901845018450184\n",
      "Classifier 201: error = 0.48096860235437, alpha = 0.03808119277158713, newAccuracy = 0.9011070110701099, bestAccuracy = 0.901845018450184\n",
      "Classifier 202: error = 0.4737082805499899, alpha = 0.05263198417127785, newAccuracy = 0.9014760147601487, bestAccuracy = 0.9011070110701099\n",
      "Classifier 203: error = 0.4675572227643604, alpha = 0.06497684415979839, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9014760147601487\n",
      "Classifier 204: error = 0.4798712143705462, alpha = 0.04027934053004621, newAccuracy = 0.8999999999999991, bestAccuracy = 0.9011070110701099\n",
      "Classifier 205: error = 0.46789483057660464, alpha = 0.0642988034952586, newAccuracy = 0.9014760147601487, bestAccuracy = 0.8999999999999991\n",
      "Classifier 206: error = 0.4790150763221741, alpha = 0.04199451627352399, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9014760147601487\n",
      "Classifier 207: error = 0.4767092003545818, alpha = 0.04661533483583106, newAccuracy = 0.901845018450184, bestAccuracy = 0.9011070110701099\n",
      "Classifier 208: error = 0.4734131220439753, alpha = 0.053223956441505, newAccuracy = 0.9029520295202949, bestAccuracy = 0.901845018450184\n",
      "Classifier 209: error = 0.47587806889255857, alpha = 0.04828134327867928, newAccuracy = 0.9029520295202949, bestAccuracy = 0.9029520295202949\n",
      "Classifier 210: error = 0.47660606660482197, alpha = 0.04682205287062542, newAccuracy = 0.9025830258302585, bestAccuracy = 0.9029520295202949\n",
      "Classifier 211: error = 0.4742515442395512, alpha = 0.05154250618170532, newAccuracy = 0.9029520295202949, bestAccuracy = 0.9025830258302585\n",
      "Classifier 212: error = 0.4783794210685808, alpha = 0.04326813886972624, newAccuracy = 0.8999999999999991, bestAccuracy = 0.9029520295202949\n",
      "Classifier 213: error = 0.4764621382309808, alpha = 0.047110545051231724, newAccuracy = 0.9014760147601487, bestAccuracy = 0.8999999999999991\n",
      "Classifier 214: error = 0.47723124580826926, alpha = 0.04556902410289753, newAccuracy = 0.9025830258302585, bestAccuracy = 0.9014760147601487\n",
      "Classifier 215: error = 0.4809695673107757, alpha = 0.03807926005884952, newAccuracy = 0.9025830258302585, bestAccuracy = 0.9025830258302585\n",
      "Classifier 216: error = 0.47381071258107044, alpha = 0.0524265543019646, newAccuracy = 0.9022140221402212, bestAccuracy = 0.9025830258302585\n",
      "Classifier 217: error = 0.47744112456868415, alpha = 0.04514840236903148, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9022140221402212\n",
      "Classifier 218: error = 0.4822444256952848, alpha = 0.0355260869194346, newAccuracy = 0.9011070110701099, bestAccuracy = 0.9011070110701099\n",
      "Classifier 219: error = 0.47533037541870415, alpha = 0.04937934425559542, newAccuracy = 0.9029520295202949, bestAccuracy = 0.9011070110701099\n",
      "Classifier 220: error = 0.48193585676076767, alpha = 0.036144017645413716, newAccuracy = 0.9033210332103322, bestAccuracy = 0.9029520295202949\n",
      "Classifier 221: error = 0.479084006120096, alpha = 0.04185641421351103, newAccuracy = 0.9036900369003693, bestAccuracy = 0.9033210332103322\n",
      "Classifier 222: error = 0.4771377652548727, alpha = 0.045756375310480615, newAccuracy = 0.9029520295202949, bestAccuracy = 0.9036900369003693\n",
      "Classifier 223: error = 0.4770814204385086, alpha = 0.045869301623642056, newAccuracy = 0.9040590405904048, bestAccuracy = 0.9029520295202949\n",
      "Classifier 224: error = 0.47504478341235934, alpha = 0.0499519383683889, newAccuracy = 0.9036900369003693, bestAccuracy = 0.9040590405904048\n",
      "Classifier 225: error = 0.4741399102486425, alpha = 0.05176637041380896, newAccuracy = 0.9040590405904048, bestAccuracy = 0.9036900369003693\n",
      "Classifier 226: error = 0.47576735135196757, alpha = 0.04850329732600237, newAccuracy = 0.9033210332103322, bestAccuracy = 0.9040590405904048\n",
      "Classifier 227: error = 0.4782302832116173, alpha = 0.04356697721408467, newAccuracy = 0.9033210332103322, bestAccuracy = 0.9033210332103322\n",
      "Classifier 228: error = 0.478849676788656, alpha = 0.04232590368179287, newAccuracy = 0.9044280442804434, bestAccuracy = 0.9033210332103322\n",
      "Classifier 229: error = 0.47902092023852644, alpha = 0.0419828078225017, newAccuracy = 0.9040590405904048, bestAccuracy = 0.9044280442804434\n",
      "Classifier 230: error = 0.47296560885418476, alpha = 0.054121563718313344, newAccuracy = 0.9044280442804434, bestAccuracy = 0.9040590405904048\n",
      "Classifier 231: error = 0.4755645796067185, alpha = 0.048909803665212095, newAccuracy = 0.9036900369003693, bestAccuracy = 0.9044280442804434\n",
      "Classifier 232: error = 0.4755057411883068, alpha = 0.049027762909970435, newAccuracy = 0.9022140221402212, bestAccuracy = 0.9036900369003693\n",
      "Classifier 233: error = 0.4775785400038157, alpha = 0.044873014323562506, newAccuracy = 0.9044280442804434, bestAccuracy = 0.9022140221402212\n",
      "Classifier 234: error = 0.4748049787423013, alpha = 0.05043275699886264, newAccuracy = 0.9059040590405896, bestAccuracy = 0.9044280442804434\n",
      "Classifier 235: error = 0.4768684048195073, alpha = 0.04629623823779378, newAccuracy = 0.9036900369003693, bestAccuracy = 0.9059040590405896\n",
      "Classifier 236: error = 0.4777281815100255, alpha = 0.04457313229785656, newAccuracy = 0.9051660516605159, bestAccuracy = 0.9036900369003693\n",
      "Classifier 237: error = 0.4766537158431505, alpha = 0.04672654574446848, newAccuracy = 0.9044280442804434, bestAccuracy = 0.9051660516605159\n",
      "Classifier 238: error = 0.48390991153742685, alpha = 0.03219129203439518, newAccuracy = 0.9051660516605159, bestAccuracy = 0.9044280442804434\n",
      "Classifier 239: error = 0.4728722925245492, alpha = 0.054308745481963516, newAccuracy = 0.9036900369003693, bestAccuracy = 0.9051660516605159\n",
      "Classifier 240: error = 0.4845411953188363, alpha = 0.030927466376321506, newAccuracy = 0.9047970479704806, bestAccuracy = 0.9036900369003693\n",
      "Classifier 241: error = 0.47693584248563003, alpha = 0.046161074459436416, newAccuracy = 0.9029520295202949, bestAccuracy = 0.9047970479704806\n",
      "Classifier 242: error = 0.4703647205530939, alpha = 0.05934011135124318, newAccuracy = 0.9044280442804434, bestAccuracy = 0.9029520295202949\n",
      "Classifier 243: error = 0.47979011209492706, alpha = 0.04044180945243536, newAccuracy = 0.9062730627306267, bestAccuracy = 0.9044280442804434\n",
      "Classifier 244: error = 0.4750195281804641, alpha = 0.05000257509856205, newAccuracy = 0.9040590405904048, bestAccuracy = 0.9062730627306267\n",
      "Classifier 245: error = 0.476372222105212, alpha = 0.0472907782495574, newAccuracy = 0.9047970479704806, bestAccuracy = 0.9040590405904048\n",
      "Classifier 246: error = 0.4811310033861578, alpha = 0.037755923475410326, newAccuracy = 0.9044280442804434, bestAccuracy = 0.9047970479704806\n",
      "Classifier 247: error = 0.48182862579264507, alpha = 0.03635876154391454, newAccuracy = 0.9036900369003693, bestAccuracy = 0.9044280442804434\n",
      "Classifier 248: error = 0.4751998468027225, alpha = 0.04964104193097578, newAccuracy = 0.9047970479704806, bestAccuracy = 0.9036900369003693\n",
      "Classifier 249: error = 0.4763361031456259, alpha = 0.047363178091173665, newAccuracy = 0.9047970479704806, bestAccuracy = 0.9047970479704806\n",
      "Classifier 250: error = 0.47147961689440554, alpha = 0.05710275079465355, newAccuracy = 0.9044280442804434, bestAccuracy = 0.9047970479704806\n",
      "Classifier 251: error = 0.4695180284411148, alpha = 0.061039638219514546, newAccuracy = 0.9062730627306267, bestAccuracy = 0.9044280442804434\n",
      "Classifier 252: error = 0.473598201827344, alpha = 0.05285275461012868, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9062730627306267\n",
      "Classifier 253: error = 0.4742373502184942, alpha = 0.05157096974887365, newAccuracy = 0.9059040590405896, bestAccuracy = 0.9066420664206654\n",
      "Classifier 254: error = 0.4797848438012876, alpha = 0.04045236328662119, newAccuracy = 0.9073800738007379, bestAccuracy = 0.9059040590405896\n",
      "Classifier 255: error = 0.4785305331921499, alpha = 0.04296535242096985, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9073800738007379\n",
      "Classifier 256: error = 0.47047333983370493, alpha = 0.059122109754153954, newAccuracy = 0.9062730627306267, bestAccuracy = 0.9066420664206654\n",
      "Classifier 257: error = 0.4743081228357183, alpha = 0.051429048768720134, newAccuracy = 0.9073800738007379, bestAccuracy = 0.9062730627306267\n",
      "Classifier 258: error = 0.4806084576375751, alpha = 0.038802547188029356, newAccuracy = 0.9073800738007379, bestAccuracy = 0.9073800738007379\n",
      "Classifier 259: error = 0.47992926020204807, alpha = 0.04016306095893282, newAccuracy = 0.9088560885608861, bestAccuracy = 0.9073800738007379\n",
      "Classifier 260: error = 0.47418864592434296, alpha = 0.051668638123182495, newAccuracy = 0.9070110701107008, bestAccuracy = 0.9088560885608861\n",
      "Classifier 261: error = 0.476783248163389, alpha = 0.04646691819986196, newAccuracy = 0.9088560885608861, bestAccuracy = 0.9070110701107008\n",
      "Classifier 262: error = 0.47840850789649547, alpha = 0.04320985638399228, newAccuracy = 0.9055350553505542, bestAccuracy = 0.9088560885608861\n",
      "Classifier 263: error = 0.4797557084899541, alpha = 0.040510729452775826, newAccuracy = 0.9077490774907753, bestAccuracy = 0.9055350553505542\n",
      "Classifier 264: error = 0.4760934399877283, alpha = 0.047849605175882624, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9077490774907753\n",
      "Classifier 265: error = 0.47702206995740926, alpha = 0.04598825315612309, newAccuracy = 0.9070110701107008, bestAccuracy = 0.9066420664206654\n",
      "Classifier 266: error = 0.4793448959258433, alpha = 0.04133373131249236, newAccuracy = 0.9073800738007379, bestAccuracy = 0.9070110701107008\n",
      "Classifier 267: error = 0.4745301008368048, alpha = 0.05098392764252369, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9073800738007379\n",
      "Classifier 268: error = 0.47759659329562154, alpha = 0.04483683504606512, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9066420664206654\n",
      "Classifier 269: error = 0.4854830162218108, alpha = 0.029042129946218322, newAccuracy = 0.9051660516605159, bestAccuracy = 0.9066420664206654\n",
      "Classifier 270: error = 0.48082477432949, alpha = 0.03836926934192759, newAccuracy = 0.9070110701107008, bestAccuracy = 0.9051660516605159\n",
      "Classifier 271: error = 0.47701879249421647, alpha = 0.0459948219574075, newAccuracy = 0.9059040590405896, bestAccuracy = 0.9070110701107008\n",
      "Classifier 272: error = 0.47389983375781874, alpha = 0.05224782326395125, newAccuracy = 0.9070110701107008, bestAccuracy = 0.9059040590405896\n",
      "Classifier 273: error = 0.47748854577283784, alpha = 0.045053366911857524, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9070110701107008\n",
      "Classifier 274: error = 0.4817194902201032, alpha = 0.036577323101277086, newAccuracy = 0.9073800738007379, bestAccuracy = 0.9066420664206654\n",
      "Classifier 275: error = 0.47923980798843197, alpha = 0.04154426830673761, newAccuracy = 0.9059040590405896, bestAccuracy = 0.9073800738007379\n",
      "Classifier 276: error = 0.4740468953722814, alpha = 0.05195290092818296, newAccuracy = 0.9081180811808116, bestAccuracy = 0.9059040590405896\n",
      "Classifier 277: error = 0.4729614168641674, alpha = 0.0541299722840385, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9081180811808116\n",
      "Classifier 278: error = 0.47974032667393934, alpha = 0.04054154363766484, newAccuracy = 0.908487084870849, bestAccuracy = 0.9066420664206654\n",
      "Classifier 279: error = 0.47958931303879426, alpha = 0.0408440713146465, newAccuracy = 0.9066420664206654, bestAccuracy = 0.908487084870849\n",
      "Classifier 280: error = 0.4814894216028889, alpha = 0.03703808402247577, newAccuracy = 0.9077490774907753, bestAccuracy = 0.9066420664206654\n",
      "Classifier 281: error = 0.47991440624816817, alpha = 0.040192816848987796, newAccuracy = 0.9062730627306267, bestAccuracy = 0.9077490774907753\n",
      "Classifier 282: error = 0.47936755772373085, alpha = 0.0412883303234011, newAccuracy = 0.9062730627306267, bestAccuracy = 0.9062730627306267\n",
      "Classifier 283: error = 0.4855926673708121, alpha = 0.028822644021923593, newAccuracy = 0.9066420664206654, bestAccuracy = 0.9062730627306267\n",
      "Classifier 284: error = 0.48275066118679993, alpha = 0.034512373702236074, newAccuracy = 0.9059040590405896, bestAccuracy = 0.9066420664206654\n",
      "Classifier 285: error = 0.4793670807021556, alpha = 0.041289285993895766, newAccuracy = 0.9070110701107008, bestAccuracy = 0.9059040590405896\n",
      "Classifier 286: error = 0.4797121892050764, alpha = 0.04059791124929826, newAccuracy = 0.9073800738007379, bestAccuracy = 0.9070110701107008\n",
      "Classifier 287: error = 0.48263475803802325, alpha = 0.034744458077137566, newAccuracy = 0.908487084870849, bestAccuracy = 0.9073800738007379\n",
      "Classifier 288: error = 0.48290597663604845, alpha = 0.034201375989897656, newAccuracy = 0.9099630996309973, bestAccuracy = 0.908487084870849\n",
      "Classifier 289: error = 0.47654946822487576, alpha = 0.046935498576318256, newAccuracy = 0.9099630996309973, bestAccuracy = 0.9099630996309973\n",
      "Classifier 290: error = 0.4795831279622337, alpha = 0.04085646212187275, newAccuracy = 0.9095940959409602, bestAccuracy = 0.9099630996309973\n",
      "Classifier 291: error = 0.477489465397684, alpha = 0.04505152392647195, newAccuracy = 0.9095940959409602, bestAccuracy = 0.9095940959409602\n",
      "Classifier 292: error = 0.47835983699133316, alpha = 0.04330738046454135, newAccuracy = 0.9095940959409602, bestAccuracy = 0.9095940959409602\n",
      "Classifier 293: error = 0.47653591255769734, alpha = 0.04696266971384231, newAccuracy = 0.9095940959409602, bestAccuracy = 0.9095940959409602\n",
      "Classifier 294: error = 0.4800247211161453, alpha = 0.039971832460182474, newAccuracy = 0.9099630996309973, bestAccuracy = 0.9095940959409602\n",
      "Classifier 295: error = 0.4742992396363761, alpha = 0.05144686221633623, newAccuracy = 0.9073800738007379, bestAccuracy = 0.9099630996309973\n",
      "Classifier 296: error = 0.4821405320737737, alpha = 0.03573413806173449, newAccuracy = 0.908487084870849, bestAccuracy = 0.9073800738007379\n",
      "Classifier 297: error = 0.48447780508546967, alpha = 0.031054368646833787, newAccuracy = 0.9077490774907753, bestAccuracy = 0.908487084870849\n",
      "Classifier 298: error = 0.4791526927788816, alpha = 0.04171880087423486, newAccuracy = 0.908487084870849, bestAccuracy = 0.9077490774907753\n",
      "Classifier 299: error = 0.4787123551347543, alpha = 0.04260104251112409, newAccuracy = 0.908487084870849, bestAccuracy = 0.908487084870849\n",
      "Classifier 300: error = 0.4801850595758377, alpha = 0.03965064701752958, newAccuracy = 0.9095940959409602, bestAccuracy = 0.908487084870849\n",
      "Classifier 301: error = 0.4815449709885613, alpha = 0.0369268332317131, newAccuracy = 0.908487084870849, bestAccuracy = 0.9095940959409602\n",
      "Classifier 302: error = 0.4763527100175925, alpha = 0.04732988983653262, newAccuracy = 0.9092250922509214, bestAccuracy = 0.908487084870849\n",
      "Classifier 303: error = 0.4747910146521681, alpha = 0.05046075631317199, newAccuracy = 0.910701107011071, bestAccuracy = 0.9092250922509214\n",
      "Se para el entrenamiento debido a que la precisión ha alcanzado el valor práctico buscado\n",
      "El número óptimo de clasificadores débiles es: 303 \n",
      "La precisión obtenida en el entrenamiento ha sido de: 0.9107\n",
      "Accuracy for digit 5: 0.9166\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_for_one_digit_detecting_overfitting(digit=5, A=200, \n",
    "                                                            verboseParam=True, n_components=50, \n",
    "                                                            split_proportion=0.75, iter_number=50,\n",
    "                                                            round1 = 4,\n",
    "                                                            round2 = 4,\n",
    "                                                            bestAccuracyBreak = 0.91,\n",
    "                                                            practicalAccuracyBreak = 6666)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAREA 2A: AdaboostClassifier de la librería sklearn utilizando como clasificador débil la clase \n",
    "DecisionTreeClassifier con profundidad 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución Minimalista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost with sklearn for digit: 0\n",
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Accuracy for digit 0 with sklearn: 0.9641\n",
      "Running AdaBoost with sklearn for digit: 1\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Accuracy for digit 1 with sklearn: 0.967\n",
      "Running AdaBoost with sklearn for digit: 2\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Accuracy for digit 2 with sklearn: 0.9355\n",
      "Running AdaBoost with sklearn for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Accuracy for digit 3 with sklearn: 0.8944\n",
      "Running AdaBoost with sklearn for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Accuracy for digit 4 with sklearn: 0.9005\n",
      "Running AdaBoost with sklearn for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Accuracy for digit 5 with sklearn: 0.8848\n",
      "Running AdaBoost with sklearn for digit: 6\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Accuracy for digit 6 with sklearn: 0.9457\n",
      "Running AdaBoost with sklearn for digit: 7\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Accuracy for digit 7 with sklearn: 0.9391\n",
      "Running AdaBoost with sklearn for digit: 8\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Accuracy for digit 8 with sklearn: 0.8889\n",
      "Running AdaBoost with sklearn for digit: 9\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Accuracy for digit 9 with sklearn: 0.8799\n",
      "Accuracies for all digits: {0: 0.9641, 1: 0.967, 2: 0.9355, 3: 0.8944, 4: 0.9005, 5: 0.8848, 6: 0.9457, 7: 0.9391, 8: 0.8889, 9: 0.8799}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #Importamos la librería numpy que sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # Importamos el dataset MNIST que contiene imágenes de dígitos escritos a mano\n",
    "from sklearn.ensemble import AdaBoostClassifier # Importamos el clasificador AdaBoost de la librería scikit-learn que se utilizará como clasificador fuerte\n",
    "from sklearn.tree import DecisionTreeClassifier # Importamos el clasificador DecisionTree de la librería scikit-learn que se utilizará como clasificador débil\n",
    "from sklearn.metrics import accuracy_score # Importamos la función accuracy_score de la librería scikit-learn que se utilizará para calcular la precisión\n",
    "\n",
    "def run_adaboost_with_sklearn(digit, T=50, A=20): # Creamos la función run_adaboost_with_sklearn\n",
    "    print(f\"Running AdaBoost with sklearn for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()  # Cargamos los datos\n",
    "\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train)\n",
    "\n",
    "    X_train_balanced = X_train_balanced.reshape(X_train_balanced.shape[0], -1)  # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)  # Aplanamos los datos de prueba reduciendo la dimensión a 1D\n",
    "\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "\n",
    "    weak_clf = DecisionTreeClassifier(max_depth=1, random_state=42) # Creamos un clasificador débil DecisionTree con profundidad 1\n",
    "\n",
    "\n",
    "    adaboost = AdaBoostClassifier(estimator=weak_clf, n_estimators=T, algorithm='SAMME', random_state=42) # Creamos el clasificador AdaBoost con el clasificador débil y el número de iteraciones\n",
    "\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced) # Ajustamos el clasificador AdaBoost\n",
    "    \n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred) # Calculamos la precisión\n",
    "    \n",
    "    print(f\"Accuracy for digit {digit} with sklearn: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits_sklearn(T=50, A=20): # Creamos la función run_adaboost_for_all_digits_sklearn\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_with_sklearn(digit, T, A) # Ejecutamos AdaBoost con sklearn\n",
    "        accuracies[digit] = accuracy  # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "if __name__ == \"__main__\": # Si el script se ejecuta de forma independiente\n",
    "    accuracies = run_adaboost_for_all_digits_sklearn(T=50, A=20)  # Ejecutamos AdaBoost con sklearn para todos los dígitos\n",
    "    print(\"Accuracies for all digits:\", accuracies) # Imprimimos las precisiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2B: clasificador débil árboles de decisión de \n",
    "profundidad mayor que 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución Minimalista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost with sklearn for digit: 0\n",
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Accuracy for digit 0 with sklearn: 0.9804\n",
      "Running AdaBoost with sklearn for digit: 1\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Accuracy for digit 1 with sklearn: 0.983\n",
      "Running AdaBoost with sklearn for digit: 2\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Accuracy for digit 2 with sklearn: 0.9548\n",
      "Running AdaBoost with sklearn for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Accuracy for digit 3 with sklearn: 0.9303\n",
      "Running AdaBoost with sklearn for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Accuracy for digit 4 with sklearn: 0.9508\n",
      "Running AdaBoost with sklearn for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Accuracy for digit 5 with sklearn: 0.9494\n",
      "Running AdaBoost with sklearn for digit: 6\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Accuracy for digit 6 with sklearn: 0.977\n",
      "Running AdaBoost with sklearn for digit: 7\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Accuracy for digit 7 with sklearn: 0.9636\n",
      "Running AdaBoost with sklearn for digit: 8\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Accuracy for digit 8 with sklearn: 0.9468\n",
      "Running AdaBoost with sklearn for digit: 9\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Accuracy for digit 9 with sklearn: 0.9232\n",
      "Accuracies for all digits: {0: 0.9804, 1: 0.983, 2: 0.9548, 3: 0.9303, 4: 0.9508, 5: 0.9494, 6: 0.977, 7: 0.9636, 8: 0.9468, 9: 0.9232}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #Importamos la librería numpy que sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # Importamos el dataset MNIST que contiene imágenes de dígitos escritos a mano\n",
    "from sklearn.ensemble import AdaBoostClassifier # Importamos el clasificador AdaBoost de la librería scikit-learn que se utilizará como clasificador fuerte\n",
    "from sklearn.tree import DecisionTreeClassifier # Importamos el clasificador DecisionTree de la librería scikit-learn que se utilizará como clasificador débil\n",
    "from sklearn.metrics import accuracy_score # Importamos la función accuracy_score de la librería scikit-learn que se utilizará para calcular la precisión\n",
    "\n",
    "def run_adaboost_with_sklearn(digit, T=50, A=20, max_depth = 1): # Creamos la función run_adaboost_with_sklearn\n",
    "    print(f\"Running AdaBoost with sklearn for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()  # Cargamos los datos\n",
    "\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train)\n",
    "\n",
    "    X_train_balanced = X_train_balanced.reshape(X_train_balanced.shape[0], -1)  # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)  # Aplanamos los datos de prueba reduciendo la dimensión a 1D\n",
    "\n",
    "      # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "\n",
    "    weak_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42) # Creamos un clasificador débil DecisionTree con profundidad 1\n",
    "\n",
    "\n",
    "    adaboost = AdaBoostClassifier(estimator=weak_clf, n_estimators=T, algorithm='SAMME', random_state=42) # Creamos el clasificador AdaBoost con el clasificador débil y el número de iteraciones\n",
    "\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced) # Ajustamos el clasificador AdaBoost\n",
    "    \n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred) # Calculamos la precisión\n",
    "    \n",
    "    print(f\"Accuracy for digit {digit} with sklearn: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits_sklearn(T=50, A=20, max_depth = 1): # Creamos la función run_adaboost_for_all_digits_sklearn\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito \n",
    "        accuracy = run_adaboost_with_sklearn(digit, T, A, max_depth=max_depth) # Ejecutamos AdaBoost con sklearn\n",
    "        accuracies[digit] = accuracy  # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "if __name__ == \"__main__\": # Si el script se ejecuta de forma independiente\n",
    "    accuracies = run_adaboost_for_all_digits_sklearn(T=50, A=20, max_depth=2)  # Ejecutamos AdaBoost con sklearn para todos los dígitos\n",
    "    print(\"Accuracies for all digits:\", accuracies) # Imprimimos las precisiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2C:  Utilización de la librería Keras para implementar un Multi-Layer \n",
    "Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1 (less fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    Y_train = to_categorical(Y_train, 10)\n",
    "    Y_test = to_categorical(Y_test, 10)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def create_mlp_model(input_shape, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    X_train, Y_train, X_test, Y_test = load_and_preprocess_data()\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    model = create_mlp_model(input_shape, learning_rate)\n",
    "    \n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    validation_split = 0.1\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2 (Faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8676 - loss: 0.4463 - val_accuracy: 0.9682 - val_loss: 0.1007\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9685 - loss: 0.0994 - val_accuracy: 0.9740 - val_loss: 0.0834\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0667 - val_accuracy: 0.9775 - val_loss: 0.0707\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9838 - loss: 0.0505 - val_accuracy: 0.9769 - val_loss: 0.0736\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0419 - val_accuracy: 0.9804 - val_loss: 0.0677\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.0355 - val_accuracy: 0.9815 - val_loss: 0.0644\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9886 - loss: 0.0339 - val_accuracy: 0.9823 - val_loss: 0.0678\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9910 - loss: 0.0262 - val_accuracy: 0.9837 - val_loss: 0.0632\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0278 - val_accuracy: 0.9830 - val_loss: 0.0681\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0238 - val_accuracy: 0.9818 - val_loss: 0.0720\n",
      "Test loss: 0.0720\n",
      "Test accuracy:  0.9818\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    Y_train = to_categorical(Y_train, 10)\n",
    "    Y_test = to_categorical(Y_test, 10)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def create_mlp_model(input_shape, learning_rate, num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.2))  # Dropout layer to prevent overfitting\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    X_train, Y_train, X_test, Y_test = load_and_preprocess_data()\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "    learning_rate = 0.001\n",
    "    num_layers = 2  # Specify the number of layers here\n",
    "    \n",
    "    model = create_mlp_model(input_shape, learning_rate, num_layers)\n",
    "    \n",
    "    batch_size = 128  # Updated to match the image's batch size\n",
    "    epochs = 10\n",
    "    validation_split = 0.1\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, verbose=True, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=False)\n",
    "    print(f'Test loss: {score[0]:.4f}')\n",
    "    print(f'Test accuracy: {score[1]: .4f}')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2D:  Modelado de un clasificador mediante CNN para MNIST \n",
    "con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - accuracy: 0.8324 - loss: 0.5472 - val_accuracy: 0.5864 - val_loss: 1.3409\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9659 - loss: 0.1171 - val_accuracy: 0.9799 - val_loss: 0.0595\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.9735 - loss: 0.0866 - val_accuracy: 0.9881 - val_loss: 0.0352\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9790 - loss: 0.0731 - val_accuracy: 0.9910 - val_loss: 0.0288\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.9825 - loss: 0.0603 - val_accuracy: 0.9915 - val_loss: 0.0265\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9826 - loss: 0.0575 - val_accuracy: 0.9903 - val_loss: 0.0278\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9844 - loss: 0.0524 - val_accuracy: 0.9912 - val_loss: 0.0242\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9852 - loss: 0.0478 - val_accuracy: 0.9927 - val_loss: 0.0225\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9862 - loss: 0.0462 - val_accuracy: 0.9909 - val_loss: 0.0258\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9878 - loss: 0.0391 - val_accuracy: 0.9918 - val_loss: 0.0263\n",
      "Test loss: 0.0263\n",
      "Test accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Reshape the data to fit the model\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "    \n",
    "    # Normalize the data to the range [0, 1]\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def build_cnn_model(num_conv_layers=2, num_dense_layers=1, conv_filters=[32, 64], dense_units=[128], input_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add convolutional layers\n",
    "    for i in range(num_conv_layers):\n",
    "        if i == 0:\n",
    "            # First layer needs to specify the input shape\n",
    "            model.add(Conv2D(conv_filters[i], kernel_size=(3, 3), input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(Conv2D(conv_filters[i], kernel_size=(3, 3)))\n",
    "        \n",
    "        # Add batch normalization\n",
    "        model.add(BatchNormalization())\n",
    "        # Add a LeakyReLU activation function with negative_slope\n",
    "        model.add(LeakyReLU(negative_slope=0.1))\n",
    "        # Add a max pooling layer\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        # Add a dropout layer to prevent overfitting\n",
    "        model.add(Dropout(0.25))\n",
    "    \n",
    "    # Flatten the layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(negative_slope=0.1))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # Add the output layer with 10 units and softmax activation function\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss: {:.4f}'.format(score[0]))\n",
    "    print('Test accuracy: {:.4f}'.format(score[1]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the data\n",
    "    (x_train, y_train), (x_test, y_test) = load_and_preprocess_data()\n",
    "    \n",
    "    # Build the CNN model\n",
    "    model = build_cnn_model(num_conv_layers=2, num_dense_layers=1, conv_filters=[32, 64], dense_units=[128])\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIPrac2Python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
