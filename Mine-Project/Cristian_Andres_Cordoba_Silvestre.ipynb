{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n",
      "(60000,) uint8\n",
      "(10000, 28, 28) uint8\n",
      "(10000,) uint8\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(X_train.shape, X_train.dtype)\n",
    "print(Y_train.shape, Y_train.dtype)\n",
    "print(X_test.shape, X_test.dtype)\n",
    "print(Y_test.shape, Y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image(imagen, title):\n",
    "    plt.figure()\n",
    "    plt.suptitle(title)\n",
    "    plt.imshow(imagen, cmap = \"Reds\")\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(3):\n",
    "    title = \"Mostrando imagen X_train[\" + str(i) + \"]\"\n",
    "    title = title + \" -- Y_train[\" + str(i) + \"] = \" + str(Y_train[i])\n",
    "    show_image(X_train[i], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_X(X, title, xscale, yscale):\n",
    "    plt.title(title)\n",
    "    plt.plot(X)\n",
    "    plt.xscale(xscale)\n",
    "    plt.yscale(yscale)\n",
    "    plt.show()\n",
    "\n",
    "# Example values for fila and columna\n",
    "fila = 1\n",
    "columna = 1\n",
    "\n",
    "features_fila_col = X_train[:, fila, columna]\n",
    "print(len(np.unique(features_fila_col)))\n",
    "\n",
    "title = \"Valores en (\" + str(fila) + \", \" + str(columna) + \")\"\n",
    "xscale = \"linear\"  # Define the xscale\n",
    "yscale = \"linear\"  # Define the yscale\n",
    "\n",
    "plot_X(features_fila_col, title, xscale, yscale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 1A: DecisionStump y ADABoost Binario sin mejoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def balance_training_dataset(digit, X_train, y_train):\n",
    "    # Find indices of the target digit and calculate how many there are\n",
    "    target_indices = np.where(y_train == digit)[0]\n",
    "    num_target_samples = len(target_indices)\n",
    "    print(f\"Total elements for target digit {digit}: {num_target_samples}\")\n",
    "\n",
    "    # Determine how many samples each non-target digit should have\n",
    "    num_samples_per_other_digit = num_target_samples // 9\n",
    "    print(f\"Each non-target digit will have {num_samples_per_other_digit} samples.\")\n",
    "\n",
    "    # Initialize lists to collect balanced dataset samples\n",
    "    X_train_balanced = []\n",
    "    y_train_balanced = []\n",
    "\n",
    "    # Add target digit samples to the balanced dataset\n",
    "    X_train_balanced.extend(X_train[target_indices])\n",
    "    y_train_balanced.extend(y_train[target_indices])\n",
    "\n",
    "    # Collect samples for each non-target digit class to balance the dataset\n",
    "    for i in range(10):  # There are 10 digit classes (0-9)\n",
    "        if i == digit:\n",
    "            continue  # Skip the target digit\n",
    "        indices = np.where(y_train == i)[0]\n",
    "        if len(indices) >= num_samples_per_other_digit:\n",
    "            balanced_indices = np.random.choice(indices, num_samples_per_other_digit, replace=False)\n",
    "        else:\n",
    "            balanced_indices = np.random.choice(indices, num_samples_per_other_digit, replace=True)\n",
    "        X_train_balanced.extend(X_train[balanced_indices])\n",
    "        y_train_balanced.extend([i] * num_samples_per_other_digit)\n",
    "        print(f\"Collected {len(balanced_indices)} samples for digit {i}.\")\n",
    "\n",
    "    # Convert lists to numpy arrays for training\n",
    "    X_train_balanced = np.array(X_train_balanced)\n",
    "    y_train_balanced = np.array(y_train_balanced)\n",
    "\n",
    "    # Convert labels to binary (1 for the specified digit, -1 for all others)\n",
    "    y_train_binary_balanced = np.where(y_train_balanced == digit, 1, -1)\n",
    "\n",
    "    # Shuffle the balanced training set\n",
    "    shuffle_indices = np.random.permutation(len(X_train_balanced))\n",
    "    X_train_balanced = X_train_balanced[shuffle_indices]\n",
    "    y_train_binary_balanced = y_train_binary_balanced[shuffle_indices]\n",
    "\n",
    "    return X_train_balanced, y_train_binary_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, img_shape): # Inicializamos la clase\n",
    "        self.feature_index = (np.random.randint(0, img_shape[0]), np.random.randint(0, img_shape[1])) # Elegimos un índice de característica aleatorio en 2D\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.img_shape = img_shape # Inicializamos la forma de la imagen\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index[0], self.feature_index[1]] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, img_rows, img_cols = X.shape # Obtenemos el número de muestras y el tamaño de la imagen\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump((img_rows, img_cols)) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index[0], clf.feature_index[1]]), max(X[:, clf.feature_index[0], clf.feature_index[1]])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train) # Convertimos las etiquetas a binarias\n",
    "    \n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A) # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.4079119086460033, alpha = 0.18630209289483984\n",
      "Classifier 2/50: error = 0.41637255962372965, alpha = 0.16884119956704424\n",
      "Classifier 3/50: error = 0.32631601000240573, alpha = 0.3624474377777707\n",
      "Classifier 4/50: error = 0.3395251458907941, alpha = 0.3327055202103747\n",
      "Classifier 5/50: error = 0.2625715923845365, alpha = 0.51632261554835\n",
      "Classifier 6/50: error = 0.4255071393813248, alpha = 0.150102971675958\n",
      "Classifier 7/50: error = 0.31738618759556725, alpha = 0.3829049906432699\n",
      "Classifier 8/50: error = 0.3542920403873021, alpha = 0.3001128903297031\n",
      "Classifier 9/50: error = 0.4005003177010318, alpha = 0.20169044233605252\n",
      "Classifier 10/50: error = 0.4161909233934201, alpha = 0.16921495012426588\n",
      "Classifier 11/50: error = 0.42815512247478027, alpha = 0.14469109789864346\n",
      "Classifier 12/50: error = 0.40110967215882154, alpha = 0.20042180130813922\n",
      "Classifier 13/50: error = 0.41284894472169786, alpha = 0.17610017312407192\n",
      "Classifier 14/50: error = 0.4361372792535177, alpha = 0.12842688221151535\n",
      "Classifier 15/50: error = 0.4548063407416575, alpha = 0.0906346829972233\n",
      "Classifier 16/50: error = 0.42547639202527454, alpha = 0.1501658629458141\n",
      "Classifier 17/50: error = 0.44496962239592014, alpha = 0.11050841553423255\n",
      "Classifier 18/50: error = 0.4254161480976607, alpha = 0.15028909052419193\n",
      "Classifier 19/50: error = 0.417106290365453, alpha = 0.16733188557154163\n",
      "Classifier 20/50: error = 0.415090065861796, alpha = 0.17148117685700395\n",
      "Classifier 21/50: error = 0.4314859157767795, alpha = 0.13789560867140493\n",
      "Classifier 22/50: error = 0.45775679836501293, alpha = 0.08468828856274223\n",
      "Classifier 23/50: error = 0.45042257222615967, alpha = 0.09948173910970141\n",
      "Classifier 24/50: error = 0.47189710079904124, alpha = 0.056265097254746356\n",
      "Classifier 25/50: error = 0.4179318745479923, alpha = 0.16563452758151886\n",
      "Classifier 26/50: error = 0.4540311154861092, alpha = 0.09219812700413982\n",
      "Classifier 27/50: error = 0.4528182580839182, alpha = 0.0946450753163122\n",
      "Classifier 28/50: error = 0.44182171841846796, alpha = 0.11688598181586374\n",
      "Classifier 29/50: error = 0.40236051977744003, alpha = 0.19781959704876223\n",
      "Classifier 30/50: error = 0.44603600734258597, alpha = 0.10835000333742988\n",
      "Classifier 31/50: error = 0.4463186765954533, alpha = 0.10777803714961388\n",
      "Classifier 32/50: error = 0.43909295579982266, alpha = 0.12242203002597266\n",
      "Classifier 33/50: error = 0.4523061954762653, alpha = 0.09567850288824792\n",
      "Classifier 34/50: error = 0.435130341214656, alpha = 0.13047469544071877\n",
      "Classifier 35/50: error = 0.4264329995703497, alpha = 0.14820974798452172\n",
      "Classifier 36/50: error = 0.4560758273300396, alpha = 0.08807538246808755\n",
      "Classifier 37/50: error = 0.43515325038097097, alpha = 0.13042809295857216\n",
      "Classifier 38/50: error = 0.44393750357355954, alpha = 0.11259844836265137\n",
      "Classifier 39/50: error = 0.4745375197100169, alpha = 0.05096905130528754\n",
      "Classifier 40/50: error = 0.4582407930262343, alpha = 0.08371342009582557\n",
      "Classifier 41/50: error = 0.4503597620849662, alpha = 0.09960860831107408\n",
      "Classifier 42/50: error = 0.46504922879697785, alpha = 0.07001572891344722\n",
      "Classifier 43/50: error = 0.46535117039134644, alpha = 0.06940890619340356\n",
      "Classifier 44/50: error = 0.4769995611176008, alpha = 0.04603336620519404\n",
      "Classifier 45/50: error = 0.455110882423587, alpha = 0.09002061652173755\n",
      "Classifier 46/50: error = 0.4262023105335544, alpha = 0.1486813679318299\n",
      "Classifier 47/50: error = 0.45594269908320606, alpha = 0.08834371606385137\n",
      "Classifier 48/50: error = 0.4627573802376012, alpha = 0.0746234491791701\n",
      "Classifier 49/50: error = 0.46526413714653203, alpha = 0.0695838147343629\n",
      "Classifier 50/50: error = 0.4547876652636056, alpha = 0.09067234174746912\n",
      "Accuracy for digit 3: 0.8778\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=50, A=20, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n",
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Classifier 1/50: error = 0.23410721823554248, alpha = 0.5926314900275834\n",
      "Classifier 2/50: error = 0.1496936437055697, alpha = 0.86850293643049\n",
      "Classifier 3/50: error = 0.3038599010556179, alpha = 0.4144920940443126\n",
      "Classifier 4/50: error = 0.3320986777698548, alpha = 0.34935414715398183\n",
      "Classifier 5/50: error = 0.2825268254289056, alpha = 0.46598102800705965\n",
      "Classifier 6/50: error = 0.29736481471336773, alpha = 0.4299390498422789\n",
      "Classifier 7/50: error = 0.294875641007898, alpha = 0.43591023561891123\n",
      "Classifier 8/50: error = 0.3437150359413196, alpha = 0.32339108036563813\n",
      "Classifier 9/50: error = 0.35389902023792263, alpha = 0.30097209399197633\n",
      "Classifier 10/50: error = 0.32852145082325734, alpha = 0.35743996777939385\n",
      "Classifier 11/50: error = 0.34922742903635795, alpha = 0.31121842929227184\n",
      "Classifier 12/50: error = 0.3924665229543559, alpha = 0.21847801905265662\n",
      "Classifier 13/50: error = 0.38955225730630944, alpha = 0.2245973354148732\n",
      "Classifier 14/50: error = 0.38304556155216085, alpha = 0.23832061781275646\n",
      "Classifier 15/50: error = 0.4149602552004614, alpha = 0.17174851981958\n",
      "Classifier 16/50: error = 0.41174213106693547, alpha = 0.17838407318516425\n",
      "Classifier 17/50: error = 0.40809458273447763, alpha = 0.1859239429727298\n",
      "Classifier 18/50: error = 0.39613697432884276, alpha = 0.21079367361159213\n",
      "Classifier 19/50: error = 0.39048346520649135, alpha = 0.22264022557719385\n",
      "Classifier 20/50: error = 0.4249130271780066, alpha = 0.15131839016556328\n",
      "Classifier 21/50: error = 0.4459418191910541, alpha = 0.10854060371683916\n",
      "Classifier 22/50: error = 0.372508346096984, alpha = 0.26073546352866916\n",
      "Classifier 23/50: error = 0.42923246222086525, alpha = 0.1424916851671819\n",
      "Classifier 24/50: error = 0.4003489697944869, alpha = 0.20200563917572198\n",
      "Classifier 25/50: error = 0.4070068584301312, alpha = 0.18817639836201067\n",
      "Classifier 26/50: error = 0.4318132472465336, alpha = 0.13722847897446938\n",
      "Classifier 27/50: error = 0.425681358009081, alpha = 0.14974664357959974\n",
      "Classifier 28/50: error = 0.41829646015267696, alpha = 0.1648852601140502\n",
      "Classifier 29/50: error = 0.4497559129869836, alpha = 0.10082847714400704\n",
      "Classifier 30/50: error = 0.4453922569496717, alpha = 0.10965286217940937\n",
      "Classifier 31/50: error = 0.4237351530371627, alpha = 0.15372936885529648\n",
      "Classifier 32/50: error = 0.4485753127012271, alpha = 0.10321434044044529\n",
      "Classifier 33/50: error = 0.4484877684482348, alpha = 0.10339130405496415\n",
      "Classifier 34/50: error = 0.4315530129860091, alpha = 0.1377588488665133\n",
      "Classifier 35/50: error = 0.4515653368321244, alpha = 0.09717404024564383\n",
      "Classifier 36/50: error = 0.4234949913154886, alpha = 0.15422117027978996\n",
      "Classifier 37/50: error = 0.4029533794271945, alpha = 0.19658716511168825\n",
      "Classifier 38/50: error = 0.4325226327540175, alpha = 0.1357831059652766\n",
      "Classifier 39/50: error = 0.4249079980801349, alpha = 0.15132868044544162\n",
      "Classifier 40/50: error = 0.41930609542084984, alpha = 0.16281129218617277\n",
      "Classifier 41/50: error = 0.44000505823955494, alpha = 0.12057076411192662\n",
      "Classifier 42/50: error = 0.455226402450202, alpha = 0.0897877039898979\n",
      "Classifier 43/50: error = 0.41179755339536883, alpha = 0.17826966609233616\n",
      "Classifier 44/50: error = 0.4528109804243369, alpha = 0.09465976142763896\n",
      "Classifier 45/50: error = 0.4441966335015135, alpha = 0.1120736208176783\n",
      "Classifier 46/50: error = 0.43691260129795495, alpha = 0.12685083772903855\n",
      "Classifier 47/50: error = 0.423426966049683, alpha = 0.15436048536630698\n",
      "Classifier 48/50: error = 0.4120228465754563, alpha = 0.17780464584292283\n",
      "Classifier 49/50: error = 0.43956916021718906, alpha = 0.1214553893777128\n",
      "Classifier 50/50: error = 0.43661825494834217, alpha = 0.1274490993080592\n",
      "Accuracy for digit 0: 0.9629\n",
      "Running AdaBoost for digit: 1\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Classifier 1/50: error = 0.23689089965141275, alpha = 0.5849006566548804\n",
      "Classifier 2/50: error = 0.2521397577281262, alpha = 0.5436162952158983\n",
      "Classifier 3/50: error = 0.24828296207565304, alpha = 0.5538954508277015\n",
      "Classifier 4/50: error = 0.2551741406670811, alpha = 0.5356021141376324\n",
      "Classifier 5/50: error = 0.296157434613921, alpha = 0.43283175780790845\n",
      "Classifier 6/50: error = 0.34107124745464223, alpha = 0.3292620108705951\n",
      "Classifier 7/50: error = 0.3656302764526721, alpha = 0.2755046476566485\n",
      "Classifier 8/50: error = 0.29094998430397057, alpha = 0.4453873453431137\n",
      "Classifier 9/50: error = 0.33116169592226663, alpha = 0.35146778470363893\n",
      "Classifier 10/50: error = 0.30871146443769004, alpha = 0.4030751150688055\n",
      "Classifier 11/50: error = 0.3520114865561975, alpha = 0.3051045812854169\n",
      "Classifier 12/50: error = 0.32258549052091023, alpha = 0.37095758568284176\n",
      "Classifier 13/50: error = 0.3957233999118979, alpha = 0.21165827902918227\n",
      "Classifier 14/50: error = 0.39144597955775073, alpha = 0.2206190810700594\n",
      "Classifier 15/50: error = 0.42330278041114533, alpha = 0.15461483169026402\n",
      "Classifier 16/50: error = 0.39304033010932515, alpha = 0.21727505965966437\n",
      "Classifier 17/50: error = 0.4161838771586266, alpha = 0.1692294500147825\n",
      "Classifier 18/50: error = 0.4095024192833704, alpha = 0.18301136368717763\n",
      "Classifier 19/50: error = 0.4286038030456036, alpha = 0.14377493893599427\n",
      "Classifier 20/50: error = 0.41087335672526193, alpha = 0.18017807099494706\n",
      "Classifier 21/50: error = 0.42061323632712133, alpha = 0.16012825362107347\n",
      "Classifier 22/50: error = 0.41783945673177414, alpha = 0.1658244866100426\n",
      "Classifier 23/50: error = 0.3924750013329622, alpha = 0.21846024001219502\n",
      "Classifier 24/50: error = 0.4096059582056253, alpha = 0.18279728066446643\n",
      "Classifier 25/50: error = 0.4286786425715874, alpha = 0.14362214778775398\n",
      "Classifier 26/50: error = 0.42762601636928277, alpha = 0.1457717875696625\n",
      "Classifier 27/50: error = 0.43683660414586234, alpha = 0.12700529395188642\n",
      "Classifier 28/50: error = 0.4105050724149337, alpha = 0.18093891488509467\n",
      "Classifier 29/50: error = 0.43144489055878554, alpha = 0.13797923018540875\n",
      "Classifier 30/50: error = 0.4307580530302594, alpha = 0.139379493723744\n",
      "Classifier 31/50: error = 0.40905542181930743, alpha = 0.18393579202807758\n",
      "Classifier 32/50: error = 0.4458900481778526, alpha = 0.10864537156325814\n",
      "Classifier 33/50: error = 0.44731159908192797, alpha = 0.10576946546579663\n",
      "Classifier 34/50: error = 0.4522971528367433, alpha = 0.09569675426432277\n",
      "Classifier 35/50: error = 0.41997785421414335, alpha = 0.16143215165478303\n",
      "Classifier 36/50: error = 0.45631914573418214, alpha = 0.08758498200252993\n",
      "Classifier 37/50: error = 0.434252477431305, alpha = 0.1322608976217807\n",
      "Classifier 38/50: error = 0.4617405998169062, alpha = 0.07666866963284\n",
      "Classifier 39/50: error = 0.44910191243922803, alpha = 0.10214999724176688\n",
      "Classifier 40/50: error = 0.4531768930438981, alpha = 0.09392141040147754\n",
      "Classifier 41/50: error = 0.4467030055091597, alpha = 0.10700048064535807\n",
      "Classifier 42/50: error = 0.42899248513110677, alpha = 0.14298148440249034\n",
      "Classifier 43/50: error = 0.42756524016279207, alpha = 0.14589594347908938\n",
      "Classifier 44/50: error = 0.44696463738196723, alpha = 0.106471232910572\n",
      "Classifier 45/50: error = 0.4082243009040397, alpha = 0.1856554479446426\n",
      "Classifier 46/50: error = 0.4583105430151463, alpha = 0.08357294187246929\n",
      "Classifier 47/50: error = 0.4500112491779014, alpha = 0.10031262215059751\n",
      "Classifier 48/50: error = 0.42320915252511426, alpha = 0.15480660538000382\n",
      "Classifier 49/50: error = 0.43354637774102034, alpha = 0.13369821750257854\n",
      "Classifier 50/50: error = 0.4521189221176832, alpha = 0.09605650246906842\n",
      "Accuracy for digit 1: 0.9681\n",
      "Running AdaBoost for digit: 2\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3254447801275596, alpha = 0.36443036952376157\n",
      "Classifier 2/50: error = 0.33004173317375796, alpha = 0.3539981556723662\n",
      "Classifier 3/50: error = 0.2831091251883602, alpha = 0.46454560359845376\n",
      "Classifier 4/50: error = 0.2959038352229323, alpha = 0.43344021189650833\n",
      "Classifier 5/50: error = 0.35795499673055964, alpha = 0.2921255643088859\n",
      "Classifier 6/50: error = 0.33337555469338553, alpha = 0.3464785951527084\n",
      "Classifier 7/50: error = 0.3748792725394011, alpha = 0.2556703803355589\n",
      "Classifier 8/50: error = 0.40917025809682045, alpha = 0.18369827133614555\n",
      "Classifier 9/50: error = 0.3839545803973228, alpha = 0.23639821423085447\n",
      "Classifier 10/50: error = 0.39192523848886496, alpha = 0.2196133669335395\n",
      "Classifier 11/50: error = 0.3938045060884896, alpha = 0.21567396122498633\n",
      "Classifier 12/50: error = 0.41599114875055965, alpha = 0.1696260779501373\n",
      "Classifier 13/50: error = 0.43318865575844345, alpha = 0.13442659719745453\n",
      "Classifier 14/50: error = 0.42561318330700737, alpha = 0.14988607630348139\n",
      "Classifier 15/50: error = 0.4098934107080794, alpha = 0.18220301415425144\n",
      "Classifier 16/50: error = 0.3824983868431363, alpha = 0.2394786224724355\n",
      "Classifier 17/50: error = 0.3887827193219505, alpha = 0.2262159426547903\n",
      "Classifier 18/50: error = 0.42335624474601385, alpha = 0.15450532821545532\n",
      "Classifier 19/50: error = 0.4078905621772431, alpha = 0.1863462852218801\n",
      "Classifier 20/50: error = 0.4110804126783506, alpha = 0.1797504017707116\n",
      "Classifier 21/50: error = 0.41009423362897257, alpha = 0.18178791732244223\n",
      "Classifier 22/50: error = 0.4213042149893984, alpha = 0.158710875641723\n",
      "Classifier 23/50: error = 0.4041725291465995, alpha = 0.1940546537507921\n",
      "Classifier 24/50: error = 0.4347696058598167, alpha = 0.13120858806833482\n",
      "Classifier 25/50: error = 0.4425727406862402, alpha = 0.11536359076895823\n",
      "Classifier 26/50: error = 0.42337858729538347, alpha = 0.15445956820436885\n",
      "Classifier 27/50: error = 0.44531336913496544, alpha = 0.10981254526514067\n",
      "Classifier 28/50: error = 0.42482380701352573, alpha = 0.15150095257513094\n",
      "Classifier 29/50: error = 0.43605930447992125, alpha = 0.1285854212908733\n",
      "Classifier 30/50: error = 0.4393929889627415, alpha = 0.1218129705117575\n",
      "Classifier 31/50: error = 0.44148324356645063, alpha = 0.117572277335808\n",
      "Classifier 32/50: error = 0.46235154214162655, alpha = 0.07543970352088145\n",
      "Classifier 33/50: error = 0.44396573133220546, alpha = 0.11254127441556046\n",
      "Classifier 34/50: error = 0.44222807156831856, alpha = 0.11606220017646407\n",
      "Classifier 35/50: error = 0.4445887106296642, alpha = 0.11127964608377798\n",
      "Classifier 36/50: error = 0.4455168433758041, alpha = 0.10940068824815606\n",
      "Classifier 37/50: error = 0.4492548825835351, alpha = 0.10184086318127318\n",
      "Classifier 38/50: error = 0.4380374547710726, alpha = 0.12456539111297049\n",
      "Classifier 39/50: error = 0.45624139697299226, alpha = 0.08774167756600044\n",
      "Classifier 40/50: error = 0.4483181764159385, alpha = 0.10373413897645684\n",
      "Classifier 41/50: error = 0.4370074279971349, alpha = 0.12665812087230377\n",
      "Classifier 42/50: error = 0.45643761762784196, alpha = 0.0873462209121606\n",
      "Classifier 43/50: error = 0.4519377639870026, alpha = 0.09642218488483408\n",
      "Classifier 44/50: error = 0.4621633949544236, alpha = 0.07581815431038019\n",
      "Classifier 45/50: error = 0.47013688739678217, alpha = 0.059797396488006416\n",
      "Classifier 46/50: error = 0.4439201132007881, alpha = 0.11263367207861766\n",
      "Classifier 47/50: error = 0.4464261948756604, alpha = 0.10756049809245152\n",
      "Classifier 48/50: error = 0.44753930829673394, alpha = 0.1053089554767824\n",
      "Classifier 49/50: error = 0.46159765052219714, alpha = 0.07695625839019464\n",
      "Classifier 50/50: error = 0.46998675533006606, alpha = 0.06009874099742871\n",
      "Accuracy for digit 2: 0.9138\n",
      "Running AdaBoost for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.33882544861337677, alpha = 0.3342664013041526\n",
      "Classifier 2/50: error = 0.33756473119958974, alpha = 0.33708277870067177\n",
      "Classifier 3/50: error = 0.318593828528017, alpha = 0.3801207676869641\n",
      "Classifier 4/50: error = 0.34779446677549997, alpha = 0.31437402727161906\n",
      "Classifier 5/50: error = 0.3235237843638905, alpha = 0.36881232781847084\n",
      "Classifier 6/50: error = 0.4093519112494586, alpha = 0.18332259233785206\n",
      "Classifier 7/50: error = 0.35495520349619913, alpha = 0.2986640860114018\n",
      "Classifier 8/50: error = 0.33649930933734873, alpha = 0.3394668989851612\n",
      "Classifier 9/50: error = 0.3558885085755766, alpha = 0.29662716534066913\n",
      "Classifier 10/50: error = 0.41525039659447893, alpha = 0.17115101179909112\n",
      "Classifier 11/50: error = 0.4169185134918649, alpha = 0.16771807806400266\n",
      "Classifier 12/50: error = 0.41227630154718287, alpha = 0.17728158870007502\n",
      "Classifier 13/50: error = 0.39603313776100757, alpha = 0.21101072176600538\n",
      "Classifier 14/50: error = 0.4211299690138173, alpha = 0.15906823987371407\n",
      "Classifier 15/50: error = 0.42178472761396957, alpha = 0.15772559155093807\n",
      "Classifier 16/50: error = 0.4253162210593411, alpha = 0.15049349899093392\n",
      "Classifier 17/50: error = 0.3925059449519437, alpha = 0.21839535283269818\n",
      "Classifier 18/50: error = 0.41707042782916737, alpha = 0.16740563865939898\n",
      "Classifier 19/50: error = 0.4459662075158459, alpha = 0.10849125042963181\n",
      "Classifier 20/50: error = 0.4286402221024618, alpha = 0.14370058555474793\n",
      "Classifier 21/50: error = 0.4030701312845246, alpha = 0.1963445318641015\n",
      "Classifier 22/50: error = 0.4400491571181694, alpha = 0.12048127893215582\n",
      "Classifier 23/50: error = 0.43589793228565377, alpha = 0.12891354525816015\n",
      "Classifier 24/50: error = 0.417653162116038, alpha = 0.1662074395794328\n",
      "Classifier 25/50: error = 0.42338196272113526, alpha = 0.15445265501508118\n",
      "Classifier 26/50: error = 0.43630401818992165, alpha = 0.12808788849317648\n",
      "Classifier 27/50: error = 0.43525816359932556, alpha = 0.1302146826792195\n",
      "Classifier 28/50: error = 0.4124968177443843, alpha = 0.17682658567541173\n",
      "Classifier 29/50: error = 0.4431774694056958, alpha = 0.1141381373112661\n",
      "Classifier 30/50: error = 0.4319315323087313, alpha = 0.13698743372696304\n",
      "Classifier 31/50: error = 0.45387036632029987, alpha = 0.0925223756601209\n",
      "Classifier 32/50: error = 0.45031383897032606, alpha = 0.09970136969954363\n",
      "Classifier 33/50: error = 0.4461418598880134, alpha = 0.10813580809493181\n",
      "Classifier 34/50: error = 0.4324773420588379, alpha = 0.13587536884733772\n",
      "Classifier 35/50: error = 0.43727029617935, alpha = 0.1261239412026115\n",
      "Classifier 36/50: error = 0.45294678703216945, alpha = 0.09438571423316135\n",
      "Classifier 37/50: error = 0.4539652021085153, alpha = 0.09233107915660349\n",
      "Classifier 38/50: error = 0.4531617999573609, alpha = 0.09395186372451027\n",
      "Classifier 39/50: error = 0.45345652234214073, alpha = 0.0933572336813734\n",
      "Classifier 40/50: error = 0.4588761946454001, alpha = 0.08243382639189253\n",
      "Classifier 41/50: error = 0.4487815569649868, alpha = 0.1027974598142341\n",
      "Classifier 42/50: error = 0.45099846939812904, alpha = 0.09831864187320809\n",
      "Classifier 43/50: error = 0.46236020983969645, alpha = 0.07542226930200815\n",
      "Classifier 44/50: error = 0.4600007469188705, alpha = 0.0801698215617835\n",
      "Classifier 45/50: error = 0.43999118890765965, alpha = 0.12059890807411817\n",
      "Classifier 46/50: error = 0.45681844862349186, alpha = 0.0865787842982815\n",
      "Classifier 47/50: error = 0.46023258616989815, alpha = 0.07970317390231477\n",
      "Classifier 48/50: error = 0.4612665618320544, alpha = 0.07762239928679851\n",
      "Classifier 49/50: error = 0.45814812182184894, alpha = 0.08390006732106126\n",
      "Classifier 50/50: error = 0.4631235182072667, alpha = 0.07388712812265337\n",
      "Accuracy for digit 3: 0.8903\n",
      "Running AdaBoost for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3076264658050159, alpha = 0.40561966274690453\n",
      "Classifier 2/50: error = 0.28315488934713007, alpha = 0.46443286637605025\n",
      "Classifier 3/50: error = 0.34514568808045554, alpha = 0.32022308701114255\n",
      "Classifier 4/50: error = 0.34840430498677794, alpha = 0.31303033421397675\n",
      "Classifier 5/50: error = 0.36457829196256775, alpha = 0.2777737821757389\n",
      "Classifier 6/50: error = 0.27905297172057797, alpha = 0.47458201898801483\n",
      "Classifier 7/50: error = 0.38387667034362116, alpha = 0.23656291192794338\n",
      "Classifier 8/50: error = 0.4045193827868909, alpha = 0.19333459309217993\n",
      "Classifier 9/50: error = 0.3702609013158027, alpha = 0.26554885429515773\n",
      "Classifier 10/50: error = 0.38457164761169227, alpha = 0.23509421096471658\n",
      "Classifier 11/50: error = 0.3952814196336986, alpha = 0.21258261365525885\n",
      "Classifier 12/50: error = 0.4178569829939972, alpha = 0.16578846156575558\n",
      "Classifier 13/50: error = 0.41001606688906156, alpha = 0.18194947898718658\n",
      "Classifier 14/50: error = 0.39609636430218087, alpha = 0.2108785578621738\n",
      "Classifier 15/50: error = 0.38225630085387585, alpha = 0.23999115838347923\n",
      "Classifier 16/50: error = 0.4122487490669351, alpha = 0.1773384443282543\n",
      "Classifier 17/50: error = 0.4290714479864075, alpha = 0.14282031173157714\n",
      "Classifier 18/50: error = 0.41415311776183966, alpha = 0.17341135357012538\n",
      "Classifier 19/50: error = 0.426212383068003, alpha = 0.14866077430443356\n",
      "Classifier 20/50: error = 0.3641265704981904, alpha = 0.27874900242784373\n",
      "Classifier 21/50: error = 0.40102961572351153, alpha = 0.20058843780839106\n",
      "Classifier 22/50: error = 0.4284331813878757, alpha = 0.14412330225214035\n",
      "Classifier 23/50: error = 0.39703966742854724, alpha = 0.20890760868099323\n",
      "Classifier 24/50: error = 0.4358863487797745, alpha = 0.1289370994842104\n",
      "Classifier 25/50: error = 0.4133723147922419, alpha = 0.17502083797556947\n",
      "Classifier 26/50: error = 0.4264466486501517, alpha = 0.1481818459011261\n",
      "Classifier 27/50: error = 0.4546622018070553, alpha = 0.09092534310262458\n",
      "Classifier 28/50: error = 0.43529715057698526, alpha = 0.1301353799328693\n",
      "Classifier 29/50: error = 0.44770082793313065, alpha = 0.10498233162547917\n",
      "Classifier 30/50: error = 0.43635829260958786, alpha = 0.12797755053623738\n",
      "Classifier 31/50: error = 0.4146197854783443, alpha = 0.17244982734211295\n",
      "Classifier 32/50: error = 0.43576349340992293, alpha = 0.12918692580480295\n",
      "Classifier 33/50: error = 0.4286315137624639, alpha = 0.14371836441424116\n",
      "Classifier 34/50: error = 0.42776865701249706, alpha = 0.14548041344407894\n",
      "Classifier 35/50: error = 0.4375228753328428, alpha = 0.12561073751831495\n",
      "Classifier 36/50: error = 0.3992109250338247, alpha = 0.20437700233716594\n",
      "Classifier 37/50: error = 0.42600262588755244, alpha = 0.14908965561296417\n",
      "Classifier 38/50: error = 0.4517780587650119, alpha = 0.09674458418029731\n",
      "Classifier 39/50: error = 0.4470124185626369, alpha = 0.10637458413335031\n",
      "Classifier 40/50: error = 0.4537722190474468, alpha = 0.09272035898281257\n",
      "Classifier 41/50: error = 0.444459446858223, alpha = 0.11154139584924848\n",
      "Classifier 42/50: error = 0.4447710633155091, alpha = 0.11091042092686298\n",
      "Classifier 43/50: error = 0.4395911699443954, alpha = 0.12141071761615208\n",
      "Classifier 44/50: error = 0.456938368842299, alpha = 0.08633714658563393\n",
      "Classifier 45/50: error = 0.4510916090428003, alpha = 0.09813055955993948\n",
      "Classifier 46/50: error = 0.4462346164323159, alpha = 0.10794812106012601\n",
      "Classifier 47/50: error = 0.44751712046560366, alpha = 0.10535382529724589\n",
      "Classifier 48/50: error = 0.4604134440009233, alpha = 0.0793391660597632\n",
      "Classifier 49/50: error = 0.44446400705136146, alpha = 0.1115321615299502\n",
      "Classifier 50/50: error = 0.46204336339381624, alpha = 0.07605960447106344\n",
      "Accuracy for digit 4: 0.9202\n",
      "Running AdaBoost for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1/50: error = 0.31718793246609456, alpha = 0.3833626095429275\n",
      "Classifier 2/50: error = 0.34012613974156136, alpha = 0.33136607387232786\n",
      "Classifier 3/50: error = 0.39903362972458933, alpha = 0.20474663912138064\n",
      "Classifier 4/50: error = 0.38427754003733816, alpha = 0.23571562893097514\n",
      "Classifier 5/50: error = 0.3906292598182236, alpha = 0.22233396279949794\n",
      "Classifier 6/50: error = 0.37140667880084555, alpha = 0.2630934338938649\n",
      "Classifier 7/50: error = 0.4162419123079273, alpha = 0.1691100261609595\n",
      "Classifier 8/50: error = 0.4104303171600413, alpha = 0.18109337812531517\n",
      "Classifier 9/50: error = 0.3739826678966672, alpha = 0.25758430192839493\n",
      "Classifier 10/50: error = 0.3918694349200954, alpha = 0.2197304469463369\n",
      "Classifier 11/50: error = 0.3798989199068974, alpha = 0.2449886399829901\n",
      "Classifier 12/50: error = 0.4199960908289174, alpha = 0.16139471985915366\n",
      "Classifier 13/50: error = 0.42559980963585176, alpha = 0.1499134291719235\n",
      "Classifier 14/50: error = 0.4241882234865073, alpha = 0.15280177514996438\n",
      "Classifier 15/50: error = 0.4169765185829573, alpha = 0.16759877624177444\n",
      "Classifier 16/50: error = 0.3947283176217231, alpha = 0.21373984708022495\n",
      "Classifier 17/50: error = 0.44155019739235524, alpha = 0.1174365122578956\n",
      "Classifier 18/50: error = 0.4431683925483183, alpha = 0.11415652859101712\n",
      "Classifier 19/50: error = 0.44341932852860977, alpha = 0.11364811725732847\n",
      "Classifier 20/50: error = 0.42960885808222615, alpha = 0.14172358848538255\n",
      "Classifier 21/50: error = 0.4262443819860444, alpha = 0.14859535229623927\n",
      "Classifier 22/50: error = 0.43466133187378064, alpha = 0.1314288918417009\n",
      "Classifier 23/50: error = 0.42893905757907336, alpha = 0.1430905406373348\n",
      "Classifier 24/50: error = 0.4457017789407238, alpha = 0.10902638787229571\n",
      "Classifier 25/50: error = 0.45295690751621726, alpha = 0.0943652924482432\n",
      "Classifier 26/50: error = 0.4319613801604709, alpha = 0.13692661127916017\n",
      "Classifier 27/50: error = 0.4517252046009749, alpha = 0.09685128607597815\n",
      "Classifier 28/50: error = 0.4490320183226484, alpha = 0.10229125122478666\n",
      "Classifier 29/50: error = 0.4298284150515747, alpha = 0.14127562370453153\n",
      "Classifier 30/50: error = 0.4367358304372375, alpha = 0.1272101152184766\n",
      "Classifier 31/50: error = 0.4550746787063127, alpha = 0.09009361279044584\n",
      "Classifier 32/50: error = 0.46691295799074006, alpha = 0.06627093090082617\n",
      "Classifier 33/50: error = 0.4404622801604301, alpha = 0.11964306518667218\n",
      "Classifier 34/50: error = 0.42965848742529844, alpha = 0.1416223241909538\n",
      "Classifier 35/50: error = 0.4562250238899784, alpha = 0.08777467657463542\n",
      "Classifier 36/50: error = 0.45816300572028956, alpha = 0.08387008956515636\n",
      "Classifier 37/50: error = 0.4433366861990117, alpha = 0.1138155490963148\n",
      "Classifier 38/50: error = 0.4419507844179261, alpha = 0.116624315002685\n",
      "Classifier 39/50: error = 0.4597194592061564, alpha = 0.08073604623283151\n",
      "Classifier 40/50: error = 0.42753671482183425, alpha = 0.14595421765313604\n",
      "Classifier 41/50: error = 0.4562258552520825, alpha = 0.08777300100745881\n",
      "Classifier 42/50: error = 0.42346849129075537, alpha = 0.1542754413676705\n",
      "Classifier 43/50: error = 0.44948746148664487, alpha = 0.10137088664383191\n",
      "Classifier 44/50: error = 0.46880756372077204, alpha = 0.06246599337715509\n",
      "Classifier 45/50: error = 0.4578766677886274, alpha = 0.08444683108961022\n",
      "Classifier 46/50: error = 0.4645161844850262, alpha = 0.07108713290907628\n",
      "Classifier 47/50: error = 0.43283592726109066, alpha = 0.13514494820238693\n",
      "Classifier 48/50: error = 0.463248744112309, alpha = 0.07363531120072221\n",
      "Classifier 49/50: error = 0.4518422310199921, alpha = 0.09661503629357045\n",
      "Classifier 50/50: error = 0.4590783866343713, alpha = 0.08202670186967023\n",
      "Accuracy for digit 5: 0.8609\n",
      "Running AdaBoost for digit: 6\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Classifier 1/50: error = 0.33953173865269215, alpha = 0.33269082055630894\n",
      "Classifier 2/50: error = 0.27788719034713005, alpha = 0.4774830656528017\n",
      "Classifier 3/50: error = 0.2768231013546968, alpha = 0.480137593093766\n",
      "Classifier 4/50: error = 0.2524350317659726, alpha = 0.5428336497898764\n",
      "Classifier 5/50: error = 0.35100324100022795, alpha = 0.30731613284083237\n",
      "Classifier 6/50: error = 0.2625526302586394, alpha = 0.5163715821006452\n",
      "Classifier 7/50: error = 0.3474356649746788, alpha = 0.3151651104201893\n",
      "Classifier 8/50: error = 0.38000341107949964, alpha = 0.24476687348793402\n",
      "Classifier 9/50: error = 0.34611006885743645, alpha = 0.31809109679888753\n",
      "Classifier 10/50: error = 0.3081247314345422, alpha = 0.4044505091343827\n",
      "Classifier 11/50: error = 0.3857516674962558, alpha = 0.23260274132121825\n",
      "Classifier 12/50: error = 0.371096819960434, alpha = 0.2637571595662532\n",
      "Classifier 13/50: error = 0.35900189943860206, alpha = 0.2898494070806896\n",
      "Classifier 14/50: error = 0.38796982230151644, alpha = 0.2279270160710404\n",
      "Classifier 15/50: error = 0.40319998360646087, alpha = 0.1960747000897917\n",
      "Classifier 16/50: error = 0.37449224768730494, alpha = 0.2564963105998524\n",
      "Classifier 17/50: error = 0.40268669006891133, alpha = 0.19714148366032103\n",
      "Classifier 18/50: error = 0.4015035068924989, alpha = 0.19960219880187377\n",
      "Classifier 19/50: error = 0.37032034703916034, alpha = 0.26542138436565316\n",
      "Classifier 20/50: error = 0.4071452613720725, alpha = 0.1878896897217734\n",
      "Classifier 21/50: error = 0.42763384820954486, alpha = 0.14575578871788739\n",
      "Classifier 22/50: error = 0.42059708024409126, alpha = 0.16016140158532233\n",
      "Classifier 23/50: error = 0.41073265796714786, alpha = 0.1804687181899451\n",
      "Classifier 24/50: error = 0.4402952931509492, alpha = 0.11998185639858237\n",
      "Classifier 25/50: error = 0.4192938701149156, alpha = 0.1628363967717674\n",
      "Classifier 26/50: error = 0.42127464805405956, alpha = 0.15877151217137442\n",
      "Classifier 27/50: error = 0.3961789828408001, alpha = 0.21070586935640473\n",
      "Classifier 28/50: error = 0.4525600285252618, alpha = 0.09516620021675298\n",
      "Classifier 29/50: error = 0.42282827312722304, alpha = 0.1555868597363586\n",
      "Classifier 30/50: error = 0.43277008650390336, alpha = 0.135279051862865\n",
      "Classifier 31/50: error = 0.44329463169154915, alpha = 0.11390075319336124\n",
      "Classifier 32/50: error = 0.4492889375490412, alpha = 0.10177204487893204\n",
      "Classifier 33/50: error = 0.4411793848404909, alpha = 0.11818847870428298\n",
      "Classifier 34/50: error = 0.4442810590964232, alpha = 0.11190264313960638\n",
      "Classifier 35/50: error = 0.4504020248410311, alpha = 0.09952324209336562\n",
      "Classifier 36/50: error = 0.4237587329405985, alpha = 0.1536810860821434\n",
      "Classifier 37/50: error = 0.431125613006596, alpha = 0.1386300777851962\n",
      "Classifier 38/50: error = 0.43839172602903353, alpha = 0.1238458615302812\n",
      "Classifier 39/50: error = 0.44435017511138397, alpha = 0.11176267508542402\n",
      "Classifier 40/50: error = 0.44830219851960373, alpha = 0.10376643998116458\n",
      "Classifier 41/50: error = 0.42917067821242316, alpha = 0.14261778137158093\n",
      "Classifier 42/50: error = 0.46055872047702295, alpha = 0.07904678708220221\n",
      "Classifier 43/50: error = 0.4476862786606346, alpha = 0.1050117521444336\n",
      "Classifier 44/50: error = 0.45047512256173106, alpha = 0.09937559597845719\n",
      "Classifier 45/50: error = 0.43879667310440673, alpha = 0.12302356488721536\n",
      "Classifier 46/50: error = 0.45457392361481597, alpha = 0.09110336605503422\n",
      "Classifier 47/50: error = 0.42453248531553234, alpha = 0.1520971252499174\n",
      "Classifier 48/50: error = 0.45314513177991356, alpha = 0.09398549531061558\n",
      "Classifier 49/50: error = 0.4596910244647345, alpha = 0.08079328747868998\n",
      "Classifier 50/50: error = 0.44910339974325464, alpha = 0.10214699148761347\n",
      "Accuracy for digit 6: 0.952\n",
      "Running AdaBoost for digit: 7\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Classifier 1/50: error = 0.28948838694229406, alpha = 0.44893505504410147\n",
      "Classifier 2/50: error = 0.3522342076516271, alpha = 0.30461644117014497\n",
      "Classifier 3/50: error = 0.3396658166682961, alpha = 0.33239190150497677\n",
      "Classifier 4/50: error = 0.3297361775202584, alpha = 0.354689263480267\n",
      "Classifier 5/50: error = 0.36952563215562195, alpha = 0.2671261964742678\n",
      "Classifier 6/50: error = 0.3516893695319878, alpha = 0.30581081814954175\n",
      "Classifier 7/50: error = 0.35879492278825786, alpha = 0.29029917966385604\n",
      "Classifier 8/50: error = 0.3153105752156222, alpha = 0.38770361732139574\n",
      "Classifier 9/50: error = 0.33469524101667036, alpha = 0.34351241512501496\n",
      "Classifier 10/50: error = 0.3630395230875939, alpha = 0.2810979502491192\n",
      "Classifier 11/50: error = 0.35722336481805744, alpha = 0.293718013343464\n",
      "Classifier 12/50: error = 0.3932843564224815, alpha = 0.21676365787887505\n",
      "Classifier 13/50: error = 0.37603743927350697, alpha = 0.25320082832960544\n",
      "Classifier 14/50: error = 0.3736014428068094, alpha = 0.2583986364923303\n",
      "Classifier 15/50: error = 0.38724179438236883, alpha = 0.22946056203496729\n",
      "Classifier 16/50: error = 0.3534417120275749, alpha = 0.30197238486873706\n",
      "Classifier 17/50: error = 0.4067911257135717, alpha = 0.188623360512942\n",
      "Classifier 18/50: error = 0.40624870618358777, alpha = 0.18974749273397426\n",
      "Classifier 19/50: error = 0.4016663127604007, alpha = 0.19926346374861922\n",
      "Classifier 20/50: error = 0.39225778782152876, alpha = 0.21891577659609607\n",
      "Classifier 21/50: error = 0.40344730202244083, alpha = 0.1955608529299162\n",
      "Classifier 22/50: error = 0.42455907926210745, alpha = 0.15204269786250094\n",
      "Classifier 23/50: error = 0.43881184574212184, alpha = 0.12299275813668867\n",
      "Classifier 24/50: error = 0.4052126001696183, alpha = 0.19189608117091558\n",
      "Classifier 25/50: error = 0.3720949035814711, alpha = 0.2616200478477132\n",
      "Classifier 26/50: error = 0.42586071307169604, alpha = 0.14937984939559776\n",
      "Classifier 27/50: error = 0.41680164826144694, alpha = 0.16795845469580045\n",
      "Classifier 28/50: error = 0.4455365160526644, alpha = 0.10936087028022441\n",
      "Classifier 29/50: error = 0.398130754831969, alpha = 0.2066298699279097\n",
      "Classifier 30/50: error = 0.4181722292094646, alpha = 0.1651405490639575\n",
      "Classifier 31/50: error = 0.41713393007963084, alpha = 0.167275044356652\n",
      "Classifier 32/50: error = 0.4241535660986705, alpha = 0.15287272171167027\n",
      "Classifier 33/50: error = 0.4379389461574531, alpha = 0.12476548617369189\n",
      "Classifier 34/50: error = 0.4224478531971532, alpha = 0.15636636047776756\n",
      "Classifier 35/50: error = 0.4198228401824644, alpha = 0.16175034579730105\n",
      "Classifier 36/50: error = 0.4139089364506602, alpha = 0.17391459330744585\n",
      "Classifier 37/50: error = 0.44109508694028987, alpha = 0.11835944396389976\n",
      "Classifier 38/50: error = 0.4502637076885766, alpha = 0.09980263323536263\n",
      "Classifier 39/50: error = 0.4527743443077269, alpha = 0.09473369269386347\n",
      "Classifier 40/50: error = 0.44710479992514696, alpha = 0.10618772651653381\n",
      "Classifier 41/50: error = 0.4341096872356154, alpha = 0.13255151396626325\n",
      "Classifier 42/50: error = 0.43412134094502575, alpha = 0.13252779470850473\n",
      "Classifier 43/50: error = 0.413129408165648, alpha = 0.1755217288836005\n",
      "Classifier 44/50: error = 0.4518081023941519, alpha = 0.09668393313241228\n",
      "Classifier 45/50: error = 0.43882174845716426, alpha = 0.12297265164124842\n",
      "Classifier 46/50: error = 0.44182777769102066, alpha = 0.1168736969653367\n",
      "Classifier 47/50: error = 0.4327379374209549, alpha = 0.13534453448443187\n",
      "Classifier 48/50: error = 0.4548809464402317, alpha = 0.09048424456588716\n",
      "Classifier 49/50: error = 0.44870083315391013, alpha = 0.10296062225114785\n",
      "Classifier 50/50: error = 0.4524700052805568, alpha = 0.09534788537861223\n",
      "Accuracy for digit 7: 0.9294\n",
      "Running AdaBoost for digit: 8\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3251858815485855, alpha = 0.3650201553291021\n",
      "Classifier 2/50: error = 0.3009638606739339, alpha = 0.42135612442081866\n",
      "Classifier 3/50: error = 0.32575377173862574, alpha = 0.36372678665072217\n",
      "Classifier 4/50: error = 0.39623437562601427, alpha = 0.2105900948041072\n",
      "Classifier 5/50: error = 0.4202311641873472, alpha = 0.16091225714770815\n",
      "Classifier 6/50: error = 0.38931277531883046, alpha = 0.22510092528531409\n",
      "Classifier 7/50: error = 0.39785818790018723, alpha = 0.2071986778860435\n",
      "Classifier 8/50: error = 0.3721780347677932, alpha = 0.26144215172074686\n",
      "Classifier 9/50: error = 0.42535231875050905, alpha = 0.15041965693931872\n",
      "Classifier 10/50: error = 0.4373458934020409, alpha = 0.12597033184895287\n",
      "Classifier 11/50: error = 0.40160145770022526, alpha = 0.19939839623823788\n",
      "Classifier 12/50: error = 0.396601713452173, alpha = 0.2098224752712563\n",
      "Classifier 13/50: error = 0.4138329452226781, alpha = 0.1740712234272271\n",
      "Classifier 14/50: error = 0.4309441149970212, alpha = 0.1390001136521848\n",
      "Classifier 15/50: error = 0.445342527948056, alpha = 0.10975352194833732\n",
      "Classifier 16/50: error = 0.42264081097357664, alpha = 0.15597095619669019\n",
      "Classifier 17/50: error = 0.42814121917918846, alpha = 0.1447194908247995\n",
      "Classifier 18/50: error = 0.42769650047094343, alpha = 0.14562780559677235\n",
      "Classifier 19/50: error = 0.4115993711338311, alpha = 0.17867879066288267\n",
      "Classifier 20/50: error = 0.455433634293523, alpha = 0.08936990558529\n",
      "Classifier 21/50: error = 0.4259824043475905, alpha = 0.1491310045839755\n",
      "Classifier 22/50: error = 0.44206844525014916, alpha = 0.11638578473794707\n",
      "Classifier 23/50: error = 0.4461227872734799, alpha = 0.10817440127137674\n",
      "Classifier 24/50: error = 0.44153525559440376, alpha = 0.11746680999389063\n",
      "Classifier 25/50: error = 0.418942840236722, alpha = 0.1635573212525764\n",
      "Classifier 26/50: error = 0.43986290688523655, alpha = 0.12085922987559727\n",
      "Classifier 27/50: error = 0.46088291999982456, alpha = 0.0783943616244935\n",
      "Classifier 28/50: error = 0.4524396283176553, alpha = 0.09540919366484607\n",
      "Classifier 29/50: error = 0.4498020000767303, alpha = 0.1007353635777447\n",
      "Classifier 30/50: error = 0.4429194254469386, alpha = 0.11466100893411614\n",
      "Classifier 31/50: error = 0.4461042403346852, alpha = 0.10821193105794363\n",
      "Classifier 32/50: error = 0.4421028624526635, alpha = 0.11631601427238589\n",
      "Classifier 33/50: error = 0.46583665021896326, alpha = 0.06843332697948573\n",
      "Classifier 34/50: error = 0.4441343145939267, alpha = 0.11219983248966403\n",
      "Classifier 35/50: error = 0.45098539118417313, alpha = 0.0983450520277916\n",
      "Classifier 36/50: error = 0.4587177699494488, alpha = 0.08275284214418283\n",
      "Classifier 37/50: error = 0.4556767831498598, alpha = 0.08887973484832909\n",
      "Classifier 38/50: error = 0.45618723512490994, alpha = 0.08785083838870933\n",
      "Classifier 39/50: error = 0.45324906448950575, alpha = 0.09377579246460546\n",
      "Classifier 40/50: error = 0.4486044498344731, alpha = 0.10315544351558562\n",
      "Classifier 41/50: error = 0.4540950304978495, alpha = 0.09206920880713515\n",
      "Classifier 42/50: error = 0.45062316434467564, alpha = 0.09907658764574383\n",
      "Classifier 43/50: error = 0.4437087325200001, alpha = 0.11306184004199263\n",
      "Classifier 44/50: error = 0.4442887012699972, alpha = 0.11188716662525167\n",
      "Classifier 45/50: error = 0.450620877163698, alpha = 0.09908120705963785\n",
      "Classifier 46/50: error = 0.4417172093743633, alpha = 0.11709787383472013\n",
      "Classifier 47/50: error = 0.4699946779258817, alpha = 0.06008283852145051\n",
      "Classifier 48/50: error = 0.46356479491949915, alpha = 0.07299980563302907\n",
      "Classifier 49/50: error = 0.4599994761701683, alpha = 0.0801723794294492\n",
      "Classifier 50/50: error = 0.4673185152198495, alpha = 0.06545629268042948\n",
      "Accuracy for digit 8: 0.8545\n",
      "Running AdaBoost for digit: 9\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Classifier 1/50: error = 0.3400571524625986, alpha = 0.33151976876306083\n",
      "Classifier 2/50: error = 0.3519877370534074, alpha = 0.3051566416344701\n",
      "Classifier 3/50: error = 0.3803983038634329, alpha = 0.24392898703510865\n",
      "Classifier 4/50: error = 0.3357440948466053, alpha = 0.3411591135540882\n",
      "Classifier 5/50: error = 0.3597133723612718, alpha = 0.28830420261055023\n",
      "Classifier 6/50: error = 0.3798895167741113, alpha = 0.24500859784927942\n",
      "Classifier 7/50: error = 0.36816574013926906, alpha = 0.2700469474968834\n",
      "Classifier 8/50: error = 0.3655389194066226, alpha = 0.2757015951446274\n",
      "Classifier 9/50: error = 0.36742638922371174, alpha = 0.2716367986160306\n",
      "Classifier 10/50: error = 0.38427138223434365, alpha = 0.23572864162161192\n",
      "Classifier 11/50: error = 0.4036679105745121, alpha = 0.1951025862143812\n",
      "Classifier 12/50: error = 0.42707374192365544, alpha = 0.14690015894253863\n",
      "Classifier 13/50: error = 0.4347335648552648, alpha = 0.1312819188584616\n",
      "Classifier 14/50: error = 0.39258194844395033, alpha = 0.21823598509669365\n",
      "Classifier 15/50: error = 0.41068295030304436, alpha = 0.18057140855900947\n",
      "Classifier 16/50: error = 0.4360845011893582, alpha = 0.12853419038988428\n",
      "Classifier 17/50: error = 0.4514587671165625, alpha = 0.09738920313922295\n",
      "Classifier 18/50: error = 0.3920393782627285, alpha = 0.21937391170077103\n",
      "Classifier 19/50: error = 0.4398943564810885, alpha = 0.12079540792598722\n",
      "Classifier 20/50: error = 0.4363414862512084, alpha = 0.12801171693125601\n",
      "Classifier 21/50: error = 0.4439364193959643, alpha = 0.1126006443260581\n",
      "Classifier 22/50: error = 0.46096825311089784, alpha = 0.07822264669783274\n",
      "Classifier 23/50: error = 0.45860463091817383, alpha = 0.082980677596376\n",
      "Classifier 24/50: error = 0.45579659382726023, alpha = 0.08863822075537828\n",
      "Classifier 25/50: error = 0.44387208547702284, alpha = 0.11273095234218676\n",
      "Classifier 26/50: error = 0.4104241031660567, alpha = 0.18110621819130074\n",
      "Classifier 27/50: error = 0.44265509746985043, alpha = 0.11519667852494746\n",
      "Classifier 28/50: error = 0.44539154528430935, alpha = 0.10965430269283413\n",
      "Classifier 29/50: error = 0.4354326091939833, alpha = 0.1298598585151983\n",
      "Classifier 30/50: error = 0.4428547123564929, alpha = 0.11479214613403103\n",
      "Classifier 31/50: error = 0.46224901943610375, alpha = 0.07564592129620848\n",
      "Classifier 32/50: error = 0.45787467929784453, alpha = 0.08445083650092584\n",
      "Classifier 33/50: error = 0.4630479758481377, alpha = 0.0740390408641005\n",
      "Classifier 34/50: error = 0.4473460908665773, alpha = 0.1056997077954559\n",
      "Classifier 35/50: error = 0.4628680310972433, alpha = 0.07440091650181979\n",
      "Classifier 36/50: error = 0.45786636430180616, alpha = 0.08446758540303156\n",
      "Classifier 37/50: error = 0.45573424768074616, alpha = 0.08876389668793304\n",
      "Classifier 38/50: error = 0.46063858270215496, alpha = 0.07888606456649114\n",
      "Classifier 39/50: error = 0.4376962172400734, alpha = 0.1252585703607353\n",
      "Classifier 40/50: error = 0.453441101860678, alpha = 0.09338834431287858\n",
      "Classifier 41/50: error = 0.4649153589063888, alpha = 0.07028478842206198\n",
      "Classifier 42/50: error = 0.44302352777890724, alpha = 0.1144500600330734\n",
      "Classifier 43/50: error = 0.4546859900013822, alpha = 0.09087737250356484\n",
      "Classifier 44/50: error = 0.4554165041796978, alpha = 0.08940444028428855\n",
      "Classifier 45/50: error = 0.4478074309216376, alpha = 0.10476677205532323\n",
      "Classifier 46/50: error = 0.44546448750744805, alpha = 0.10950665946024478\n",
      "Classifier 47/50: error = 0.4456847593355219, alpha = 0.10906083343298223\n",
      "Classifier 48/50: error = 0.45649114756959586, alpha = 0.08723834316421769\n",
      "Classifier 49/50: error = 0.45827772431320635, alpha = 0.08363903914929711\n",
      "Classifier 50/50: error = 0.4507267658939531, alpha = 0.0988673482630407\n",
      "Accuracy for digit 9: 0.8677\n",
      "Accuracies for all digits: {0: 0.9629, 1: 0.9681, 2: 0.9138, 3: 0.8903, 4: 0.9202, 5: 0.8609, 6: 0.952, 7: 0.9294, 8: 0.8545, 9: 0.8677}\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=50, A=50, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 1B: Experimentación con los parámetros T y A del método AdaboostBinario.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tareas 1C y 1D: ADABoost Binario con mejoras y ADABoost Multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclase con ADABoosti Binario sin Mejoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, img_shape): # Inicializamos la clase\n",
    "        self.feature_index = (np.random.randint(0, img_shape[0]), np.random.randint(0, img_shape[1])) # Elegimos un índice de característica aleatorio en 2D\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.img_shape = img_shape # Inicializamos la forma de la imagen\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index[0], self.feature_index[1]] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, img_rows, img_cols = X.shape # Obtenemos el número de muestras y el tamaño de la imagen\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump((img_rows, img_cols)) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index[0], clf.feature_index[1]]), max(X[:, clf.feature_index[0], clf.feature_index[1]])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train) # Convertimos las etiquetas a binarias\n",
    "    \n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A) # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3950244698205547, alpha = 0.21312014962273784\n",
      "Classifier 2/50: error = 0.3119191083171587, alpha = 0.39558125999382115\n",
      "Classifier 3/50: error = 0.31984957387585244, alpha = 0.37723159239788917\n",
      "Classifier 4/50: error = 0.3355120857365491, alpha = 0.3416793556513819\n",
      "Classifier 5/50: error = 0.40581352393301107, alpha = 0.19064972467869068\n",
      "Classifier 6/50: error = 0.37500372545637967, alpha = 0.2554048642051783\n",
      "Classifier 7/50: error = 0.3825585205933858, alpha = 0.23935132857028116\n",
      "Classifier 8/50: error = 0.3725471385041592, alpha = 0.2606524854098987\n",
      "Classifier 9/50: error = 0.3750612208104931, alpha = 0.25528221169746834\n",
      "Classifier 10/50: error = 0.41038237310242487, alpha = 0.181192447158234\n",
      "Classifier 11/50: error = 0.38395730072663803, alpha = 0.23639246382734436\n",
      "Classifier 12/50: error = 0.4161331786900045, alpha = 0.1693337804709854\n",
      "Classifier 13/50: error = 0.4204312348555547, alpha = 0.1605016921887417\n",
      "Classifier 14/50: error = 0.41517662982972137, alpha = 0.17130291325421781\n",
      "Classifier 15/50: error = 0.43257424180048143, alpha = 0.13567797460201744\n",
      "Classifier 16/50: error = 0.39734736693035155, alpha = 0.20826504415117283\n",
      "Classifier 17/50: error = 0.39503726146119655, alpha = 0.21309338679451495\n",
      "Classifier 18/50: error = 0.4105945767916391, alpha = 0.18075398746710458\n",
      "Classifier 19/50: error = 0.4346877850203579, alpha = 0.13137506677328958\n",
      "Classifier 20/50: error = 0.44492189334751686, alpha = 0.11060504515688216\n",
      "Classifier 21/50: error = 0.43277796862324847, alpha = 0.1352629974024718\n",
      "Classifier 22/50: error = 0.43515392910616796, alpha = 0.1304267122849335\n",
      "Classifier 23/50: error = 0.4435191805875856, alpha = 0.11344582728609157\n",
      "Classifier 24/50: error = 0.45400638840080776, alpha = 0.09224800298090066\n",
      "Classifier 25/50: error = 0.3770590169303618, alpha = 0.25102503259603803\n",
      "Classifier 26/50: error = 0.4308751685641973, alpha = 0.13914069068327947\n",
      "Classifier 27/50: error = 0.45041938490568556, alpha = 0.09948817705052641\n",
      "Classifier 28/50: error = 0.4751956022824779, alpha = 0.049649551911218365\n",
      "Classifier 29/50: error = 0.44823699306181614, alpha = 0.10389826193743512\n",
      "Classifier 30/50: error = 0.4481526209973641, alpha = 0.10406883721141044\n",
      "Classifier 31/50: error = 0.4373648417948516, alpha = 0.12593183069533032\n",
      "Classifier 32/50: error = 0.4184975288058902, alpha = 0.16447211827195427\n",
      "Classifier 33/50: error = 0.46994383875725465, alpha = 0.06018488497930579\n",
      "Classifier 34/50: error = 0.44927799952710434, alpha = 0.10179414833791468\n",
      "Classifier 35/50: error = 0.43345925850540923, alpha = 0.13387559330896126\n",
      "Classifier 36/50: error = 0.41645289877261826, alpha = 0.16867590163344212\n",
      "Classifier 37/50: error = 0.4468346542962459, alpha = 0.10673416458214935\n",
      "Classifier 38/50: error = 0.46753049170458594, alpha = 0.06503053250080157\n",
      "Classifier 39/50: error = 0.440668937121766, alpha = 0.11922382751861138\n",
      "Classifier 40/50: error = 0.41800479587989053, alpha = 0.1654846507154089\n",
      "Classifier 41/50: error = 0.4376761890002927, alpha = 0.12529925881487453\n",
      "Classifier 42/50: error = 0.45493950376128, alpha = 0.09036616969737614\n",
      "Classifier 43/50: error = 0.4708274401362167, alpha = 0.05841146050722914\n",
      "Classifier 44/50: error = 0.4607986455845492, alpha = 0.07856395067687914\n",
      "Classifier 45/50: error = 0.46362362962719184, alpha = 0.07288150906525521\n",
      "Classifier 46/50: error = 0.4646195537748987, alpha = 0.07087935090064858\n",
      "Classifier 47/50: error = 0.46418149660508695, alpha = 0.07175992920473609\n",
      "Classifier 48/50: error = 0.45181310861342483, alpha = 0.09667382681761436\n",
      "Classifier 49/50: error = 0.45083954070543897, alpha = 0.09863959186362929\n",
      "Classifier 50/50: error = 0.46275392187747333, alpha = 0.0746304044914345\n",
      "Accuracy for digit 3: 0.8932\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=50, A=20, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n",
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Classifier 1/50: error = 0.24491346559729854, alpha = 0.5629637054988017\n",
      "Classifier 2/50: error = 0.24448462194092319, alpha = 0.5641238625246932\n",
      "Classifier 3/50: error = 0.24412186669791447, alpha = 0.565106304642309\n",
      "Classifier 4/50: error = 0.2967349614341819, alpha = 0.43144723834067106\n",
      "Classifier 5/50: error = 0.2673657983871189, alpha = 0.5040143910448964\n",
      "Classifier 6/50: error = 0.2908849036473995, alpha = 0.44554509025745376\n",
      "Classifier 7/50: error = 0.3588952326140195, alpha = 0.2900811861819371\n",
      "Classifier 8/50: error = 0.3222999207350743, alpha = 0.37161114302744624\n",
      "Classifier 9/50: error = 0.3487297464846908, alpha = 0.31231371752258474\n",
      "Classifier 10/50: error = 0.3532267314618984, alpha = 0.3024428242114688\n",
      "Classifier 11/50: error = 0.3754886098037984, alpha = 0.2543707154330119\n",
      "Classifier 12/50: error = 0.4052380397357519, alpha = 0.1918433058772353\n",
      "Classifier 13/50: error = 0.4142264379840401, alpha = 0.1732602629667486\n",
      "Classifier 14/50: error = 0.3773315893989124, alpha = 0.2504448912067702\n",
      "Classifier 15/50: error = 0.3360053645741524, alpha = 0.34057347214429784\n",
      "Classifier 16/50: error = 0.3677224465504958, alpha = 0.2710000161036304\n",
      "Classifier 17/50: error = 0.4131827356493267, alpha = 0.17541175632204445\n",
      "Classifier 18/50: error = 0.4289144237197743, alpha = 0.14314082437058254\n",
      "Classifier 19/50: error = 0.3931346283974143, alpha = 0.21707742709501848\n",
      "Classifier 20/50: error = 0.3907480458060454, alpha = 0.22208446589790592\n",
      "Classifier 21/50: error = 0.44983989336317753, alpha = 0.10065880593542781\n",
      "Classifier 22/50: error = 0.4006029199092045, alpha = 0.20147678567813732\n",
      "Classifier 23/50: error = 0.4150263104810761, alpha = 0.17161247699922597\n",
      "Classifier 24/50: error = 0.40715359208242324, alpha = 0.18787243321178057\n",
      "Classifier 25/50: error = 0.39675375081781733, alpha = 0.20950483674833206\n",
      "Classifier 26/50: error = 0.43732227864628603, alpha = 0.12601831508434339\n",
      "Classifier 27/50: error = 0.4474647198179454, alpha = 0.10545979531625789\n",
      "Classifier 28/50: error = 0.44754026365051236, alpha = 0.10530702350146941\n",
      "Classifier 29/50: error = 0.4340083826262467, alpha = 0.1327577095125809\n",
      "Classifier 30/50: error = 0.4264461817840831, alpha = 0.14818280028666167\n",
      "Classifier 31/50: error = 0.4328059689225182, alpha = 0.1352059663858932\n",
      "Classifier 32/50: error = 0.43351255774700925, alpha = 0.13376707442143274\n",
      "Classifier 33/50: error = 0.45842404157486194, alpha = 0.08334435996412518\n",
      "Classifier 34/50: error = 0.4389382249159118, alpha = 0.12273616499155371\n",
      "Classifier 35/50: error = 0.4476208785984148, alpha = 0.10514400180350884\n",
      "Classifier 36/50: error = 0.422639380311808, alpha = 0.15597388769507786\n",
      "Classifier 37/50: error = 0.445960689024238, alpha = 0.10850241784598696\n",
      "Classifier 38/50: error = 0.44530448836205994, alpha = 0.10983052189145087\n",
      "Classifier 39/50: error = 0.45525687969228945, alpha = 0.08972625711700546\n",
      "Classifier 40/50: error = 0.4577090788012925, alpha = 0.08478441460868591\n",
      "Classifier 41/50: error = 0.4499817741320695, alpha = 0.10037216778166702\n",
      "Classifier 42/50: error = 0.4642661577159445, alpha = 0.07158973563613041\n",
      "Classifier 43/50: error = 0.45293833496766367, alpha = 0.09440276943046055\n",
      "Classifier 44/50: error = 0.44363683963753, alpha = 0.1132074740513085\n",
      "Classifier 45/50: error = 0.4326216827368421, alpha = 0.13558133661024174\n",
      "Classifier 46/50: error = 0.41835819251341366, alpha = 0.16475841083777262\n",
      "Classifier 47/50: error = 0.4276613661826479, alpha = 0.14569957570060868\n",
      "Classifier 48/50: error = 0.4554483370528566, alpha = 0.089340264657697\n",
      "Classifier 49/50: error = 0.46075755790988254, alpha = 0.07864663481724808\n",
      "Classifier 50/50: error = 0.44803995057910323, alpha = 0.10429663276977326\n",
      "Accuracy for digit 0: 0.9593\n",
      "Running AdaBoost for digit: 1\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2720462805013719, alpha = 0.49213263655199685\n",
      "Classifier 2/50: error = 0.2400522534802772, alpha = 0.5761965267870346\n",
      "Classifier 3/50: error = 0.2578562092541039, alpha = 0.5285704555615829\n",
      "Classifier 4/50: error = 0.28467717849331897, alpha = 0.46068905398412097\n",
      "Classifier 5/50: error = 0.35038865880542236, alpha = 0.3086656275917756\n",
      "Classifier 6/50: error = 0.2613455680951092, alpha = 0.5194933241160422\n",
      "Classifier 7/50: error = 0.31871125623610186, alpha = 0.3798503378758189\n",
      "Classifier 8/50: error = 0.2980142064472806, alpha = 0.4283860042562835\n",
      "Classifier 9/50: error = 0.3265843185848887, alpha = 0.36183731398499414\n",
      "Classifier 10/50: error = 0.33588080260924347, alpha = 0.3408526522386711\n",
      "Classifier 11/50: error = 0.34633619872631494, alpha = 0.31759158895246825\n",
      "Classifier 12/50: error = 0.343895374147417, alpha = 0.3229914002628623\n",
      "Classifier 13/50: error = 0.38436000403422765, alpha = 0.2355413732780422\n",
      "Classifier 14/50: error = 0.4083284353319061, alpha = 0.18543992615514246\n",
      "Classifier 15/50: error = 0.39946770641750673, alpha = 0.2038417454369319\n",
      "Classifier 16/50: error = 0.3829052030593101, alpha = 0.23861760338679958\n",
      "Classifier 17/50: error = 0.40249264950079877, alpha = 0.19754487557878614\n",
      "Classifier 18/50: error = 0.36648220732327497, alpha = 0.2736690564807662\n",
      "Classifier 19/50: error = 0.37530179495904203, alpha = 0.2547690861072826\n",
      "Classifier 20/50: error = 0.39940549658258284, alpha = 0.20397141030885224\n",
      "Classifier 21/50: error = 0.35982900610073876, alpha = 0.28805319147873293\n",
      "Classifier 22/50: error = 0.413812543648826, alpha = 0.1741132757855447\n",
      "Classifier 23/50: error = 0.3953649606728984, alpha = 0.21240787287351176\n",
      "Classifier 24/50: error = 0.4124035191745984, alpha = 0.17701908481851272\n",
      "Classifier 25/50: error = 0.40437383627403245, alpha = 0.19363672035217513\n",
      "Classifier 26/50: error = 0.44552199999000297, alpha = 0.10939025110442004\n",
      "Classifier 27/50: error = 0.41305879777401877, alpha = 0.1756673489272179\n",
      "Classifier 28/50: error = 0.4365460180505851, alpha = 0.12759593529868893\n",
      "Classifier 29/50: error = 0.42600088128017655, alpha = 0.1490932229632527\n",
      "Classifier 30/50: error = 0.40359446914966024, alpha = 0.19525513594995264\n",
      "Classifier 31/50: error = 0.42343393076915037, alpha = 0.15434622141511212\n",
      "Classifier 32/50: error = 0.43182280068898304, alpha = 0.13720901006267822\n",
      "Classifier 33/50: error = 0.4425079167942483, alpha = 0.11549497365532331\n",
      "Classifier 34/50: error = 0.43260348647015845, alpha = 0.13561840241636053\n",
      "Classifier 35/50: error = 0.4057525590258081, alpha = 0.19077614328170778\n",
      "Classifier 36/50: error = 0.4337541651946412, alpha = 0.13327519349747086\n",
      "Classifier 37/50: error = 0.40983470548403034, alpha = 0.18232436836583685\n",
      "Classifier 38/50: error = 0.44144256391099723, alpha = 0.11765476728421614\n",
      "Classifier 39/50: error = 0.4084755239439771, alpha = 0.18513553326282858\n",
      "Classifier 40/50: error = 0.42359935567371465, alpha = 0.15400744462712343\n",
      "Classifier 41/50: error = 0.389025999537825, alpha = 0.22570411374732258\n",
      "Classifier 42/50: error = 0.4499971860423081, alpha = 0.10034103247711802\n",
      "Classifier 43/50: error = 0.42764401645449013, alpha = 0.14573501717856865\n",
      "Classifier 44/50: error = 0.43393668463473606, alpha = 0.1329036504883133\n",
      "Classifier 45/50: error = 0.4547701242680512, alpha = 0.09070771306867548\n",
      "Classifier 46/50: error = 0.439629764018453, alpha = 0.12133238681090482\n",
      "Classifier 47/50: error = 0.44830999026793983, alpha = 0.10375068811214498\n",
      "Classifier 48/50: error = 0.43953592357272153, alpha = 0.12152284862270404\n",
      "Classifier 49/50: error = 0.4434005411951, alpha = 0.11368617949174364\n",
      "Classifier 50/50: error = 0.45098412685280215, alpha = 0.09834760522671189\n",
      "Accuracy for digit 1: 0.966\n",
      "Running AdaBoost for digit: 2\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Classifier 1/50: error = 0.34331990600872775, alpha = 0.3242671486699815\n",
      "Classifier 2/50: error = 0.3568517660869362, alpha = 0.29452737894715253\n",
      "Classifier 3/50: error = 0.3036306242399127, alpha = 0.41503416043991886\n",
      "Classifier 4/50: error = 0.33738303064042063, alpha = 0.337489112646647\n",
      "Classifier 5/50: error = 0.3260403251983719, alpha = 0.36307460432426875\n",
      "Classifier 6/50: error = 0.3454900741246742, alpha = 0.31946141845251863\n",
      "Classifier 7/50: error = 0.3772177275802253, alpha = 0.2506872138228987\n",
      "Classifier 8/50: error = 0.3466456934198352, alpha = 0.31690818146377064\n",
      "Classifier 9/50: error = 0.3730130213045809, alpha = 0.25965622198976124\n",
      "Classifier 10/50: error = 0.3821586784332318, alpha = 0.24019787612247442\n",
      "Classifier 11/50: error = 0.37028789758848857, alpha = 0.2654909649315685\n",
      "Classifier 12/50: error = 0.3908215938017253, alpha = 0.22193000005960947\n",
      "Classifier 13/50: error = 0.39648993131323795, alpha = 0.21005603864769062\n",
      "Classifier 14/50: error = 0.3961120392642231, alpha = 0.21084579324900285\n",
      "Classifier 15/50: error = 0.3859835895743704, alpha = 0.23211340013314463\n",
      "Classifier 16/50: error = 0.3975816341537767, alpha = 0.20777594088928078\n",
      "Classifier 17/50: error = 0.36602496528799044, alpha = 0.27465401693806435\n",
      "Classifier 18/50: error = 0.43290402500110414, alpha = 0.13500625262898072\n",
      "Classifier 19/50: error = 0.39385066665893587, alpha = 0.21557728072898244\n",
      "Classifier 20/50: error = 0.4375593025451215, alpha = 0.12553672821840878\n",
      "Classifier 21/50: error = 0.4370243218641866, alpha = 0.12662378834973415\n",
      "Classifier 22/50: error = 0.4082372552987904, alpha = 0.18562863595686296\n",
      "Classifier 23/50: error = 0.40840816307350514, alpha = 0.18527492916955052\n",
      "Classifier 24/50: error = 0.43438453200391225, alpha = 0.13199215091860558\n",
      "Classifier 25/50: error = 0.4182003953267084, alpha = 0.16508266710257713\n",
      "Classifier 26/50: error = 0.44838890157973343, alpha = 0.1035911631787051\n",
      "Classifier 27/50: error = 0.4492075549091673, alpha = 0.10193650458109377\n",
      "Classifier 28/50: error = 0.43580065028111936, alpha = 0.1291113656393392\n",
      "Classifier 29/50: error = 0.4464350852711501, alpha = 0.10754251083100212\n",
      "Classifier 30/50: error = 0.43813322569612234, alpha = 0.1243708664798385\n",
      "Classifier 31/50: error = 0.4309401806815557, alpha = 0.13900813530372003\n",
      "Classifier 32/50: error = 0.43332779116655973, alpha = 0.13414327824213743\n",
      "Classifier 33/50: error = 0.44893524119453376, alpha = 0.10248684171160756\n",
      "Classifier 34/50: error = 0.4644334728203711, alpha = 0.07125339555772861\n",
      "Classifier 35/50: error = 0.4443704636712995, alpha = 0.11172158919506113\n",
      "Classifier 36/50: error = 0.46566191391139444, alpha = 0.06878444721117187\n",
      "Classifier 37/50: error = 0.4551491183271479, alpha = 0.08994352386682296\n",
      "Classifier 38/50: error = 0.4508732975806049, alpha = 0.09857141954371204\n",
      "Classifier 39/50: error = 0.46297575833583515, alpha = 0.07418427065722065\n",
      "Classifier 40/50: error = 0.4603089898895205, alpha = 0.0795493955615361\n",
      "Classifier 41/50: error = 0.4440304221461516, alpha = 0.11241024909295978\n",
      "Classifier 42/50: error = 0.4336942838818729, alpha = 0.13339709797427\n",
      "Classifier 43/50: error = 0.45081941976273754, alpha = 0.09868022672717186\n",
      "Classifier 44/50: error = 0.4573050049951949, alpha = 0.08559844166295816\n",
      "Classifier 45/50: error = 0.4592885876019954, alpha = 0.08160347959565477\n",
      "Classifier 46/50: error = 0.4566471801881826, alpha = 0.08692390550805133\n",
      "Classifier 47/50: error = 0.45710756417199383, alpha = 0.08599623725014531\n",
      "Classifier 48/50: error = 0.45504431346798224, alpha = 0.09015483787984971\n",
      "Classifier 49/50: error = 0.46364767056895384, alpha = 0.0728331715019413\n",
      "Classifier 50/50: error = 0.45610312178275225, alpha = 0.08802036927134499\n",
      "Accuracy for digit 2: 0.91\n",
      "Running AdaBoost for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3374388254486133, alpha = 0.33736432822206097\n",
      "Classifier 2/50: error = 0.3468969703204089, alpha = 0.3163535376642776\n",
      "Classifier 3/50: error = 0.29547377864663715, alpha = 0.4344727264455497\n",
      "Classifier 4/50: error = 0.31566784858627495, alpha = 0.3868764261776182\n",
      "Classifier 5/50: error = 0.3760013671898744, alpha = 0.2532776988626734\n",
      "Classifier 6/50: error = 0.37161883101297666, alpha = 0.2626391290359665\n",
      "Classifier 7/50: error = 0.35566917759885097, alpha = 0.2971056355306971\n",
      "Classifier 8/50: error = 0.3867476317786761, alpha = 0.23050208937347585\n",
      "Classifier 9/50: error = 0.38491469762702124, alpha = 0.23436960800141685\n",
      "Classifier 10/50: error = 0.40228084373485473, alpha = 0.1979852721229908\n",
      "Classifier 11/50: error = 0.4036727566976397, alpha = 0.19509252034710683\n",
      "Classifier 12/50: error = 0.42357869255073666, alpha = 0.15404975910870858\n",
      "Classifier 13/50: error = 0.4095466597768332, alpha = 0.18291988747295176\n",
      "Classifier 14/50: error = 0.44688524515666583, alpha = 0.10663182691018197\n",
      "Classifier 15/50: error = 0.43069136068380415, alpha = 0.1395154890185567\n",
      "Classifier 16/50: error = 0.40514826466396253, alpha = 0.19202955220128815\n",
      "Classifier 17/50: error = 0.43754540808731246, alpha = 0.1255649574775304\n",
      "Classifier 18/50: error = 0.4060409870766657, alpha = 0.19017810359716944\n",
      "Classifier 19/50: error = 0.41359675273431795, alpha = 0.17455810808707903\n",
      "Classifier 20/50: error = 0.4424296213226032, alpha = 0.11565366558005181\n",
      "Classifier 21/50: error = 0.4123386125933006, alpha = 0.17715301158482702\n",
      "Classifier 22/50: error = 0.4140947896412621, alpha = 0.17353155562555486\n",
      "Classifier 23/50: error = 0.43155512869433604, alpha = 0.1377545366414322\n",
      "Classifier 24/50: error = 0.4275046383572029, alpha = 0.14601974756301458\n",
      "Classifier 25/50: error = 0.4505033061311392, alpha = 0.0993186706715424\n",
      "Classifier 26/50: error = 0.42939269790333967, alpha = 0.14216467795672913\n",
      "Classifier 27/50: error = 0.44598363284353726, alpha = 0.10845598809221103\n",
      "Classifier 28/50: error = 0.43185087378659315, alpha = 0.1371518006379011\n",
      "Classifier 29/50: error = 0.43588835654580543, alpha = 0.12893301682620437\n",
      "Classifier 30/50: error = 0.44432029828548214, alpha = 0.11182317863415964\n",
      "Classifier 31/50: error = 0.4510849417259454, alpha = 0.09814402303135672\n",
      "Classifier 32/50: error = 0.452419161094924, alpha = 0.09545050202740597\n",
      "Classifier 33/50: error = 0.45433200509215876, alpha = 0.0915912516480772\n",
      "Classifier 34/50: error = 0.4431555682204662, alpha = 0.1141825130238441\n",
      "Classifier 35/50: error = 0.4433743246832214, alpha = 0.11373929343255906\n",
      "Classifier 36/50: error = 0.4527834907820698, alpha = 0.09471523511609188\n",
      "Classifier 37/50: error = 0.4504222942766879, alpha = 0.0994823005283675\n",
      "Classifier 38/50: error = 0.44126974500403554, alpha = 0.11800522615503176\n",
      "Classifier 39/50: error = 0.4362956391115216, alpha = 0.12810492313667132\n",
      "Classifier 40/50: error = 0.45425176020422686, alpha = 0.09175309392854748\n",
      "Classifier 41/50: error = 0.4474438796513371, alpha = 0.1055019411162201\n",
      "Classifier 42/50: error = 0.4586825571855894, alpha = 0.08282375146536541\n",
      "Classifier 43/50: error = 0.4625199858096801, alpha = 0.07510090389915905\n",
      "Classifier 44/50: error = 0.45809754041803197, alpha = 0.08400194477763527\n",
      "Classifier 45/50: error = 0.460650178094275, alpha = 0.07886272920899995\n",
      "Classifier 46/50: error = 0.4542166468178903, alpha = 0.09182391403438564\n",
      "Classifier 47/50: error = 0.45326405900109906, alpha = 0.09374553903259619\n",
      "Classifier 48/50: error = 0.4594384806609275, alpha = 0.08130170013484744\n",
      "Classifier 49/50: error = 0.4781221682657437, alpha = 0.04378361983433454\n",
      "Classifier 50/50: error = 0.4520181183066241, alpha = 0.0962599799906686\n",
      "Accuracy for digit 3: 0.8948\n",
      "Running AdaBoost for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.27578532911067344, alpha = 0.48273254248153097\n",
      "Classifier 2/50: error = 0.3139277222664315, alpha = 0.3909101037918206\n",
      "Classifier 3/50: error = 0.28801380312137015, alpha = 0.4525250590593337\n",
      "Classifier 4/50: error = 0.35404297239646276, alpha = 0.30065734199392474\n",
      "Classifier 5/50: error = 0.2741350204994476, alpha = 0.4868716290463633\n",
      "Classifier 6/50: error = 0.34004546838290983, alpha = 0.3315458009042438\n",
      "Classifier 7/50: error = 0.42226659854684445, alpha = 0.1567378271828755\n",
      "Classifier 8/50: error = 0.37173085795655286, alpha = 0.26239927618872355\n",
      "Classifier 9/50: error = 0.3764643438183293, alpha = 0.25229130604905053\n",
      "Classifier 10/50: error = 0.38148173092190496, alpha = 0.24163188346346287\n",
      "Classifier 11/50: error = 0.38576782931356324, alpha = 0.2325686373451366\n",
      "Classifier 12/50: error = 0.43417311804836867, alpha = 0.13242241250520662\n",
      "Classifier 13/50: error = 0.4202852439539577, alpha = 0.16080127477598175\n",
      "Classifier 14/50: error = 0.3724934027921565, alpha = 0.26076742864125574\n",
      "Classifier 15/50: error = 0.43870201266129555, alpha = 0.12321577011560426\n",
      "Classifier 16/50: error = 0.3966279340698834, alpha = 0.20976769182556387\n",
      "Classifier 17/50: error = 0.4271643992149503, alpha = 0.14671490843082585\n",
      "Classifier 18/50: error = 0.42047614705239966, alpha = 0.160409535260656\n",
      "Classifier 19/50: error = 0.41676617085374357, alpha = 0.16803143092886078\n",
      "Classifier 20/50: error = 0.43055305801162175, alpha = 0.13979752442386456\n",
      "Classifier 21/50: error = 0.41011260284555506, alpha = 0.181749951626062\n",
      "Classifier 22/50: error = 0.3934760145403371, alpha = 0.2163620815019838\n",
      "Classifier 23/50: error = 0.4233351019824888, alpha = 0.15454863152367773\n",
      "Classifier 24/50: error = 0.4283559524369587, alpha = 0.14428099431636476\n",
      "Classifier 25/50: error = 0.42832685377593294, alpha = 0.14434041206823375\n",
      "Classifier 26/50: error = 0.44264093097842006, alpha = 0.11522538925490981\n",
      "Classifier 27/50: error = 0.4291455894630442, alpha = 0.1426689867820883\n",
      "Classifier 28/50: error = 0.44869892565938874, alpha = 0.10296447782710344\n",
      "Classifier 29/50: error = 0.4283607893473038, alpha = 0.14427111772841641\n",
      "Classifier 30/50: error = 0.43836199281543353, alpha = 0.12390624516348951\n",
      "Classifier 31/50: error = 0.4334936052879043, alpha = 0.13380566184939544\n",
      "Classifier 32/50: error = 0.4448225268373107, alpha = 0.11080622376401802\n",
      "Classifier 33/50: error = 0.4614936236229217, alpha = 0.07716555014015496\n",
      "Classifier 34/50: error = 0.4559560794507174, alpha = 0.0883167459918473\n",
      "Classifier 35/50: error = 0.4382775619385477, alpha = 0.12407771633150556\n",
      "Classifier 36/50: error = 0.4429170272697046, alpha = 0.11466586862648716\n",
      "Classifier 37/50: error = 0.4393022511384004, alpha = 0.12199715642810668\n",
      "Classifier 38/50: error = 0.44588286244734904, alpha = 0.1086599133537507\n",
      "Classifier 39/50: error = 0.4291751372643843, alpha = 0.14260868065348856\n",
      "Classifier 40/50: error = 0.448763097092735, alpha = 0.10283477122021642\n",
      "Classifier 41/50: error = 0.46496096810812115, alpha = 0.07019311925103074\n",
      "Classifier 42/50: error = 0.4623976096994181, alpha = 0.07534704370140294\n",
      "Classifier 43/50: error = 0.4691748630183117, alpha = 0.06172855844149617\n",
      "Classifier 44/50: error = 0.4429424312492417, alpha = 0.11461439000062004\n",
      "Classifier 45/50: error = 0.4519247798092597, alpha = 0.09644839548948324\n",
      "Classifier 46/50: error = 0.46033958876131886, alpha = 0.07948781003554876\n",
      "Classifier 47/50: error = 0.44434378134949704, alpha = 0.11177562302163455\n",
      "Classifier 48/50: error = 0.466186433173539, alpha = 0.0677305134374593\n",
      "Classifier 49/50: error = 0.4534321538103937, alpha = 0.09340639697680585\n",
      "Classifier 50/50: error = 0.45733967706756684, alpha = 0.08552858860160444\n",
      "Accuracy for digit 4: 0.9016\n",
      "Running AdaBoost for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1/50: error = 0.34837162099824726, alpha = 0.31310232100497104\n",
      "Classifier 2/50: error = 0.3339625198294984, alpha = 0.34515858736962846\n",
      "Classifier 3/50: error = 0.40959995954850315, alpha = 0.18280968338007422\n",
      "Classifier 4/50: error = 0.3793864861982241, alpha = 0.2460765445331558\n",
      "Classifier 5/50: error = 0.3880852810301154, alpha = 0.2276839063394317\n",
      "Classifier 6/50: error = 0.3736758087376928, alpha = 0.25823975720261794\n",
      "Classifier 7/50: error = 0.43237632643570767, alpha = 0.13608115875035487\n",
      "Classifier 8/50: error = 0.4026190054710109, alpha = 0.1972821863131149\n",
      "Classifier 9/50: error = 0.42495957165170384, alpha = 0.151223154780473\n",
      "Classifier 10/50: error = 0.39273683360829237, alpha = 0.21791124828664893\n",
      "Classifier 11/50: error = 0.397129932332974, alpha = 0.20871909215131204\n",
      "Classifier 12/50: error = 0.41031578265431823, alpha = 0.1813300519313258\n",
      "Classifier 13/50: error = 0.36285944080781607, alpha = 0.281487372656237\n",
      "Classifier 14/50: error = 0.37503425460493345, alpha = 0.2553397366740428\n",
      "Classifier 15/50: error = 0.42020446339088935, alpha = 0.1609670539094116\n",
      "Classifier 16/50: error = 0.42131622769660304, alpha = 0.158686240044846\n",
      "Classifier 17/50: error = 0.3977775102246359, alpha = 0.2073670659248967\n",
      "Classifier 18/50: error = 0.4253312627634392, alpha = 0.15046272922703127\n",
      "Classifier 19/50: error = 0.4392035957593099, alpha = 0.12219742329191437\n",
      "Classifier 20/50: error = 0.41796049305521354, alpha = 0.1655757064362788\n",
      "Classifier 21/50: error = 0.439340901202241, alpha = 0.12191870084704802\n",
      "Classifier 22/50: error = 0.4450336371359651, alpha = 0.11037881801695668\n",
      "Classifier 23/50: error = 0.413958771418806, alpha = 0.17381188002521417\n",
      "Classifier 24/50: error = 0.39812427937274775, alpha = 0.20664338174922595\n",
      "Classifier 25/50: error = 0.45147063478798843, alpha = 0.09736524201789253\n",
      "Classifier 26/50: error = 0.44331332168786197, alpha = 0.11386288631768171\n",
      "Classifier 27/50: error = 0.4554952209652, alpha = 0.08924574721037049\n",
      "Classifier 28/50: error = 0.4363931884465524, alpha = 0.12790661018292657\n",
      "Classifier 29/50: error = 0.4494488925323119, alpha = 0.1014488205586596\n",
      "Classifier 30/50: error = 0.4388325101679802, alpha = 0.12295080115098414\n",
      "Classifier 31/50: error = 0.4510659817427379, alpha = 0.098182309570004\n",
      "Classifier 32/50: error = 0.44600631531414336, alpha = 0.10841008766879762\n",
      "Classifier 33/50: error = 0.4506343784462985, alpha = 0.09905393861278022\n",
      "Classifier 34/50: error = 0.43980675751154563, alpha = 0.12097317853005266\n",
      "Classifier 35/50: error = 0.4392225037360833, alpha = 0.12215904002550645\n",
      "Classifier 36/50: error = 0.450823335662338, alpha = 0.09867231842185993\n",
      "Classifier 37/50: error = 0.4391603232895708, alpha = 0.12228526792227143\n",
      "Classifier 38/50: error = 0.4519464428782629, alpha = 0.09640466525164065\n",
      "Classifier 39/50: error = 0.45050627224356793, alpha = 0.09931267974093652\n",
      "Classifier 40/50: error = 0.44710958399603196, alpha = 0.1061780500897777\n",
      "Classifier 41/50: error = 0.46050917035965644, alpha = 0.07914650860982425\n",
      "Classifier 42/50: error = 0.4498134605779638, alpha = 0.100712209247808\n",
      "Classifier 43/50: error = 0.4384625235782382, alpha = 0.12370208609502152\n",
      "Classifier 44/50: error = 0.4448113712738062, alpha = 0.11082881000673812\n",
      "Classifier 45/50: error = 0.4672698065429032, alpha = 0.06555412864391001\n",
      "Classifier 46/50: error = 0.44506638467359244, alpha = 0.11031252222071837\n",
      "Classifier 47/50: error = 0.44739053896236214, alpha = 0.10560981556401983\n",
      "Classifier 48/50: error = 0.4567004584571317, alpha = 0.08681654282320826\n",
      "Classifier 49/50: error = 0.4559872754719498, alpha = 0.0882538663825979\n",
      "Classifier 50/50: error = 0.46237841852433736, alpha = 0.07538564447969855\n",
      "Accuracy for digit 5: 0.8801\n",
      "Running AdaBoost for digit: 6\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Classifier 1/50: error = 0.2589806440706619, alpha = 0.5256367102384616\n",
      "Classifier 2/50: error = 0.2433015817687565, alpha = 0.567331515994873\n",
      "Classifier 3/50: error = 0.338254536760397, alpha = 0.3355411529146282\n",
      "Classifier 4/50: error = 0.2652105461459191, alpha = 0.509529988154156\n",
      "Classifier 5/50: error = 0.27514875129635874, alpha = 0.4843272970729203\n",
      "Classifier 6/50: error = 0.3626496440987196, alpha = 0.28194115654971863\n",
      "Classifier 7/50: error = 0.38057272021154503, alpha = 0.24355901609992264\n",
      "Classifier 8/50: error = 0.35836736861430407, alpha = 0.2912286424824522\n",
      "Classifier 9/50: error = 0.3915570963025097, alpha = 0.2203858660276298\n",
      "Classifier 10/50: error = 0.3616536854052317, alpha = 0.2840969325131378\n",
      "Classifier 11/50: error = 0.36474847895650053, alpha = 0.27740649959328767\n",
      "Classifier 12/50: error = 0.337970773690766, alpha = 0.3361751399735673\n",
      "Classifier 13/50: error = 0.3576465955918011, alpha = 0.2927966452919609\n",
      "Classifier 14/50: error = 0.38356604565419605, alpha = 0.23721968131959859\n",
      "Classifier 15/50: error = 0.3780390388399659, alpha = 0.24893992980808585\n",
      "Classifier 16/50: error = 0.40934086799925873, alpha = 0.18334542955013078\n",
      "Classifier 17/50: error = 0.37278597626005056, alpha = 0.260141681590489\n",
      "Classifier 18/50: error = 0.38429964131690825, alpha = 0.23566892507583748\n",
      "Classifier 19/50: error = 0.42284558561127494, alpha = 0.15555139000125934\n",
      "Classifier 20/50: error = 0.3885350601236931, alpha = 0.2267371040896804\n",
      "Classifier 21/50: error = 0.4194905047410147, alpha = 0.16243263351390663\n",
      "Classifier 22/50: error = 0.409447966121024, alpha = 0.1831239608295305\n",
      "Classifier 23/50: error = 0.43543547472526245, alpha = 0.129854030266289\n",
      "Classifier 24/50: error = 0.40556203146345315, alpha = 0.19117126563028555\n",
      "Classifier 25/50: error = 0.42121358040540596, alpha = 0.15889675473150586\n",
      "Classifier 26/50: error = 0.42191357118249195, alpha = 0.15746145141011295\n",
      "Classifier 27/50: error = 0.4028341957445301, alpha = 0.19683487574335007\n",
      "Classifier 28/50: error = 0.4367714662917024, alpha = 0.1271376845902162\n",
      "Classifier 29/50: error = 0.42180946261105456, alpha = 0.15767488103675015\n",
      "Classifier 30/50: error = 0.43855742082583865, alpha = 0.12350937704951248\n",
      "Classifier 31/50: error = 0.43680658809401224, alpha = 0.1270663000820837\n",
      "Classifier 32/50: error = 0.3944071802136943, alpha = 0.2144120088141815\n",
      "Classifier 33/50: error = 0.41150650731498817, alpha = 0.17887051773461413\n",
      "Classifier 34/50: error = 0.4415048849219161, alpha = 0.11752839378098502\n",
      "Classifier 35/50: error = 0.4202764605525508, alpha = 0.16081929978479179\n",
      "Classifier 36/50: error = 0.4246685708071566, alpha = 0.1518186209884079\n",
      "Classifier 37/50: error = 0.42992615797581235, alpha = 0.14107621574415055\n",
      "Classifier 38/50: error = 0.4307399464335623, alpha = 0.13941641517408593\n",
      "Classifier 39/50: error = 0.44160956634428683, alpha = 0.11731613095155312\n",
      "Classifier 40/50: error = 0.453514743305038, alpha = 0.09323977522945039\n",
      "Classifier 41/50: error = 0.4375853347328472, alpha = 0.1254838393659542\n",
      "Classifier 42/50: error = 0.44459481123398076, alpha = 0.11126729317785759\n",
      "Classifier 43/50: error = 0.4373876634764334, alpha = 0.1258854599167816\n",
      "Classifier 44/50: error = 0.4212342608346018, alpha = 0.15885434105180157\n",
      "Classifier 45/50: error = 0.43647954376202713, alpha = 0.12773106246479987\n",
      "Classifier 46/50: error = 0.4612947855316884, alpha = 0.07756561134472312\n",
      "Classifier 47/50: error = 0.43229643648955185, alpha = 0.1362439193529692\n",
      "Classifier 48/50: error = 0.4507477288037689, alpha = 0.09882501146849569\n",
      "Classifier 49/50: error = 0.45481888949630633, alpha = 0.09060937881350346\n",
      "Classifier 50/50: error = 0.4566625410128551, alpha = 0.08689295123098979\n",
      "Accuracy for digit 6: 0.9507\n",
      "Running AdaBoost for digit: 7\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Classifier 1/50: error = 0.28366190438183436, alpha = 0.4631846022144742\n",
      "Classifier 2/50: error = 0.28625621727186623, alpha = 0.45681838782488965\n",
      "Classifier 3/50: error = 0.2743363415370197, alpha = 0.4863658747210926\n",
      "Classifier 4/50: error = 0.34165401115531735, alpha = 0.32796602510286404\n",
      "Classifier 5/50: error = 0.296231671815552, alpha = 0.4326536996372159\n",
      "Classifier 6/50: error = 0.3329743057595671, alpha = 0.3473816200002874\n",
      "Classifier 7/50: error = 0.3422588369026378, alpha = 0.32662210039237316\n",
      "Classifier 8/50: error = 0.3889688262548392, alpha = 0.22582438819565429\n",
      "Classifier 9/50: error = 0.3969749234365203, alpha = 0.20904283428615045\n",
      "Classifier 10/50: error = 0.37859864509368535, alpha = 0.24775025861076305\n",
      "Classifier 11/50: error = 0.3858674055530549, alpha = 0.23235852754117195\n",
      "Classifier 12/50: error = 0.3711793017453241, alpha = 0.2635804592807651\n",
      "Classifier 13/50: error = 0.38066074890342994, alpha = 0.24337231498664003\n",
      "Classifier 14/50: error = 0.3595391143997021, alpha = 0.2886825386716213\n",
      "Classifier 15/50: error = 0.35114128598016925, alpha = 0.3070131641363205\n",
      "Classifier 16/50: error = 0.4000263840172948, alpha = 0.20267758791393561\n",
      "Classifier 17/50: error = 0.3919962134420071, alpha = 0.21946446479524293\n",
      "Classifier 18/50: error = 0.35670031245819256, alpha = 0.2948573621234294\n",
      "Classifier 19/50: error = 0.390355406654876, alpha = 0.2229092650799391\n",
      "Classifier 20/50: error = 0.4121660747470014, alpha = 0.17750905285571647\n",
      "Classifier 21/50: error = 0.3869881846171508, alpha = 0.22999502424152343\n",
      "Classifier 22/50: error = 0.41256448641155385, alpha = 0.1766869757829122\n",
      "Classifier 23/50: error = 0.43723575237395296, alpha = 0.12619413426514045\n",
      "Classifier 24/50: error = 0.42776046847208815, alpha = 0.14549713963108543\n",
      "Classifier 25/50: error = 0.41757085610282707, alpha = 0.16637664574382466\n",
      "Classifier 26/50: error = 0.4171878392860111, alpha = 0.16716418287641413\n",
      "Classifier 27/50: error = 0.4343974707330136, alpha = 0.1319658200919465\n",
      "Classifier 28/50: error = 0.4206692905131041, alpha = 0.16001324811820153\n",
      "Classifier 29/50: error = 0.4409562998583979, alpha = 0.11864093418642568\n",
      "Classifier 30/50: error = 0.4216874788849252, alpha = 0.15792497407085665\n",
      "Classifier 31/50: error = 0.42437100341264433, alpha = 0.15242763463960002\n",
      "Classifier 32/50: error = 0.43805937639168846, alpha = 0.12452086429626173\n",
      "Classifier 33/50: error = 0.420600240553594, alpha = 0.16015491744744267\n",
      "Classifier 34/50: error = 0.4178435921371981, alpha = 0.1658159862900426\n",
      "Classifier 35/50: error = 0.41295135726386245, alpha = 0.17588893799602873\n",
      "Classifier 36/50: error = 0.4266288331814827, alpha = 0.1478094377280781\n",
      "Classifier 37/50: error = 0.3981754694268803, alpha = 0.20653656959972283\n",
      "Classifier 38/50: error = 0.43860206909994, alpha = 0.12341871239051672\n",
      "Classifier 39/50: error = 0.4538922193349062, alpha = 0.09247829460069072\n",
      "Classifier 40/50: error = 0.4493243304507457, alpha = 0.10170052389201222\n",
      "Classifier 41/50: error = 0.4391051198167191, alpha = 0.12239733563336619\n",
      "Classifier 42/50: error = 0.4179097748427283, alpha = 0.165679951062376\n",
      "Classifier 43/50: error = 0.43375282454851183, alpha = 0.1332779226992223\n",
      "Classifier 44/50: error = 0.45214110509191163, alpha = 0.09601172609398316\n",
      "Classifier 45/50: error = 0.4426674072319984, alpha = 0.11517173091645407\n",
      "Classifier 46/50: error = 0.45746866606819925, alpha = 0.08526872460518688\n",
      "Classifier 47/50: error = 0.4277006261005536, alpha = 0.14561937811907075\n",
      "Classifier 48/50: error = 0.4499813437906993, alpha = 0.1003730371646512\n",
      "Classifier 49/50: error = 0.44983628757300287, alpha = 0.10066609083735698\n",
      "Classifier 50/50: error = 0.4535040090285557, alpha = 0.09326143100771123\n",
      "Accuracy for digit 7: 0.9408\n",
      "Running AdaBoost for digit: 8\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Classifier 1/50: error = 0.30681138364242366, alpha = 0.4075344812031741\n",
      "Classifier 2/50: error = 0.3557336077562744, alpha = 0.2969650675392191\n",
      "Classifier 3/50: error = 0.34300890752674207, alpha = 0.32495702203977483\n",
      "Classifier 4/50: error = 0.37632568901443963, alpha = 0.25258666591827805\n",
      "Classifier 5/50: error = 0.33568370136883663, alpha = 0.3412945188208875\n",
      "Classifier 6/50: error = 0.38316768129518364, alpha = 0.23806225730949065\n",
      "Classifier 7/50: error = 0.37718786603683485, alpha = 0.25075077042886007\n",
      "Classifier 8/50: error = 0.4362745993805457, alpha = 0.12814769717822724\n",
      "Classifier 9/50: error = 0.40811472826682915, alpha = 0.18588224333525047\n",
      "Classifier 10/50: error = 0.4142338389219308, alpha = 0.17324501232754272\n",
      "Classifier 11/50: error = 0.3846432076917131, alpha = 0.23494303912253472\n",
      "Classifier 12/50: error = 0.4164859968689678, alpha = 0.16860780490092556\n",
      "Classifier 13/50: error = 0.428915499190593, alpha = 0.14313862905653166\n",
      "Classifier 14/50: error = 0.41382813752281056, alpha = 0.1740811331527695\n",
      "Classifier 15/50: error = 0.4417638273229254, alpha = 0.11700335468231451\n",
      "Classifier 16/50: error = 0.43724601676749775, alpha = 0.12617327687347202\n",
      "Classifier 17/50: error = 0.4215884183972889, alpha = 0.15812808392121996\n",
      "Classifier 18/50: error = 0.4452462733494583, alpha = 0.10994836355809295\n",
      "Classifier 19/50: error = 0.4411039680138864, alpha = 0.11834143186163831\n",
      "Classifier 20/50: error = 0.4422794502056995, alpha = 0.11595805374312095\n",
      "Classifier 21/50: error = 0.4486713468521799, alpha = 0.1030202225933975\n",
      "Classifier 22/50: error = 0.42865369895623706, alpha = 0.14367307152326303\n",
      "Classifier 23/50: error = 0.4419606164200167, alpha = 0.11660438237519999\n",
      "Classifier 24/50: error = 0.44343088431164523, alpha = 0.11362470595935521\n",
      "Classifier 25/50: error = 0.43909325128291066, alpha = 0.12242143015860484\n",
      "Classifier 26/50: error = 0.4497412230405424, alpha = 0.10085815682618106\n",
      "Classifier 27/50: error = 0.4084371596839105, alpha = 0.18521492300374964\n",
      "Classifier 28/50: error = 0.45261995018068346, alpha = 0.0950452696387367\n",
      "Classifier 29/50: error = 0.44858218989692, alpha = 0.10320043901958233\n",
      "Classifier 30/50: error = 0.4347617358892798, alpha = 0.13122460057543509\n",
      "Classifier 31/50: error = 0.45720567942725343, alpha = 0.0857985553147089\n",
      "Classifier 32/50: error = 0.44205716614162327, alpha = 0.11640864996349615\n",
      "Classifier 33/50: error = 0.45564838068405356, alpha = 0.08893698999009794\n",
      "Classifier 34/50: error = 0.4554885127031779, alpha = 0.08925927089467565\n",
      "Classifier 35/50: error = 0.4490777081916415, alpha = 0.1021989128642586\n",
      "Classifier 36/50: error = 0.4536073141713183, alpha = 0.09305302250910409\n",
      "Classifier 37/50: error = 0.4490435866644492, alpha = 0.1022678716606626\n",
      "Classifier 38/50: error = 0.4278893014446587, alpha = 0.14523399044780033\n",
      "Classifier 39/50: error = 0.45096136493064576, alpha = 0.09839357101769738\n",
      "Classifier 40/50: error = 0.4649080821660422, alpha = 0.07029941392971294\n",
      "Classifier 41/50: error = 0.47093826739492683, alpha = 0.058189051749050494\n",
      "Classifier 42/50: error = 0.4672241065589816, alpha = 0.06564592250360457\n",
      "Classifier 43/50: error = 0.4617218398988574, alpha = 0.07670641055518722\n",
      "Classifier 44/50: error = 0.46132882698475686, alpha = 0.07749711836388165\n",
      "Classifier 45/50: error = 0.45334873426555955, alpha = 0.09357469857454426\n",
      "Classifier 46/50: error = 0.45291743353605246, alpha = 0.09444494611135332\n",
      "Classifier 47/50: error = 0.4507848371569809, alpha = 0.09875006812370317\n",
      "Classifier 48/50: error = 0.4500347222900358, alpha = 0.10026520216233964\n",
      "Classifier 49/50: error = 0.4604010632920159, alpha = 0.07936408371983114\n",
      "Classifier 50/50: error = 0.46383172027925745, alpha = 0.0724631259190724\n",
      "Accuracy for digit 8: 0.8656\n",
      "Running AdaBoost for digit: 9\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Classifier 1/50: error = 0.31601949907547494, alpha = 0.38606274600373075\n",
      "Classifier 2/50: error = 0.3463627099968958, alpha = 0.31753303713088127\n",
      "Classifier 3/50: error = 0.3267544165987863, alpha = 0.3614506511152323\n",
      "Classifier 4/50: error = 0.41780942642607044, alpha = 0.16588621457335173\n",
      "Classifier 5/50: error = 0.3414263135226463, alpha = 0.32847226627211434\n",
      "Classifier 6/50: error = 0.3965268423356617, alpha = 0.20997891235381747\n",
      "Classifier 7/50: error = 0.38065380168381124, alpha = 0.24338704882352005\n",
      "Classifier 8/50: error = 0.38002321479402545, alpha = 0.2447248458012321\n",
      "Classifier 9/50: error = 0.40738683137257015, alpha = 0.18738933845443617\n",
      "Classifier 10/50: error = 0.3556627593124163, alpha = 0.29711963900236404\n",
      "Classifier 11/50: error = 0.41835022897820373, alpha = 0.16477477422379905\n",
      "Classifier 12/50: error = 0.4312499718608347, alpha = 0.1383765582946128\n",
      "Classifier 13/50: error = 0.3992191266459968, alpha = 0.2043599044167248\n",
      "Classifier 14/50: error = 0.4066172016851035, alpha = 0.18898375627584446\n",
      "Classifier 15/50: error = 0.42519081548570553, alpha = 0.1507500434322826\n",
      "Classifier 16/50: error = 0.4263319704964168, alpha = 0.14841628344054048\n",
      "Classifier 17/50: error = 0.4495421717516688, alpha = 0.1012603390796202\n",
      "Classifier 18/50: error = 0.42886460894592116, alpha = 0.14324251070792826\n",
      "Classifier 19/50: error = 0.4415842697411272, alpha = 0.11736742398358517\n",
      "Classifier 20/50: error = 0.4391612381917738, alpha = 0.1222834106193022\n",
      "Classifier 21/50: error = 0.44422828379972534, alpha = 0.1120095222583389\n",
      "Classifier 22/50: error = 0.43054585484054786, alpha = 0.139812214183055\n",
      "Classifier 23/50: error = 0.44188404299188583, alpha = 0.11675962375829413\n",
      "Classifier 24/50: error = 0.4380352983995127, alpha = 0.12456977112413666\n",
      "Classifier 25/50: error = 0.4437700943930143, alpha = 0.11293754256366334\n",
      "Classifier 26/50: error = 0.4408861588758236, alpha = 0.11878320238724571\n",
      "Classifier 27/50: error = 0.4424986612294639, alpha = 0.11551373284681554\n",
      "Classifier 28/50: error = 0.4227623167917204, alpha = 0.1557219943222842\n",
      "Classifier 29/50: error = 0.4598457284523828, alpha = 0.08048186323630512\n",
      "Classifier 30/50: error = 0.4475351917660887, alpha = 0.10531728018781637\n",
      "Classifier 31/50: error = 0.43882893914484966, alpha = 0.12295805171440337\n",
      "Classifier 32/50: error = 0.40384571724357793, alpha = 0.19473329007945683\n",
      "Classifier 33/50: error = 0.4489444679393298, alpha = 0.10246819375075562\n",
      "Classifier 34/50: error = 0.43970385112121124, alpha = 0.12118202325515359\n",
      "Classifier 35/50: error = 0.45924543830525627, alpha = 0.08169035475396279\n",
      "Classifier 36/50: error = 0.4408940833878449, alpha = 0.11876712871904793\n",
      "Classifier 37/50: error = 0.44278573787820436, alpha = 0.11493192310899515\n",
      "Classifier 38/50: error = 0.4462333546306094, alpha = 0.10795067418567962\n",
      "Classifier 39/50: error = 0.4465239413978128, alpha = 0.10736273879053088\n",
      "Classifier 40/50: error = 0.43942590617629496, alpha = 0.12174615490682254\n",
      "Classifier 41/50: error = 0.45750122588418635, alpha = 0.08520313072160003\n",
      "Classifier 42/50: error = 0.4513088656914934, alpha = 0.09769186742008873\n",
      "Classifier 43/50: error = 0.4478868615135593, alpha = 0.10460616350138627\n",
      "Classifier 44/50: error = 0.4395051151241254, alpha = 0.12158538042713243\n",
      "Classifier 45/50: error = 0.45653776746305663, alpha = 0.0871443927390834\n",
      "Classifier 46/50: error = 0.45616242834796356, alpha = 0.08790083605373863\n",
      "Classifier 47/50: error = 0.45454504891639425, alpha = 0.09116159639730954\n",
      "Classifier 48/50: error = 0.4526669932300776, alpha = 0.09495033189487431\n",
      "Classifier 49/50: error = 0.462427608847656, alpha = 0.07528670441265005\n",
      "Classifier 50/50: error = 0.4575206024394296, alpha = 0.08516409572812156\n",
      "Accuracy for digit 9: 0.8639\n",
      "Accuracies for all digits: {0: 0.9593, 1: 0.966, 2: 0.91, 3: 0.8948, 4: 0.9016, 5: 0.8801, 6: 0.9507, 7: 0.9408, 8: 0.8656, 9: 0.8639}\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=50, A=50, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Accuracy: 0.828\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostMulticlass: # Creamos la clase AdaBoostMulticlass\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.models = [] # Inicializamos los modelos\n",
    "\n",
    "    def fit(self, X, y, verbose=False): # Creamos la función fit\n",
    "        for digit in range(10): # Para cada dígito\n",
    "            y_binary = np.where(y == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "            model = AdaBoost(T=self.T, A=self.A) # Creamos el clasificador AdaBoost\n",
    "            model.fit(X, y_binary, verbose) # Ajustamos el clasificador AdaBoost\n",
    "            self.models.append(model) # Añadimos el clasificador AdaBoost\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        model_preds = np.array([model.predict(X) for model in self.models]) # Realizamos las predicciones\n",
    "        return np.argmax(model_preds, axis=0) # Devolvemos el índice del valor máximo\n",
    "\n",
    "def run_adaboost_multiclass_on_mnist(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_multiclass_on_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    adaboost_multiclass = AdaBoostMulticlass(T=T, A=A) # Creamos el clasificador AdaBoostMulticlass\n",
    "    adaboost_multiclass.fit(X_train, y_train, verboseParam) # Ajustamos el clasificador AdaBoostMulticlass\n",
    "    y_pred = adaboost_multiclass.predict(X_test) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test == y_pred) / len(y_test) # Calculamos la precisión\n",
    "    print(f\"Multiclass Accuracy: {accuracy}\") # Mostramos la precisión\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "accuracy = run_adaboost_multiclass_on_mnist(T=50, A=20, verboseParam=False) # Ejecutamos AdaBoostMulticlass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclase con ADABoosti Binario con Mejoras (Version 1: Solo añadido el parámetro n_componentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False):\n",
    "    print(f\"Running AdaBoost for digit: {digit}\")\n",
    "\n",
    "    # Load MNIST data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Balance the training dataset for the specified digit\n",
    "    X_train_balanced, Y_train_binary_balanced = balance_training_dataset(digit, X_train, y_train)\n",
    "\n",
    "    # Flatten the images\n",
    "    X_train_balanced = X_train_balanced.reshape(X_train_balanced.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # Create the AdaBoost classifier (assuming AdaBoost class is properly defined and imported)\n",
    "    adaboost = AdaBoost(T=T, A=A)\n",
    "    adaboost.fit(X_train_balanced, Y_train_binary_balanced, verboseParam)  # Train the model\n",
    "    y_pred = adaboost.predict(X_test)  # Predict on the test set\n",
    "\n",
    "    # Convert test labels to binary\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1)\n",
    "\n",
    "    # Calculate accuracy using the correct labels\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary)\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3238038175126252, alpha = 0.36817270683057024\n",
      "Classifier 2/50: error = 0.3303056143923935, alpha = 0.3534015685867737\n",
      "Classifier 3/50: error = 0.33487353706673195, alpha = 0.3431121164959386\n",
      "Classifier 4/50: error = 0.381654878632417, alpha = 0.24126500456687197\n",
      "Classifier 5/50: error = 0.3554041397618325, alpha = 0.29768399159368425\n",
      "Classifier 6/50: error = 0.3388560748273497, alpha = 0.3341980476720072\n",
      "Classifier 7/50: error = 0.386269231956563, alpha = 0.23151086245633945\n",
      "Classifier 8/50: error = 0.37330672312370117, alpha = 0.25902841675653543\n",
      "Classifier 9/50: error = 0.3910403751816382, alpha = 0.22147057596270858\n",
      "Classifier 10/50: error = 0.3010857676873029, alpha = 0.4210664336595687\n",
      "Classifier 11/50: error = 0.36034709874284154, alpha = 0.28692897860419947\n",
      "Classifier 12/50: error = 0.37353219306854657, alpha = 0.2585465967551975\n",
      "Classifier 13/50: error = 0.3688186201019261, alpha = 0.2686441453511719\n",
      "Classifier 14/50: error = 0.3820006397618717, alpha = 0.24053256938466633\n",
      "Classifier 15/50: error = 0.38938800436988363, alpha = 0.22494271930693496\n",
      "Classifier 16/50: error = 0.4179166579676391, alpha = 0.1656658034937022\n",
      "Classifier 17/50: error = 0.42586309247127907, alpha = 0.14937498361828047\n",
      "Classifier 18/50: error = 0.4137427809315265, alpha = 0.17425707747510105\n",
      "Classifier 19/50: error = 0.38383724698087596, alpha = 0.23664625561409125\n",
      "Classifier 20/50: error = 0.43011802758334783, alpha = 0.14068480980934336\n",
      "Classifier 21/50: error = 0.40540284083621614, alpha = 0.19150144563237526\n",
      "Classifier 22/50: error = 0.3661962082339437, alpha = 0.27428507594303014\n",
      "Classifier 23/50: error = 0.42559895021764477, alpha = 0.14991518692824202\n",
      "Classifier 24/50: error = 0.43926322766775927, alpha = 0.12207637150206181\n",
      "Classifier 25/50: error = 0.4270660498743116, alpha = 0.14691587745574072\n",
      "Classifier 26/50: error = 0.4463938396871834, alpha = 0.10762596047144003\n",
      "Classifier 27/50: error = 0.42725922753561085, alpha = 0.14652114547117934\n",
      "Classifier 28/50: error = 0.4295747300009608, alpha = 0.14179322549849677\n",
      "Classifier 29/50: error = 0.4173555971196432, alpha = 0.166819223645016\n",
      "Classifier 30/50: error = 0.46153175790181034, alpha = 0.0770888269906441\n",
      "Classifier 31/50: error = 0.42693338379745754, alpha = 0.1471869886624377\n",
      "Classifier 32/50: error = 0.44608152553244607, alpha = 0.10825789494243532\n",
      "Classifier 33/50: error = 0.4441102722216579, alpha = 0.11224852537288299\n",
      "Classifier 34/50: error = 0.4575326635086485, alpha = 0.0851397982604331\n",
      "Classifier 35/50: error = 0.43919309329432776, alpha = 0.12221874349145251\n",
      "Classifier 36/50: error = 0.42551872897472287, alpha = 0.15007926639085362\n",
      "Classifier 37/50: error = 0.4397394776347638, alpha = 0.12110971936979734\n",
      "Classifier 38/50: error = 0.4179564544532477, alpha = 0.1655840071216198\n",
      "Classifier 39/50: error = 0.4479592588642607, alpha = 0.1044597808361079\n",
      "Classifier 40/50: error = 0.45891043485477123, alpha = 0.08236487996157402\n",
      "Classifier 41/50: error = 0.44829221958972243, alpha = 0.10378661355114538\n",
      "Classifier 42/50: error = 0.44541056359121767, alpha = 0.10961580705006904\n",
      "Classifier 43/50: error = 0.4444938624850797, alpha = 0.11147170521469381\n",
      "Classifier 44/50: error = 0.4676175920830369, alpha = 0.06485559600061933\n",
      "Classifier 45/50: error = 0.4577985950043615, alpha = 0.08460409490818852\n",
      "Classifier 46/50: error = 0.457686466973124, alpha = 0.08482996430582183\n",
      "Classifier 47/50: error = 0.46001255588105444, alpha = 0.08014605155999686\n",
      "Classifier 48/50: error = 0.45097880886337516, alpha = 0.09835834442283403\n",
      "Classifier 49/50: error = 0.4518113586506898, alpha = 0.09667735955601754\n",
      "Classifier 50/50: error = 0.46477215324557064, alpha = 0.07057262275424385\n",
      "Accuracy for digit 4: 0.9088\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=4, T=50, A=50, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n",
      "Total elements for target digit 0: 5923\n",
      "Each non-target digit will have 658 samples.\n",
      "Collected 658 samples for digit 1.\n",
      "Collected 658 samples for digit 2.\n",
      "Collected 658 samples for digit 3.\n",
      "Collected 658 samples for digit 4.\n",
      "Collected 658 samples for digit 5.\n",
      "Collected 658 samples for digit 6.\n",
      "Collected 658 samples for digit 7.\n",
      "Collected 658 samples for digit 8.\n",
      "Collected 658 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3511186154495568, alpha = 0.3070629156255924\n",
      "Classifier 2/50: error = 0.20106241385725143, alpha = 0.6898337254080968\n",
      "Classifier 3/50: error = 0.20274171865047586, alpha = 0.6846229229138829\n",
      "Classifier 4/50: error = 0.25230076133422674, alpha = 0.5431894681450896\n",
      "Classifier 5/50: error = 0.35104183271278433, alpha = 0.3072314297145393\n",
      "Classifier 6/50: error = 0.38280134339680305, alpha = 0.2388373874099552\n",
      "Classifier 7/50: error = 0.24998877302048728, alpha = 0.5493360832609213\n",
      "Classifier 8/50: error = 0.363080954464222, alpha = 0.2810083679264333\n",
      "Classifier 9/50: error = 0.328533402096462, alpha = 0.3574128793223373\n",
      "Classifier 10/50: error = 0.35927320047153594, alpha = 0.28926002649537097\n",
      "Classifier 11/50: error = 0.31706888300619485, alpha = 0.38363747694913086\n",
      "Classifier 12/50: error = 0.39335812669797554, alpha = 0.21660908075590093\n",
      "Classifier 13/50: error = 0.4240266673021089, alpha = 0.15313250716390084\n",
      "Classifier 14/50: error = 0.41331568777294647, alpha = 0.17513759917611496\n",
      "Classifier 15/50: error = 0.352758419599414, alpha = 0.3034680756223638\n",
      "Classifier 16/50: error = 0.4052109412195802, alpha = 0.1918995227590306\n",
      "Classifier 17/50: error = 0.35820026167774976, alpha = 0.2915920503329471\n",
      "Classifier 18/50: error = 0.3936662364001958, alpha = 0.21596358241195834\n",
      "Classifier 19/50: error = 0.3758453159300069, alpha = 0.2536102818701903\n",
      "Classifier 20/50: error = 0.4528141401530144, alpha = 0.09465338517947944\n",
      "Classifier 21/50: error = 0.40257556823654794, alpha = 0.19737248764110019\n",
      "Classifier 22/50: error = 0.4542519395484721, alpha = 0.09175273221191994\n",
      "Classifier 23/50: error = 0.42405535975183245, alpha = 0.15307376658652144\n",
      "Classifier 24/50: error = 0.4192703444558328, alpha = 0.16288470713097877\n",
      "Classifier 25/50: error = 0.41849125402465326, alpha = 0.164485010412863\n",
      "Classifier 26/50: error = 0.4418306122738703, alpha = 0.1168679500125043\n",
      "Classifier 27/50: error = 0.4627533006599704, alpha = 0.07463165385940017\n",
      "Classifier 28/50: error = 0.4226095542304731, alpha = 0.15603500345130047\n",
      "Classifier 29/50: error = 0.4687408903969857, alpha = 0.06259986213925707\n",
      "Classifier 30/50: error = 0.41146124595136846, alpha = 0.1789639692719997\n",
      "Classifier 31/50: error = 0.4285168171335305, alpha = 0.14395253632554939\n",
      "Classifier 32/50: error = 0.4540949351169563, alpha = 0.09206940119053701\n",
      "Classifier 33/50: error = 0.405897382205994, alpha = 0.1904758435628488\n",
      "Classifier 34/50: error = 0.4432726255706718, alpha = 0.11394533912168008\n",
      "Classifier 35/50: error = 0.4335060397269024, alpha = 0.13378034514177137\n",
      "Classifier 36/50: error = 0.44634683112669266, alpha = 0.10772107180689261\n",
      "Classifier 37/50: error = 0.45648408238372196, alpha = 0.08725258136628954\n",
      "Classifier 38/50: error = 0.40561515856610086, alpha = 0.19106108296628987\n",
      "Classifier 39/50: error = 0.4089486384341211, alpha = 0.184156675000491\n",
      "Classifier 40/50: error = 0.4327510289977292, alpha = 0.1353178688639993\n",
      "Classifier 41/50: error = 0.42638602164668804, alpha = 0.148305784191312\n",
      "Classifier 42/50: error = 0.4523982723510317, alpha = 0.09549266146795625\n",
      "Classifier 43/50: error = 0.4494304999051092, alpha = 0.1014859858421066\n",
      "Classifier 44/50: error = 0.45370663424853164, alpha = 0.09285266110851914\n",
      "Classifier 45/50: error = 0.45728006137046906, alpha = 0.08564869555024303\n",
      "Classifier 46/50: error = 0.4460986394385215, alpha = 0.1082232645479057\n",
      "Classifier 47/50: error = 0.4413702206627732, alpha = 0.1178014684044237\n",
      "Classifier 48/50: error = 0.4642797947058337, alpha = 0.07156232168949167\n",
      "Classifier 49/50: error = 0.43702721337929107, alpha = 0.12661791210455647\n",
      "Classifier 50/50: error = 0.44636902874788476, alpha = 0.1076761596304465\n",
      "Accuracy for digit 0: 0.9547\n",
      "Running AdaBoost for digit: 1\n",
      "Total elements for target digit 1: 6742\n",
      "Each non-target digit will have 749 samples.\n",
      "Collected 749 samples for digit 0.\n",
      "Collected 749 samples for digit 2.\n",
      "Collected 749 samples for digit 3.\n",
      "Collected 749 samples for digit 4.\n",
      "Collected 749 samples for digit 5.\n",
      "Collected 749 samples for digit 6.\n",
      "Collected 749 samples for digit 7.\n",
      "Collected 749 samples for digit 8.\n",
      "Collected 749 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3085366758139878, alpha = 0.4034846954309479\n",
      "Classifier 2/50: error = 0.36252368267398183, alpha = 0.28221366162794076\n",
      "Classifier 3/50: error = 0.25122659710461964, alpha = 0.5460405477238945\n",
      "Classifier 4/50: error = 0.2891971831958554, alpha = 0.4496431543408099\n",
      "Classifier 5/50: error = 0.38081804037999156, alpha = 0.24303875501344008\n",
      "Classifier 6/50: error = 0.3355401994635857, alpha = 0.341616305785768\n",
      "Classifier 7/50: error = 0.36496421890791647, alpha = 0.27694101348862266\n",
      "Classifier 8/50: error = 0.3123565245571107, alpha = 0.3945626284077712\n",
      "Classifier 9/50: error = 0.31161602730507076, alpha = 0.39628751687042796\n",
      "Classifier 10/50: error = 0.3723057405371192, alpha = 0.26116889997224474\n",
      "Classifier 11/50: error = 0.4066962250878454, alpha = 0.18882000233796492\n",
      "Classifier 12/50: error = 0.34892462786682277, alpha = 0.3118847407496704\n",
      "Classifier 13/50: error = 0.3601366633903436, alpha = 0.28738551847074817\n",
      "Classifier 14/50: error = 0.4023030530075664, alpha = 0.19793908999989396\n",
      "Classifier 15/50: error = 0.27034465488739, alpha = 0.4964373239450171\n",
      "Classifier 16/50: error = 0.3875882737254288, alpha = 0.2287305927785909\n",
      "Classifier 17/50: error = 0.3575168751784783, alpha = 0.2930789935573473\n",
      "Classifier 18/50: error = 0.3744585059820692, alpha = 0.2565683332927111\n",
      "Classifier 19/50: error = 0.3638242671292927, alpha = 0.27940193307663935\n",
      "Classifier 20/50: error = 0.4072082273349751, alpha = 0.18775926267630527\n",
      "Classifier 21/50: error = 0.43075569469583297, alpha = 0.1393843026197289\n",
      "Classifier 22/50: error = 0.4189499958390136, alpha = 0.1635426238180441\n",
      "Classifier 23/50: error = 0.3633863783204383, alpha = 0.28034812038680423\n",
      "Classifier 24/50: error = 0.41248279909323005, alpha = 0.17685550895964997\n",
      "Classifier 25/50: error = 0.40622727692101757, alpha = 0.18979191332044887\n",
      "Classifier 26/50: error = 0.3739025066078556, alpha = 0.2577555065980095\n",
      "Classifier 27/50: error = 0.4155010561014224, alpha = 0.1706349089711997\n",
      "Classifier 28/50: error = 0.4192303585980236, alpha = 0.16296682053018846\n",
      "Classifier 29/50: error = 0.3802817780457719, alpha = 0.24417619740418142\n",
      "Classifier 30/50: error = 0.41759402899492326, alpha = 0.1663290055461583\n",
      "Classifier 31/50: error = 0.46854077800150784, alpha = 0.06300166747266757\n",
      "Classifier 32/50: error = 0.425761176716001, alpha = 0.1495834035693115\n",
      "Classifier 33/50: error = 0.4404994859236171, alpha = 0.11956758408543845\n",
      "Classifier 34/50: error = 0.4347388921783353, alpha = 0.13127107953805742\n",
      "Classifier 35/50: error = 0.43672115826978153, alpha = 0.1272399370950823\n",
      "Classifier 36/50: error = 0.4394576332859974, alpha = 0.12168175600726273\n",
      "Classifier 37/50: error = 0.40990266042860735, alpha = 0.18218389380952613\n",
      "Classifier 38/50: error = 0.38002538680797116, alpha = 0.24472023637808993\n",
      "Classifier 39/50: error = 0.439014584606215, alpha = 0.1225811363793769\n",
      "Classifier 40/50: error = 0.40493080097017975, alpha = 0.19248075444438922\n",
      "Classifier 41/50: error = 0.3849571872075107, alpha = 0.23427987676857182\n",
      "Classifier 42/50: error = 0.45880687984368534, alpha = 0.0825734017689175\n",
      "Classifier 43/50: error = 0.42493703369349156, alpha = 0.15126926971380078\n",
      "Classifier 44/50: error = 0.4415870419952692, alpha = 0.1173618027514283\n",
      "Classifier 45/50: error = 0.4193698607358082, alpha = 0.16268035382673685\n",
      "Classifier 46/50: error = 0.43311691434789734, alpha = 0.13457269133469923\n",
      "Classifier 47/50: error = 0.4361286425586116, alpha = 0.12844444210851125\n",
      "Classifier 48/50: error = 0.4484781371910928, alpha = 0.10341077325474861\n",
      "Classifier 49/50: error = 0.4576287455217394, alpha = 0.08494624108919131\n",
      "Classifier 50/50: error = 0.39357364863311284, alpha = 0.21615753769463505\n",
      "Accuracy for digit 1: 0.9625\n",
      "Running AdaBoost for digit: 2\n",
      "Total elements for target digit 2: 5958\n",
      "Each non-target digit will have 662 samples.\n",
      "Collected 662 samples for digit 0.\n",
      "Collected 662 samples for digit 1.\n",
      "Collected 662 samples for digit 3.\n",
      "Collected 662 samples for digit 4.\n",
      "Collected 662 samples for digit 5.\n",
      "Collected 662 samples for digit 6.\n",
      "Collected 662 samples for digit 7.\n",
      "Collected 662 samples for digit 8.\n",
      "Collected 662 samples for digit 9.\n",
      "Classifier 1/50: error = 0.36018798254447804, alpha = 0.28727417073105593\n",
      "Classifier 2/50: error = 0.3368105844213591, alpha = 0.33876996781097707\n",
      "Classifier 3/50: error = 0.39768996107726395, alpha = 0.20754980901324216\n",
      "Classifier 4/50: error = 0.3689960925783907, alpha = 0.2682630000353888\n",
      "Classifier 5/50: error = 0.3421495714263263, alpha = 0.3268648041699764\n",
      "Classifier 6/50: error = 0.33690882562310626, alpha = 0.33855007593177766\n",
      "Classifier 7/50: error = 0.3536789900298625, alpha = 0.30145330288991234\n",
      "Classifier 8/50: error = 0.3270530787627429, alpha = 0.3607719896937514\n",
      "Classifier 9/50: error = 0.38725541678841335, alpha = 0.22943185755689227\n",
      "Classifier 10/50: error = 0.40135814386146795, alpha = 0.1999046803673627\n",
      "Classifier 11/50: error = 0.3233495042263802, alpha = 0.3692105451996024\n",
      "Classifier 12/50: error = 0.3663414425681646, alpha = 0.2739722272971866\n",
      "Classifier 13/50: error = 0.3702590260956676, alpha = 0.26555287548153633\n",
      "Classifier 14/50: error = 0.40910246358295277, alpha = 0.18383849109407452\n",
      "Classifier 15/50: error = 0.40671693217012583, alpha = 0.1887770943467133\n",
      "Classifier 16/50: error = 0.4154053450589218, alpha = 0.17083196542200907\n",
      "Classifier 17/50: error = 0.4123814429312513, alpha = 0.17706463573235914\n",
      "Classifier 18/50: error = 0.42703567368399475, alpha = 0.1469779511562321\n",
      "Classifier 19/50: error = 0.44048067218292924, alpha = 0.11960575224639773\n",
      "Classifier 20/50: error = 0.42800345841889165, alpha = 0.14500083457435392\n",
      "Classifier 21/50: error = 0.4358396196644372, alpha = 0.12903212122423613\n",
      "Classifier 22/50: error = 0.44981925073288287, alpha = 0.10070051109555156\n",
      "Classifier 23/50: error = 0.4284976868555135, alpha = 0.14399159543938492\n",
      "Classifier 24/50: error = 0.45684683009127125, alpha = 0.08652159509244414\n",
      "Classifier 25/50: error = 0.4461501585345072, alpha = 0.1081190159966591\n",
      "Classifier 26/50: error = 0.4418554151620897, alpha = 0.11681766391850557\n",
      "Classifier 27/50: error = 0.4592214531253334, alpha = 0.08173864613792028\n",
      "Classifier 28/50: error = 0.43599951785997715, alpha = 0.1287069843905772\n",
      "Classifier 29/50: error = 0.4400711816424541, alpha = 0.1204365876187712\n",
      "Classifier 30/50: error = 0.4333223229133246, alpha = 0.13415441274446452\n",
      "Classifier 31/50: error = 0.45312905053045155, alpha = 0.09401794284584698\n",
      "Classifier 32/50: error = 0.4298741653390976, alpha = 0.14118228592781856\n",
      "Classifier 33/50: error = 0.41825395799571374, alpha = 0.16497259771901754\n",
      "Classifier 34/50: error = 0.4438165664161795, alpha = 0.11284340897380814\n",
      "Classifier 35/50: error = 0.4218484984463406, alpha = 0.15759485326056993\n",
      "Classifier 36/50: error = 0.43928550057879956, alpha = 0.12203115877363721\n",
      "Classifier 37/50: error = 0.4661024092850321, alpha = 0.06789933522913884\n",
      "Classifier 38/50: error = 0.45243540324592135, alpha = 0.09541772096984541\n",
      "Classifier 39/50: error = 0.4517490301794266, alpha = 0.09680318676707089\n",
      "Classifier 40/50: error = 0.46681854159689107, alpha = 0.06646059660869891\n",
      "Classifier 41/50: error = 0.43348516197378, alpha = 0.13382285266304342\n",
      "Classifier 42/50: error = 0.46497855037358493, alpha = 0.07015778126462259\n",
      "Classifier 43/50: error = 0.449255011587695, alpha = 0.10184060248774716\n",
      "Classifier 44/50: error = 0.4489420078852756, alpha = 0.1024731657022555\n",
      "Classifier 45/50: error = 0.45817811561991606, alpha = 0.08383965677263605\n",
      "Classifier 46/50: error = 0.44384429810435133, alpha = 0.11278723669877966\n",
      "Classifier 47/50: error = 0.4657136511037616, alpha = 0.06868048322741568\n",
      "Classifier 48/50: error = 0.4536278584564746, alpha = 0.0930115772897003\n",
      "Classifier 49/50: error = 0.4636318537987104, alpha = 0.07286497321841054\n",
      "Classifier 50/50: error = 0.45256671990287645, alpha = 0.09515269591015561\n",
      "Accuracy for digit 2: 0.9033\n",
      "Running AdaBoost for digit: 3\n",
      "Total elements for target digit 3: 6131\n",
      "Each non-target digit will have 681 samples.\n",
      "Collected 681 samples for digit 0.\n",
      "Collected 681 samples for digit 1.\n",
      "Collected 681 samples for digit 2.\n",
      "Collected 681 samples for digit 4.\n",
      "Collected 681 samples for digit 5.\n",
      "Collected 681 samples for digit 6.\n",
      "Collected 681 samples for digit 7.\n",
      "Collected 681 samples for digit 8.\n",
      "Collected 681 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3749592169657425, alpha = 0.2554998175287345\n",
      "Classifier 2/50: error = 0.3759846158151265, alpha = 0.2533133974673467\n",
      "Classifier 3/50: error = 0.3113746007044175, alpha = 0.39685037212117824\n",
      "Classifier 4/50: error = 0.39464561859291825, alpha = 0.21391292338081844\n",
      "Classifier 5/50: error = 0.3611464888773236, alpha = 0.2851957592595209\n",
      "Classifier 6/50: error = 0.34287322221466054, alpha = 0.3252581000441879\n",
      "Classifier 7/50: error = 0.37941121868363303, alpha = 0.24602402399365075\n",
      "Classifier 8/50: error = 0.3963743181650262, alpha = 0.21029763049655195\n",
      "Classifier 9/50: error = 0.38104426680868086, alpha = 0.24255910118828172\n",
      "Classifier 10/50: error = 0.4587287565510947, alpha = 0.08273071816429049\n",
      "Classifier 11/50: error = 0.450399389290571, alpha = 0.09952856557929976\n",
      "Classifier 12/50: error = 0.37510841691255703, alpha = 0.25518153578506086\n",
      "Classifier 13/50: error = 0.4101453335132972, alpha = 0.18168230481073344\n",
      "Classifier 14/50: error = 0.3898884786485417, alpha = 0.22389050810209254\n",
      "Classifier 15/50: error = 0.4267470439307627, alpha = 0.14756782181643205\n",
      "Classifier 16/50: error = 0.45006012230702697, alpha = 0.10021388997940193\n",
      "Classifier 17/50: error = 0.384489077751629, alpha = 0.23526865426062096\n",
      "Classifier 18/50: error = 0.4297757468177992, alpha = 0.1413830782069371\n",
      "Classifier 19/50: error = 0.4251183320871347, alpha = 0.15089833300625574\n",
      "Classifier 20/50: error = 0.3942372207188196, alpha = 0.21476782242507597\n",
      "Classifier 21/50: error = 0.4016510034755234, alpha = 0.1992953144327213\n",
      "Classifier 22/50: error = 0.45627284037867155, alpha = 0.08767830572183163\n",
      "Classifier 23/50: error = 0.42578489435512445, alpha = 0.14953489932697234\n",
      "Classifier 24/50: error = 0.43252422160583015, alpha = 0.13577986931463265\n",
      "Classifier 25/50: error = 0.4527356194391473, alpha = 0.09481184016030404\n",
      "Classifier 26/50: error = 0.4559059761493776, alpha = 0.08841773712372505\n",
      "Classifier 27/50: error = 0.4695558365682002, alpha = 0.060963740232455425\n",
      "Classifier 28/50: error = 0.4736340387116065, alpha = 0.052780880713289735\n",
      "Classifier 29/50: error = 0.4211087948327845, alpha = 0.15911166912948446\n",
      "Classifier 30/50: error = 0.4607750149896244, alpha = 0.07861150435469293\n",
      "Classifier 31/50: error = 0.42873508863421406, alpha = 0.14350691283861164\n",
      "Classifier 32/50: error = 0.40689063364329026, alpha = 0.1884171874610826\n",
      "Classifier 33/50: error = 0.45752814401481523, alpha = 0.0851489029352898\n",
      "Classifier 34/50: error = 0.4179673503432749, alpha = 0.16556161245479187\n",
      "Classifier 35/50: error = 0.43804655622801714, alpha = 0.12454690433136727\n",
      "Classifier 36/50: error = 0.43018724791868557, alpha = 0.14054361375159813\n",
      "Classifier 37/50: error = 0.458671336344028, alpha = 0.08284634748788046\n",
      "Classifier 38/50: error = 0.46450205303032654, alpha = 0.07111553893971778\n",
      "Classifier 39/50: error = 0.43230756236490264, alpha = 0.13622125206377747\n",
      "Classifier 40/50: error = 0.45715025975236667, alpha = 0.08591021366800863\n",
      "Classifier 41/50: error = 0.45764987273839586, alpha = 0.08490368117303727\n",
      "Classifier 42/50: error = 0.44092042800979125, alpha = 0.11871369309654582\n",
      "Classifier 43/50: error = 0.470027480611665, alpha = 0.06001699629289404\n",
      "Classifier 44/50: error = 0.46399003155842106, alpha = 0.0721448452112476\n",
      "Classifier 45/50: error = 0.4265476814957504, alpha = 0.14797531698150465\n",
      "Classifier 46/50: error = 0.4505005765041872, alpha = 0.09932418395691693\n",
      "Classifier 47/50: error = 0.4530907498955099, alpha = 0.0940952237803851\n",
      "Classifier 48/50: error = 0.46362640406759764, alpha = 0.07287593066041576\n",
      "Classifier 49/50: error = 0.4569643898100234, alpha = 0.08628471599611919\n",
      "Classifier 50/50: error = 0.42629792719966486, alpha = 0.14848588156238457\n",
      "Accuracy for digit 3: 0.8805\n",
      "Running AdaBoost for digit: 4\n",
      "Total elements for target digit 4: 5842\n",
      "Each non-target digit will have 649 samples.\n",
      "Collected 649 samples for digit 0.\n",
      "Collected 649 samples for digit 1.\n",
      "Collected 649 samples for digit 2.\n",
      "Collected 649 samples for digit 3.\n",
      "Collected 649 samples for digit 5.\n",
      "Collected 649 samples for digit 6.\n",
      "Collected 649 samples for digit 7.\n",
      "Collected 649 samples for digit 8.\n",
      "Collected 649 samples for digit 9.\n",
      "Classifier 1/50: error = 0.32346143969870755, alpha = 0.368954767958435\n",
      "Classifier 2/50: error = 0.31212238714592466, alpha = 0.3951077781271967\n",
      "Classifier 3/50: error = 0.3887595633933833, alpha = 0.22626466569001127\n",
      "Classifier 4/50: error = 0.3983398049970438, alpha = 0.2061937016541941\n",
      "Classifier 5/50: error = 0.36776774392661327, alpha = 0.2709026060114335\n",
      "Classifier 6/50: error = 0.34351736251195353, alpha = 0.32382929494850915\n",
      "Classifier 7/50: error = 0.38371047031258304, alpha = 0.23691429205026002\n",
      "Classifier 8/50: error = 0.39657208933478916, alpha = 0.20988437123434028\n",
      "Classifier 9/50: error = 0.4109247696712063, alpha = 0.18007187267905103\n",
      "Classifier 10/50: error = 0.4022574195587395, alpha = 0.19803398142838485\n",
      "Classifier 11/50: error = 0.3624802361788557, alpha = 0.2823076632914471\n",
      "Classifier 12/50: error = 0.3805251232252995, alpha = 0.24365997205705528\n",
      "Classifier 13/50: error = 0.40285010674508703, alpha = 0.19680180503906583\n",
      "Classifier 14/50: error = 0.3249631090462033, alpha = 0.3655278386122697\n",
      "Classifier 15/50: error = 0.41616822958697025, alpha = 0.16926165016972516\n",
      "Classifier 16/50: error = 0.4402990428733846, alpha = 0.11997424848222826\n",
      "Classifier 17/50: error = 0.34334058122266886, alpha = 0.3242212963895377\n",
      "Classifier 18/50: error = 0.40369477138997867, alpha = 0.19504679418959187\n",
      "Classifier 19/50: error = 0.4141977374856011, alpha = 0.17331940500246773\n",
      "Classifier 20/50: error = 0.41779327288960666, alpha = 0.16591941904604285\n",
      "Classifier 21/50: error = 0.4685449150134986, alpha = 0.06299336056821522\n",
      "Classifier 22/50: error = 0.4233523471422951, alpha = 0.15451331100451046\n",
      "Classifier 23/50: error = 0.41924935455136925, alpha = 0.16292781090917988\n",
      "Classifier 24/50: error = 0.38416244739273986, alpha = 0.23595885607023317\n",
      "Classifier 25/50: error = 0.4113247892148686, alpha = 0.1792457314280701\n",
      "Classifier 26/50: error = 0.44344974646473756, alpha = 0.1135864926763213\n",
      "Classifier 27/50: error = 0.42999131796321133, alpha = 0.14094328729920771\n",
      "Classifier 28/50: error = 0.4459718941025342, alpha = 0.10847974287827795\n",
      "Classifier 29/50: error = 0.4641204375279109, alpha = 0.07188267836510577\n",
      "Classifier 30/50: error = 0.43748441923716186, alpha = 0.12568887039339274\n",
      "Classifier 31/50: error = 0.46030343079052616, alpha = 0.07956058427513424\n",
      "Classifier 32/50: error = 0.41920813799259127, alpha = 0.16301245284107865\n",
      "Classifier 33/50: error = 0.4135502464509917, alpha = 0.17465398528813236\n",
      "Classifier 34/50: error = 0.414646876330022, alpha = 0.17239401882765798\n",
      "Classifier 35/50: error = 0.4411251066731241, alpha = 0.11829855990958212\n",
      "Classifier 36/50: error = 0.45621869367453294, alpha = 0.08778743481147637\n",
      "Classifier 37/50: error = 0.45529235283253305, alpha = 0.08965473858527535\n",
      "Classifier 38/50: error = 0.4421056294094179, alpha = 0.11631040515364638\n",
      "Classifier 39/50: error = 0.4405964456628242, alpha = 0.11937088362032523\n",
      "Classifier 40/50: error = 0.4577007504523294, alpha = 0.0848011913525494\n",
      "Classifier 41/50: error = 0.46242980318506255, alpha = 0.07528229081687299\n",
      "Classifier 42/50: error = 0.44366726419404223, alpha = 0.11314584219115613\n",
      "Classifier 43/50: error = 0.44286520470518076, alpha = 0.1147708837512719\n",
      "Classifier 44/50: error = 0.44140733362144735, alpha = 0.11772620833072157\n",
      "Classifier 45/50: error = 0.4713590119582184, alpha = 0.05734475138631026\n",
      "Classifier 46/50: error = 0.4439259738834309, alpha = 0.11262180139797572\n",
      "Classifier 47/50: error = 0.44829860790401344, alpha = 0.10377369881943817\n",
      "Classifier 48/50: error = 0.41314994305809183, alpha = 0.1754793810904321\n",
      "Classifier 49/50: error = 0.467039410106775, alpha = 0.06601691859215705\n",
      "Classifier 50/50: error = 0.46759544793735164, alpha = 0.06490007096900502\n",
      "Accuracy for digit 4: 0.8953\n",
      "Running AdaBoost for digit: 5\n",
      "Total elements for target digit 5: 5421\n",
      "Each non-target digit will have 602 samples.\n",
      "Collected 602 samples for digit 0.\n",
      "Collected 602 samples for digit 1.\n",
      "Collected 602 samples for digit 2.\n",
      "Collected 602 samples for digit 3.\n",
      "Collected 602 samples for digit 4.\n",
      "Collected 602 samples for digit 6.\n",
      "Collected 602 samples for digit 7.\n",
      "Collected 602 samples for digit 8.\n",
      "Collected 602 samples for digit 9.\n",
      "Classifier 1/50: error = 0.34514254082479956, alpha = 0.32023004936063215\n",
      "Classifier 2/50: error = 0.3585250416810828, alpha = 0.2908858190812984\n",
      "Classifier 3/50: error = 0.38234392427749253, alpha = 0.23980563089072268\n",
      "Classifier 4/50: error = 0.39244741950928586, alpha = 0.21851807920609934\n",
      "Classifier 5/50: error = 0.3643660747904711, alpha = 0.27823187299076946\n",
      "Classifier 6/50: error = 0.4077628513584942, alpha = 0.18661069238514286\n",
      "Classifier 7/50: error = 0.40338005819429634, alpha = 0.1957005536107511\n",
      "Classifier 8/50: error = 0.42923106307152586, alpha = 0.14249454066890868\n",
      "Classifier 9/50: error = 0.3960204952542288, alpha = 0.21103714956216899\n",
      "Classifier 10/50: error = 0.4158081185157261, alpha = 0.17000279634788484\n",
      "Classifier 11/50: error = 0.39963239565648456, alpha = 0.2034985138652489\n",
      "Classifier 12/50: error = 0.3966953768342423, alpha = 0.20962678753404793\n",
      "Classifier 13/50: error = 0.42307616112982804, alpha = 0.15507902495785864\n",
      "Classifier 14/50: error = 0.3963792137877479, alpha = 0.2102873998328669\n",
      "Classifier 15/50: error = 0.4295490661026602, alpha = 0.1418455925776418\n",
      "Classifier 16/50: error = 0.45086537728139453, alpha = 0.09858741457880053\n",
      "Classifier 17/50: error = 0.4370536057656932, alpha = 0.12656427691143057\n",
      "Classifier 18/50: error = 0.43290467238108654, alpha = 0.13500493412633602\n",
      "Classifier 19/50: error = 0.4471414497346746, alpha = 0.10611359785318385\n",
      "Classifier 20/50: error = 0.4375153486923025, alpha = 0.12562602959193425\n",
      "Classifier 21/50: error = 0.4175169151887418, alpha = 0.1664875435537182\n",
      "Classifier 22/50: error = 0.4490041943945565, alpha = 0.10234748370839587\n",
      "Classifier 23/50: error = 0.44475864530207776, alpha = 0.11093556378930211\n",
      "Classifier 24/50: error = 0.4622317827731982, alpha = 0.07568059235624564\n",
      "Classifier 25/50: error = 0.46000782462446965, alpha = 0.0801555749920791\n",
      "Classifier 26/50: error = 0.46699472088958305, alpha = 0.06610668765629583\n",
      "Classifier 27/50: error = 0.4472661164955838, alpha = 0.10586145297464863\n",
      "Classifier 28/50: error = 0.4372748686607765, alpha = 0.12611465000640684\n",
      "Classifier 29/50: error = 0.4491138469626538, alpha = 0.1021258783221489\n",
      "Classifier 30/50: error = 0.4449690017704363, alpha = 0.11050967200547232\n",
      "Classifier 31/50: error = 0.41816479154363195, alpha = 0.16515583380616417\n",
      "Classifier 32/50: error = 0.4366570986083531, alpha = 0.1273701440345343\n",
      "Classifier 33/50: error = 0.457678110325033, alpha = 0.08484679818568774\n",
      "Classifier 34/50: error = 0.39084335812555093, alpha = 0.2218842925249505\n",
      "Classifier 35/50: error = 0.42580895606454994, alpha = 0.14948569215030588\n",
      "Classifier 36/50: error = 0.45648395911396367, alpha = 0.08725282978748782\n",
      "Classifier 37/50: error = 0.4543958726368912, alpha = 0.09146244350933766\n",
      "Classifier 38/50: error = 0.4599135594088335, alpha = 0.08034532219389968\n",
      "Classifier 39/50: error = 0.4612799467749493, alpha = 0.07759546783808464\n",
      "Classifier 40/50: error = 0.44511023481137885, alpha = 0.11022375126305677\n",
      "Classifier 41/50: error = 0.47684943015005865, alpha = 0.04633426904014226\n",
      "Classifier 42/50: error = 0.47271996364851443, alpha = 0.05461430776261737\n",
      "Classifier 43/50: error = 0.47404734913349256, alpha = 0.05195199095410196\n",
      "Classifier 44/50: error = 0.44724121608391115, alpha = 0.10591181425370583\n",
      "Classifier 45/50: error = 0.4652184742063459, alpha = 0.06967558410562368\n",
      "Classifier 46/50: error = 0.4679128450585251, alpha = 0.06426262545421316\n",
      "Classifier 47/50: error = 0.4687817246368453, alpha = 0.06251787362359838\n",
      "Classifier 48/50: error = 0.46567371185313966, alpha = 0.06876073955089647\n",
      "Classifier 49/50: error = 0.4697141295825127, alpha = 0.0606459822662853\n",
      "Classifier 50/50: error = 0.46561172555399805, alpha = 0.0688853002866361\n",
      "Accuracy for digit 5: 0.863\n",
      "Running AdaBoost for digit: 6\n",
      "Total elements for target digit 6: 5918\n",
      "Each non-target digit will have 657 samples.\n",
      "Collected 657 samples for digit 0.\n",
      "Collected 657 samples for digit 1.\n",
      "Collected 657 samples for digit 2.\n",
      "Collected 657 samples for digit 3.\n",
      "Collected 657 samples for digit 4.\n",
      "Collected 657 samples for digit 5.\n",
      "Collected 657 samples for digit 7.\n",
      "Collected 657 samples for digit 8.\n",
      "Collected 657 samples for digit 9.\n",
      "Classifier 1/50: error = 0.40410785225255674, alpha = 0.19418894355028415\n",
      "Classifier 2/50: error = 0.2615122359306591, alpha = 0.5190617296142895\n",
      "Classifier 3/50: error = 0.3328905538434309, alpha = 0.3475701753374167\n",
      "Classifier 4/50: error = 0.2615613289290738, alpha = 0.5189346347743782\n",
      "Classifier 5/50: error = 0.3504672066837953, alpha = 0.30849309211563275\n",
      "Classifier 6/50: error = 0.35201797134909035, alpha = 0.3050903665053435\n",
      "Classifier 7/50: error = 0.32512075828235376, alpha = 0.3651685481433548\n",
      "Classifier 8/50: error = 0.39968766809534717, alpha = 0.20338333026307007\n",
      "Classifier 9/50: error = 0.3971921290882824, alpha = 0.20858920392506222\n",
      "Classifier 10/50: error = 0.421969090820132, alpha = 0.1573476382084454\n",
      "Classifier 11/50: error = 0.4125227838586821, alpha = 0.17677301315592125\n",
      "Classifier 12/50: error = 0.32369994920596934, alpha = 0.3684099177552005\n",
      "Classifier 13/50: error = 0.4414714558892156, alpha = 0.11759618014931521\n",
      "Classifier 14/50: error = 0.35672923182483196, alpha = 0.29479434855001196\n",
      "Classifier 15/50: error = 0.3897248126605799, alpha = 0.22423455031563827\n",
      "Classifier 16/50: error = 0.41875015757848466, alpha = 0.16395311314298136\n",
      "Classifier 17/50: error = 0.39502008203630856, alpha = 0.21312932986829694\n",
      "Classifier 18/50: error = 0.3963276903438111, alpha = 0.21039507350454298\n",
      "Classifier 19/50: error = 0.4025538534642399, alpha = 0.19741763150321603\n",
      "Classifier 20/50: error = 0.3679318628743408, alpha = 0.270549717168433\n",
      "Classifier 21/50: error = 0.3909967730112698, alpha = 0.22156212982658674\n",
      "Classifier 22/50: error = 0.4556764870669133, alpha = 0.08888033170446227\n",
      "Classifier 23/50: error = 0.3964021357410972, alpha = 0.21023949908886103\n",
      "Classifier 24/50: error = 0.4314169574994728, alpha = 0.1380361671095233\n",
      "Classifier 25/50: error = 0.41655018127963106, alpha = 0.16847575493333097\n",
      "Classifier 26/50: error = 0.3930777556701015, alpha = 0.21719662029209505\n",
      "Classifier 27/50: error = 0.37012464566305314, alpha = 0.2658410601815508\n",
      "Classifier 28/50: error = 0.4211747179427896, alpha = 0.1589764596422005\n",
      "Classifier 29/50: error = 0.3837451637985418, alpha = 0.23684093833828737\n",
      "Classifier 30/50: error = 0.397312640847967, alpha = 0.20833755364334328\n",
      "Classifier 31/50: error = 0.36316470175026827, alpha = 0.2808273041754548\n",
      "Classifier 32/50: error = 0.40174634095155426, alpha = 0.19909697306981045\n",
      "Classifier 33/50: error = 0.39993761496193636, alpha = 0.20286252622088563\n",
      "Classifier 34/50: error = 0.422619303732382, alpha = 0.15601502590317243\n",
      "Classifier 35/50: error = 0.4425904015650971, alpha = 0.11532779698052327\n",
      "Classifier 36/50: error = 0.4326743004506187, alpha = 0.13547415637885496\n",
      "Classifier 37/50: error = 0.41809797832364015, alpha = 0.16529314144781515\n",
      "Classifier 38/50: error = 0.44109506243215124, alpha = 0.1183594936700568\n",
      "Classifier 39/50: error = 0.38050759852971083, alpha = 0.2436971441754342\n",
      "Classifier 40/50: error = 0.40212960554789967, alpha = 0.1982997800874517\n",
      "Classifier 41/50: error = 0.4234658435354077, alpha = 0.15428086392396673\n",
      "Classifier 42/50: error = 0.4211276781802231, alpha = 0.15907293845403028\n",
      "Classifier 43/50: error = 0.46124418902045417, alpha = 0.07766741521188915\n",
      "Classifier 44/50: error = 0.4443897795444858, alpha = 0.11168247341898935\n",
      "Classifier 45/50: error = 0.4722799175571739, alpha = 0.05549707037121887\n",
      "Classifier 46/50: error = 0.4409810516955147, alpha = 0.1185907307369036\n",
      "Classifier 47/50: error = 0.4529426403831173, alpha = 0.09439408163970915\n",
      "Classifier 48/50: error = 0.4389515692511509, alpha = 0.12270907234527552\n",
      "Classifier 49/50: error = 0.4466311354388601, alpha = 0.10714587502609375\n",
      "Classifier 50/50: error = 0.45556852016994476, alpha = 0.08909798001261911\n",
      "Accuracy for digit 6: 0.9447\n",
      "Running AdaBoost for digit: 7\n",
      "Total elements for target digit 7: 6265\n",
      "Each non-target digit will have 696 samples.\n",
      "Collected 696 samples for digit 0.\n",
      "Collected 696 samples for digit 1.\n",
      "Collected 696 samples for digit 2.\n",
      "Collected 696 samples for digit 3.\n",
      "Collected 696 samples for digit 4.\n",
      "Collected 696 samples for digit 5.\n",
      "Collected 696 samples for digit 6.\n",
      "Collected 696 samples for digit 8.\n",
      "Collected 696 samples for digit 9.\n",
      "Classifier 1/50: error = 0.3127145023545377, alpha = 0.3937295680086988\n",
      "Classifier 2/50: error = 0.35550064500458606, alpha = 0.2974733789345773\n",
      "Classifier 3/50: error = 0.3107118338394952, alpha = 0.398396759156837\n",
      "Classifier 4/50: error = 0.259906995478274, alpha = 0.5232260012570984\n",
      "Classifier 5/50: error = 0.34436032585557674, alpha = 0.3219613963128719\n",
      "Classifier 6/50: error = 0.37876840969534725, alpha = 0.2473894907288022\n",
      "Classifier 7/50: error = 0.3991721143866793, alpha = 0.20445791258439341\n",
      "Classifier 8/50: error = 0.3667291864958317, alpha = 0.27313724588540766\n",
      "Classifier 9/50: error = 0.3697306186959055, alpha = 0.26668631717903013\n",
      "Classifier 10/50: error = 0.35650874354472334, alpha = 0.2952748368554703\n",
      "Classifier 11/50: error = 0.3954897203919019, alpha = 0.21214693975913607\n",
      "Classifier 12/50: error = 0.3789382388202367, alpha = 0.24702864886787776\n",
      "Classifier 13/50: error = 0.41732764414828155, alpha = 0.1668767004064078\n",
      "Classifier 14/50: error = 0.3559873788683665, alpha = 0.29641152310527275\n",
      "Classifier 15/50: error = 0.3278846204575262, alpha = 0.3588841208873898\n",
      "Classifier 16/50: error = 0.40281927981915255, alpha = 0.1968658785893134\n",
      "Classifier 17/50: error = 0.4028603528965611, alpha = 0.19678050883806456\n",
      "Classifier 18/50: error = 0.39391707167577195, alpha = 0.21543820643671244\n",
      "Classifier 19/50: error = 0.4114156336796092, alpha = 0.17905814846159426\n",
      "Classifier 20/50: error = 0.3904104281032942, alpha = 0.2227936660765047\n",
      "Classifier 21/50: error = 0.3853410020039477, alpha = 0.23346948868401046\n",
      "Classifier 22/50: error = 0.4042758258575142, alpha = 0.19384019126353258\n",
      "Classifier 23/50: error = 0.4270030580707101, alpha = 0.14704460236630404\n",
      "Classifier 24/50: error = 0.4308692772924345, alpha = 0.13915270283424255\n",
      "Classifier 25/50: error = 0.42933326525170523, alpha = 0.14228596392842374\n",
      "Classifier 26/50: error = 0.44439432916381394, alpha = 0.11167326022254317\n",
      "Classifier 27/50: error = 0.4465084632465828, alpha = 0.10739405339679313\n",
      "Classifier 28/50: error = 0.44363588159152245, alpha = 0.11320941480533735\n",
      "Classifier 29/50: error = 0.44054321096520654, alpha = 0.11947887874717207\n",
      "Classifier 30/50: error = 0.44622867754466233, alpha = 0.107960137798716\n",
      "Classifier 31/50: error = 0.42912349463934096, alpha = 0.14271408229243568\n",
      "Classifier 32/50: error = 0.43958220170242235, alpha = 0.12142891983564659\n",
      "Classifier 33/50: error = 0.40205343784476055, alpha = 0.1984581895925487\n",
      "Classifier 34/50: error = 0.4413456041622865, alpha = 0.11785138807936736\n",
      "Classifier 35/50: error = 0.40715375011402677, alpha = 0.18787210586093936\n",
      "Classifier 36/50: error = 0.4333366542178657, alpha = 0.13412523129420875\n",
      "Classifier 37/50: error = 0.42709509594078254, alpha = 0.14685652291271542\n",
      "Classifier 38/50: error = 0.4250375126145636, alpha = 0.15106368463670566\n",
      "Classifier 39/50: error = 0.4474912742116273, alpha = 0.10540609397523519\n",
      "Classifier 40/50: error = 0.4514683692171647, alpha = 0.09736981625265892\n",
      "Classifier 41/50: error = 0.4569735970205695, alpha = 0.08626616416775756\n",
      "Classifier 42/50: error = 0.4540756971750266, alpha = 0.09210820428665359\n",
      "Classifier 43/50: error = 0.4465972774315201, alpha = 0.10721437191711394\n",
      "Classifier 44/50: error = 0.4427674771541559, alpha = 0.11496892926620018\n",
      "Classifier 45/50: error = 0.44674803068336466, alpha = 0.10690939624469466\n",
      "Classifier 46/50: error = 0.46383124761767147, alpha = 0.0724640762148055\n",
      "Classifier 47/50: error = 0.4590176858004609, alpha = 0.08214892343081767\n",
      "Classifier 48/50: error = 0.41526579822657705, alpha = 0.1711192975507186\n",
      "Classifier 49/50: error = 0.4413454393902836, alpha = 0.11785172222163569\n",
      "Classifier 50/50: error = 0.4611877034181704, alpha = 0.07778107025752172\n",
      "Accuracy for digit 7: 0.9324\n",
      "Running AdaBoost for digit: 8\n",
      "Total elements for target digit 8: 5851\n",
      "Each non-target digit will have 650 samples.\n",
      "Collected 650 samples for digit 0.\n",
      "Collected 650 samples for digit 1.\n",
      "Collected 650 samples for digit 2.\n",
      "Collected 650 samples for digit 3.\n",
      "Collected 650 samples for digit 4.\n",
      "Collected 650 samples for digit 5.\n",
      "Collected 650 samples for digit 6.\n",
      "Collected 650 samples for digit 7.\n",
      "Collected 650 samples for digit 9.\n",
      "Classifier 1/50: error = 0.39979488932569884, alpha = 0.20315990446568838\n",
      "Classifier 2/50: error = 0.39704427771976125, alpha = 0.20889797982122407\n",
      "Classifier 3/50: error = 0.29177126604280224, alpha = 0.44339847602312965\n",
      "Classifier 4/50: error = 0.3639909194636313, alpha = 0.2790419599482591\n",
      "Classifier 5/50: error = 0.3991291585478408, alpha = 0.20454746757183268\n",
      "Classifier 6/50: error = 0.3875175347981008, alpha = 0.22887960740551552\n",
      "Classifier 7/50: error = 0.40488993968070064, alpha = 0.19256554369373088\n",
      "Classifier 8/50: error = 0.43925891322692945, alpha = 0.12208512962622156\n",
      "Classifier 9/50: error = 0.37247807819685474, alpha = 0.2608002099107229\n",
      "Classifier 10/50: error = 0.36096616688181493, alpha = 0.2855865832201569\n",
      "Classifier 11/50: error = 0.3851423965684665, alpha = 0.23388878732084492\n",
      "Classifier 12/50: error = 0.3942526296974545, alpha = 0.21473556121268228\n",
      "Classifier 13/50: error = 0.45210787322394425, alpha = 0.09607880482505296\n",
      "Classifier 14/50: error = 0.4036172003375682, alpha = 0.1952079186320092\n",
      "Classifier 15/50: error = 0.4596752035120191, alpha = 0.08082513645998902\n",
      "Classifier 16/50: error = 0.4126329230683661, alpha = 0.17654578835365337\n",
      "Classifier 17/50: error = 0.417253246780128, alpha = 0.16702968123098305\n",
      "Classifier 18/50: error = 0.4216488934396081, alpha = 0.15800408666075372\n",
      "Classifier 19/50: error = 0.4188965646376136, alpha = 0.1636523719084673\n",
      "Classifier 20/50: error = 0.43312224322660176, alpha = 0.13456183941514976\n",
      "Classifier 21/50: error = 0.43561052306356496, alpha = 0.12949801333232858\n",
      "Classifier 22/50: error = 0.45952349956094096, alpha = 0.08113053828708042\n",
      "Classifier 23/50: error = 0.42636606576155, alpha = 0.14834658050513075\n",
      "Classifier 24/50: error = 0.4423986421284208, alpha = 0.11571645686524229\n",
      "Classifier 25/50: error = 0.4173088008735988, alpha = 0.166915446470987\n",
      "Classifier 26/50: error = 0.44348492531556816, alpha = 0.11351522389099462\n",
      "Classifier 27/50: error = 0.4440472075367041, alpha = 0.11237625244891992\n",
      "Classifier 28/50: error = 0.4435895761680443, alpha = 0.113303218658889\n",
      "Classifier 29/50: error = 0.449930667193138, alpha = 0.10047541595013351\n",
      "Classifier 30/50: error = 0.45349033596217736, alpha = 0.09328901574991488\n",
      "Classifier 31/50: error = 0.44390085845215566, alpha = 0.11267267236067073\n",
      "Classifier 32/50: error = 0.42881821574632417, alpha = 0.14333721527027632\n",
      "Classifier 33/50: error = 0.4421767427393748, alpha = 0.11616624814913278\n",
      "Classifier 34/50: error = 0.4623277705078235, alpha = 0.07548751805015884\n",
      "Classifier 35/50: error = 0.454068205406301, alpha = 0.09212331532412496\n",
      "Classifier 36/50: error = 0.45943133044611795, alpha = 0.0813160953150885\n",
      "Classifier 37/50: error = 0.44046146051705415, alpha = 0.11964472805144195\n",
      "Classifier 38/50: error = 0.45538387571598327, alpha = 0.08947022059754192\n",
      "Classifier 39/50: error = 0.45497179649848357, alpha = 0.09030105575994714\n",
      "Classifier 40/50: error = 0.4469826129524283, alpha = 0.1064348728209728\n",
      "Classifier 41/50: error = 0.4564809638296379, alpha = 0.08725886608169851\n",
      "Classifier 42/50: error = 0.47544063927343394, alpha = 0.049158280799538545\n",
      "Classifier 43/50: error = 0.46205807832409784, alpha = 0.07603000409483868\n",
      "Classifier 44/50: error = 0.4514925949607157, alpha = 0.09732090418139522\n",
      "Classifier 45/50: error = 0.4231511664908531, alpha = 0.1549253811691539\n",
      "Classifier 46/50: error = 0.4345734219226365, alpha = 0.13160777048874578\n",
      "Classifier 47/50: error = 0.4499175465699137, alpha = 0.10050192307310213\n",
      "Classifier 48/50: error = 0.4585392387535132, alpha = 0.08311236598486743\n",
      "Classifier 49/50: error = 0.47952475245947324, alpha = 0.0409734086814997\n",
      "Classifier 50/50: error = 0.47720111464226755, alpha = 0.045629411824562795\n",
      "Accuracy for digit 8: 0.8691\n",
      "Running AdaBoost for digit: 9\n",
      "Total elements for target digit 9: 5949\n",
      "Each non-target digit will have 661 samples.\n",
      "Collected 661 samples for digit 0.\n",
      "Collected 661 samples for digit 1.\n",
      "Collected 661 samples for digit 2.\n",
      "Collected 661 samples for digit 3.\n",
      "Collected 661 samples for digit 4.\n",
      "Collected 661 samples for digit 5.\n",
      "Collected 661 samples for digit 6.\n",
      "Collected 661 samples for digit 7.\n",
      "Collected 661 samples for digit 8.\n",
      "Classifier 1/50: error = 0.3451840645486638, alpha = 0.32013819311865654\n",
      "Classifier 2/50: error = 0.37095156432473364, alpha = 0.26406837908733294\n",
      "Classifier 3/50: error = 0.37740517368520043, alpha = 0.25028830323217643\n",
      "Classifier 4/50: error = 0.3974143003466539, alpha = 0.20812529050730463\n",
      "Classifier 5/50: error = 0.4007725633461725, alpha = 0.201123563588908\n",
      "Classifier 6/50: error = 0.3446697457949982, alpha = 0.32127630666035795\n",
      "Classifier 7/50: error = 0.3493219841523885, alpha = 0.3110104164186199\n",
      "Classifier 8/50: error = 0.3901442151035036, alpha = 0.22335302891681194\n",
      "Classifier 9/50: error = 0.4522929124619246, alpha = 0.095705312923913\n",
      "Classifier 10/50: error = 0.43757297462120115, alpha = 0.1255089509647529\n",
      "Classifier 11/50: error = 0.3758634420994029, alpha = 0.253571647797173\n",
      "Classifier 12/50: error = 0.3878592875920197, alpha = 0.228159782612221\n",
      "Classifier 13/50: error = 0.3603560027635908, alpha = 0.2869096638840103\n",
      "Classifier 14/50: error = 0.3962688816092229, alpha = 0.21051797784952916\n",
      "Classifier 15/50: error = 0.4193081292736702, alpha = 0.16280711570247608\n",
      "Classifier 16/50: error = 0.3921860621310015, alpha = 0.21906621816264965\n",
      "Classifier 17/50: error = 0.40413022186955694, alpha = 0.19414249632953248\n",
      "Classifier 18/50: error = 0.41422468568757564, alpha = 0.17326387382364566\n",
      "Classifier 19/50: error = 0.4412823322197893, alpha = 0.11797969960999391\n",
      "Classifier 20/50: error = 0.4106742580085191, alpha = 0.1805893662358333\n",
      "Classifier 21/50: error = 0.42920575428928154, alpha = 0.14254619336029062\n",
      "Classifier 22/50: error = 0.4532500522535949, alpha = 0.09377379951321918\n",
      "Classifier 23/50: error = 0.41707949279906154, alpha = 0.16738699592800535\n",
      "Classifier 24/50: error = 0.4532489859107136, alpha = 0.09377595100827596\n",
      "Classifier 25/50: error = 0.4525581278797281, alpha = 0.09517003604006531\n",
      "Classifier 26/50: error = 0.44787459501802585, alpha = 0.10463096598818401\n",
      "Classifier 27/50: error = 0.4505878317500087, alpha = 0.09914794927228193\n",
      "Classifier 28/50: error = 0.43668897675181684, alpha = 0.1273053483459191\n",
      "Classifier 29/50: error = 0.4446167680535813, alpha = 0.11122283384146588\n",
      "Classifier 30/50: error = 0.45474845501060457, alpha = 0.09075140931985945\n",
      "Classifier 31/50: error = 0.45977407040044066, alpha = 0.08062611132395224\n",
      "Classifier 32/50: error = 0.4608106029231529, alpha = 0.07853988813269652\n",
      "Classifier 33/50: error = 0.43536914804810145, alpha = 0.12998893538218398\n",
      "Classifier 34/50: error = 0.4342504326016051, alpha = 0.1322650592417679\n",
      "Classifier 35/50: error = 0.43772997646342837, alpha = 0.1251899876033549\n",
      "Classifier 36/50: error = 0.4742183172217318, alpha = 0.0516091371462322\n",
      "Classifier 37/50: error = 0.4597281874522692, alpha = 0.08071847573121806\n",
      "Classifier 38/50: error = 0.46234286073208375, alpha = 0.07545716536491075\n",
      "Classifier 39/50: error = 0.46153206134162983, alpha = 0.07708821649738826\n",
      "Classifier 40/50: error = 0.4551696518543774, alpha = 0.0899021238434468\n",
      "Classifier 41/50: error = 0.4414445000873412, alpha = 0.11765084108193805\n",
      "Classifier 42/50: error = 0.43721129545091597, alpha = 0.12624383151600566\n",
      "Classifier 43/50: error = 0.44671138747364436, alpha = 0.10698352408148996\n",
      "Classifier 44/50: error = 0.4484252536308755, alpha = 0.1035176766418338\n",
      "Classifier 45/50: error = 0.4362254137919246, alpha = 0.12824769392735216\n",
      "Classifier 46/50: error = 0.46155401941256846, alpha = 0.07704403901251344\n",
      "Classifier 47/50: error = 0.46247354388998946, alpha = 0.07519431325688948\n",
      "Classifier 48/50: error = 0.47013290203928493, alpha = 0.059805395741921945\n",
      "Classifier 49/50: error = 0.4461127170605609, alpha = 0.10819477833990918\n",
      "Classifier 50/50: error = 0.47809504594157015, alpha = 0.04383796866571629\n",
      "Accuracy for digit 9: 0.8613\n",
      "Accuracies for all digits: {0: 0.9547, 1: 0.9625, 2: 0.9033, 3: 0.8805, 4: 0.8953, 5: 0.863, 6: 0.9447, 7: 0.9324, 8: 0.8691, 9: 0.8613}\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=50, A=20, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 1/50: error = 0.09866666666666671, alpha = 1.1060639909681136\n",
      "Classifier 2/50: error = 0.4502146468560444, alpha = 0.09990173647425882\n",
      "Classifier 3/50: error = 0.41605589022012435, alpha = 0.1694928365100204\n",
      "Classifier 4/50: error = 0.4256140034960232, alpha = 0.14988439879660334\n",
      "Classifier 5/50: error = 0.4679091233703502, alpha = 0.06427009961528354\n",
      "Classifier 6/50: error = 0.46177830605167136, alpha = 0.07659281345335602\n",
      "Classifier 7/50: error = 0.12352752043735227, alpha = 0.9797206681384945\n",
      "Classifier 8/50: error = 0.4005681545376596, alpha = 0.20154917831513952\n",
      "Classifier 9/50: error = 0.44751133652885994, alpha = 0.10536552205762431\n",
      "Classifier 10/50: error = 0.282074728259039, alpha = 0.4670967277007162\n",
      "Classifier 11/50: error = 0.31405173560506616, alpha = 0.3906222364828058\n",
      "Classifier 12/50: error = 0.40777727175438827, alpha = 0.1865808357065479\n",
      "Classifier 13/50: error = 0.44276836203847625, alpha = 0.11496713600213491\n",
      "Classifier 14/50: error = 0.4233947781670756, alpha = 0.1544264079102889\n",
      "Classifier 15/50: error = 0.445631137733536, alpha = 0.10916935856127623\n",
      "Classifier 16/50: error = 0.4484172676917242, alpha = 0.10353382031244579\n",
      "Classifier 17/50: error = 0.44216403487918554, alpha = 0.11619200846671121\n",
      "Classifier 18/50: error = 0.4533081709438904, alpha = 0.09365653828182696\n",
      "Classifier 19/50: error = 0.3068307085950346, alpha = 0.40748904961412574\n",
      "Classifier 20/50: error = 0.3521947405150831, alpha = 0.3047029314263768\n",
      "Classifier 21/50: error = 0.43573318839249114, alpha = 0.12924855350420905\n",
      "Classifier 22/50: error = 0.4019720725980366, alpha = 0.19862741953152524\n",
      "Classifier 23/50: error = 0.3988160818145229, alpha = 0.2052002725620045\n",
      "Classifier 24/50: error = 0.3725567206632443, alpha = 0.26063198942577853\n",
      "Classifier 25/50: error = 0.42793446845328426, alpha = 0.14514173881562614\n",
      "Classifier 26/50: error = 0.41460274473225767, alpha = 0.1724849326794881\n",
      "Classifier 27/50: error = 0.3065150336949436, alpha = 0.4082313787930079\n",
      "Classifier 28/50: error = 0.34251896294740575, alpha = 0.32604445003604077\n",
      "Classifier 29/50: error = 0.4074547419245783, alpha = 0.18724869560082116\n",
      "Classifier 30/50: error = 0.3753711448280235, alpha = 0.25462119272751205\n",
      "Classifier 31/50: error = 0.4187944064846552, alpha = 0.16386221640490958\n",
      "Classifier 32/50: error = 0.4586442540120015, alpha = 0.08290088500924112\n",
      "Classifier 33/50: error = 0.38427136908895454, alpha = 0.2357286694005776\n",
      "Classifier 34/50: error = 0.3559201261108991, alpha = 0.2965582026222522\n",
      "Classifier 35/50: error = 0.4238510329015968, alpha = 0.15349209731436603\n",
      "Classifier 36/50: error = 0.3775106034792798, alpha = 0.25006396870322595\n",
      "Classifier 37/50: error = 0.36670733073954953, alpha = 0.27318430094665486\n",
      "Classifier 38/50: error = 0.3792823975429457, alpha = 0.2462975960409056\n",
      "Classifier 39/50: error = 0.4058207963322814, alpha = 0.19063464482227768\n",
      "Classifier 40/50: error = 0.4019584456733242, alpha = 0.19865576299558454\n",
      "Classifier 41/50: error = 0.4356380081700377, alpha = 0.12944211652125642\n",
      "Classifier 42/50: error = 0.42575246149890833, alpha = 0.14960122697755687\n",
      "Classifier 43/50: error = 0.41293681306691055, alpha = 0.17591893577033388\n",
      "Classifier 44/50: error = 0.4585995989631098, alpha = 0.08299081097220753\n",
      "Classifier 45/50: error = 0.43606884464075096, alpha = 0.12856602379671186\n",
      "Classifier 46/50: error = 0.446919458741703, alpha = 0.10656261925484954\n",
      "Classifier 47/50: error = 0.46475960489680945, alpha = 0.07059784469821369\n",
      "Classifier 48/50: error = 0.4380067443511322, alpha = 0.12462777041334043\n",
      "Classifier 49/50: error = 0.451747623800698, alpha = 0.09680602596568842\n",
      "Classifier 50/50: error = 0.4222482176343567, alpha = 0.15677549976676378\n",
      "Classifier 1/50: error = 0.11133333333333306, alpha = 1.0385967534881404\n",
      "Classifier 2/50: error = 0.3541387779902273, alpha = 0.30044789471724426\n",
      "Classifier 3/50: error = 0.381861548088011, alpha = 0.24082718056843297\n",
      "Classifier 4/50: error = 0.4463523918411556, alpha = 0.10770982084045339\n",
      "Classifier 5/50: error = 0.3106741819680915, alpha = 0.3984846639199364\n",
      "Classifier 6/50: error = 0.3405587474300983, alpha = 0.33040262288208283\n",
      "Classifier 7/50: error = 0.3161196036338427, alpha = 0.385831204280073\n",
      "Classifier 8/50: error = 0.4225085279677915, alpha = 0.15624202202346313\n",
      "Classifier 9/50: error = 0.3216531003998969, alpha = 0.37309258569835296\n",
      "Classifier 10/50: error = 0.39026174994443186, alpha = 0.22310604950605858\n",
      "Classifier 11/50: error = 0.10346921338854043, alpha = 1.079629259610469\n",
      "Classifier 12/50: error = 0.34574335156562797, alpha = 0.3189014809007806\n",
      "Classifier 13/50: error = 0.32798565722946194, alpha = 0.35865490203937245\n",
      "Classifier 14/50: error = 0.4413075112298125, alpha = 0.11792863769403815\n",
      "Classifier 15/50: error = 0.4338034034489162, alpha = 0.1331749587717692\n",
      "Classifier 16/50: error = 0.4119304784130433, alpha = 0.17799529071656192\n",
      "Classifier 17/50: error = 0.40087378566877396, alpha = 0.20091282771466912\n",
      "Classifier 18/50: error = 0.395109436097986, alpha = 0.21294238776364574\n",
      "Classifier 19/50: error = 0.43538906929271093, alpha = 0.1299484160813222\n",
      "Classifier 20/50: error = 0.3928800112394581, alpha = 0.21761109807711948\n",
      "Classifier 21/50: error = 0.3383394571382963, alpha = 0.33535147329247605\n",
      "Classifier 22/50: error = 0.39760825008320894, alpha = 0.2077203783378717\n",
      "Classifier 23/50: error = 0.4302015025436523, alpha = 0.14051453777301823\n",
      "Classifier 24/50: error = 0.3918863773566641, alpha = 0.21969489983523516\n",
      "Classifier 25/50: error = 0.4004915033136852, alpha = 0.20170879807615988\n",
      "Classifier 26/50: error = 0.46079243606412823, alpha = 0.07857644654177727\n",
      "Classifier 27/50: error = 0.39042011195533366, alpha = 0.22277332109616838\n",
      "Classifier 28/50: error = 0.4026582792784841, alpha = 0.19720054303520718\n",
      "Classifier 29/50: error = 0.3919550647055371, alpha = 0.21955079175611034\n",
      "Classifier 30/50: error = 0.44232747435265984, alpha = 0.1158607092525552\n",
      "Classifier 31/50: error = 0.40362159237716055, alpha = 0.1951987955682119\n",
      "Classifier 32/50: error = 0.44949080699580646, alpha = 0.10136412663700237\n",
      "Classifier 33/50: error = 0.3898660423329104, alpha = 0.2239376683878459\n",
      "Classifier 34/50: error = 0.4536967721098123, alpha = 0.0928725559670409\n",
      "Classifier 35/50: error = 0.4600619955925824, alpha = 0.08004653642651031\n",
      "Classifier 36/50: error = 0.4446231187137666, alpha = 0.11120997476852826\n",
      "Classifier 37/50: error = 0.42891844978947513, alpha = 0.14313260612824438\n",
      "Classifier 38/50: error = 0.4345001385732843, alpha = 0.13175689341299163\n",
      "Classifier 39/50: error = 0.45779498832419363, alpha = 0.08461136002845938\n",
      "Classifier 40/50: error = 0.39891901445193556, alpha = 0.204985625855326\n",
      "Classifier 41/50: error = 0.40074787965329284, alpha = 0.20117495551512488\n",
      "Classifier 42/50: error = 0.4594040178297695, alpha = 0.08137108278880692\n",
      "Classifier 43/50: error = 0.4259126704719036, alpha = 0.14927360017978555\n",
      "Classifier 44/50: error = 0.38282218122463585, alpha = 0.23879328934472174\n",
      "Classifier 45/50: error = 0.45729739966775224, alpha = 0.08561376406024249\n",
      "Classifier 46/50: error = 0.4479138326191502, alpha = 0.10455162918410577\n",
      "Classifier 47/50: error = 0.4198135083970743, alpha = 0.16176950199854048\n",
      "Classifier 48/50: error = 0.4045825614520423, alpha = 0.19320345688724627\n",
      "Classifier 49/50: error = 0.45010273293284253, alpha = 0.1001278107367153\n",
      "Classifier 50/50: error = 0.464230362333473, alpha = 0.07166169430761862\n",
      "Classifier 1/50: error = 0.09383333333333299, alpha = 1.133851544025435\n",
      "Classifier 2/50: error = 0.22657170084337955, alpha = 0.6138857569654499\n",
      "Classifier 3/50: error = 0.4150425778518837, alpha = 0.17157897482943732\n",
      "Classifier 4/50: error = 0.3978540891839164, alpha = 0.20720723232503854\n",
      "Classifier 5/50: error = 0.4199357681868169, alpha = 0.16151853757752002\n",
      "Classifier 6/50: error = 0.3309848134251769, alpha = 0.35186713334746067\n",
      "Classifier 7/50: error = 0.36751692666576574, alpha = 0.2714420410280063\n",
      "Classifier 8/50: error = 0.35472889205781644, alpha = 0.29915836868665635\n",
      "Classifier 9/50: error = 0.40995385690837416, alpha = 0.18207806657909043\n",
      "Classifier 10/50: error = 0.4342462712465065, alpha = 0.13227352841028464\n",
      "Classifier 11/50: error = 0.4312471434826191, alpha = 0.13838232406470757\n",
      "Classifier 12/50: error = 0.40406366466873256, alpha = 0.19428069500326425\n",
      "Classifier 13/50: error = 0.40166330279704876, alpha = 0.19926972589019976\n",
      "Classifier 14/50: error = 0.4056323019271886, alpha = 0.19102552956454447\n",
      "Classifier 15/50: error = 0.42558492076898735, alpha = 0.1499438812979587\n",
      "Classifier 16/50: error = 0.40946838653237727, alpha = 0.1830817353799767\n",
      "Classifier 17/50: error = 0.45043708961640283, alpha = 0.09945241611921501\n",
      "Classifier 18/50: error = 0.45173721146822354, alpha = 0.09682704644007384\n",
      "Classifier 19/50: error = 0.4421006941494861, alpha = 0.11632040981780088\n",
      "Classifier 20/50: error = 0.43173849093088634, alpha = 0.13738082806085267\n",
      "Classifier 21/50: error = 0.43018100380781743, alpha = 0.14055635029809915\n",
      "Classifier 22/50: error = 0.44361898396029076, alpha = 0.11324364518609588\n",
      "Classifier 23/50: error = 0.4179526273502432, alpha = 0.16559187312632836\n",
      "Classifier 24/50: error = 0.4408685073551468, alpha = 0.11881900603371144\n",
      "Classifier 25/50: error = 0.4682092343378235, alpha = 0.06366741820104706\n",
      "Classifier 26/50: error = 0.461515548145762, alpha = 0.077121439625535\n",
      "Classifier 27/50: error = 0.4328835836430932, alpha = 0.1350478852723903\n",
      "Classifier 28/50: error = 0.44878416285155953, alpha = 0.10279219277519852\n",
      "Classifier 29/50: error = 0.45287559938509037, alpha = 0.09452936361465572\n",
      "Classifier 30/50: error = 0.4344030634626017, alpha = 0.13195443872183626\n",
      "Classifier 31/50: error = 0.46185442740670857, alpha = 0.07643967766128786\n",
      "Classifier 32/50: error = 0.4033624320226078, alpha = 0.19573717365662993\n",
      "Classifier 33/50: error = 0.41256363639933585, alpha = 0.17668872943431171\n",
      "Classifier 34/50: error = 0.4155253186899432, alpha = 0.17058495757767275\n",
      "Classifier 35/50: error = 0.44221210861697124, alpha = 0.11609455819040145\n",
      "Classifier 36/50: error = 0.42238162838013066, alpha = 0.15650207790116563\n",
      "Classifier 37/50: error = 0.43693783380808415, alpha = 0.1267995566361994\n",
      "Classifier 38/50: error = 0.4546135768064363, alpha = 0.09102340020055498\n",
      "Classifier 39/50: error = 0.4214529885332352, alpha = 0.15840578509729653\n",
      "Classifier 40/50: error = 0.4410426719083499, alpha = 0.11846575078997416\n",
      "Classifier 41/50: error = 0.43594548051403254, alpha = 0.12881686084583974\n",
      "Classifier 42/50: error = 0.4159286559600759, alpha = 0.16975469714600014\n",
      "Classifier 43/50: error = 0.41659752438811004, alpha = 0.16837835719361424\n",
      "Classifier 44/50: error = 0.45142185441814114, alpha = 0.09746373150053324\n",
      "Classifier 45/50: error = 0.43705321269220393, alpha = 0.12656507571878106\n",
      "Classifier 46/50: error = 0.45617959287231025, alpha = 0.08786624118076934\n",
      "Classifier 47/50: error = 0.4678624362322037, alpha = 0.06436386068346356\n",
      "Classifier 48/50: error = 0.45468934232642155, alpha = 0.09087061233335622\n",
      "Classifier 49/50: error = 0.4425537558402568, alpha = 0.11540206820617468\n",
      "Classifier 50/50: error = 0.4868317282790908, alpha = 0.026342635099442124\n",
      "Classifier 1/50: error = 0.10218333333333335, alpha = 1.0865986518323094\n",
      "Classifier 2/50: error = 0.44076141106705136, alpha = 0.11903624241309832\n",
      "Classifier 3/50: error = 0.44525933049573574, alpha = 0.10992193238156413\n",
      "Classifier 4/50: error = 0.4453540401429476, alpha = 0.10973021915448164\n",
      "Classifier 5/50: error = 0.3424612553681501, alpha = 0.32617258054095\n",
      "Classifier 6/50: error = 0.40151044983906226, alpha = 0.1995877523352648\n",
      "Classifier 7/50: error = 0.43758537428035105, alpha = 0.12548375901895126\n",
      "Classifier 8/50: error = 0.2939247292804801, alpha = 0.43819906782894724\n",
      "Classifier 9/50: error = 0.37095547945805973, alpha = 0.2640599900075154\n",
      "Classifier 10/50: error = 0.3022248109506551, alpha = 0.4183629122038787\n",
      "Classifier 11/50: error = 0.3418428724470186, alpha = 0.3275462520756315\n",
      "Classifier 12/50: error = 0.3764957402681952, alpha = 0.2522244319059468\n",
      "Classifier 13/50: error = 0.4112013956074221, alpha = 0.1795005444796085\n",
      "Classifier 14/50: error = 0.44811340695868584, alpha = 0.10414811841294866\n",
      "Classifier 15/50: error = 0.44617877642456344, alpha = 0.10806110889615253\n",
      "Classifier 16/50: error = 0.45653976771937654, alpha = 0.08714036177040052\n",
      "Classifier 17/50: error = 0.46188979084249954, alpha = 0.0763685371113581\n",
      "Classifier 18/50: error = 0.43369253390401763, alpha = 0.13340066058292813\n",
      "Classifier 19/50: error = 0.42450439903812764, alpha = 0.1521546078246946\n",
      "Classifier 20/50: error = 0.4562938806933138, alpha = 0.08763590092576694\n",
      "Classifier 21/50: error = 0.4492556849644902, alpha = 0.10183924171812275\n",
      "Classifier 22/50: error = 0.42790139880622136, alpha = 0.14520928187728696\n",
      "Classifier 23/50: error = 0.3607404923846107, alpha = 0.2860758224239438\n",
      "Classifier 24/50: error = 0.41743553904206854, alpha = 0.16665485348583117\n",
      "Classifier 25/50: error = 0.4741437773518238, alpha = 0.0517586154662438\n",
      "Classifier 26/50: error = 0.4333925793100043, alpha = 0.13401135860594318\n",
      "Classifier 27/50: error = 0.43841206412341804, alpha = 0.12380455847236072\n",
      "Classifier 28/50: error = 0.4566243200061949, alpha = 0.08696997237932348\n",
      "Classifier 29/50: error = 0.4537834924655061, alpha = 0.09269761780178168\n",
      "Classifier 30/50: error = 0.4642380265527987, alpha = 0.07164628703347024\n",
      "Classifier 31/50: error = 0.4637961114956439, alpha = 0.07253471846881404\n",
      "Classifier 32/50: error = 0.46736484962659813, alpha = 0.0653632268234773\n",
      "Classifier 33/50: error = 0.45685341336930385, alpha = 0.08650832974071139\n",
      "Classifier 34/50: error = 0.4668101855659238, alpha = 0.06647738261543851\n",
      "Classifier 35/50: error = 0.4675822268193355, alpha = 0.06492662478240087\n",
      "Classifier 36/50: error = 0.4433360737393096, alpha = 0.11381678995210823\n",
      "Classifier 37/50: error = 0.46417354446338954, alpha = 0.07177591554602009\n",
      "Classifier 38/50: error = 0.45207290164907843, alpha = 0.09614939609612483\n",
      "Classifier 39/50: error = 0.4625351455292761, alpha = 0.07507041320185481\n",
      "Classifier 40/50: error = 0.36257671416059534, alpha = 0.28209892827932603\n",
      "Classifier 41/50: error = 0.38563389766330686, alpha = 0.23285127024317415\n",
      "Classifier 42/50: error = 0.44080222253703294, alpha = 0.11895345824526908\n",
      "Classifier 43/50: error = 0.4630495529058024, alpha = 0.07403586942773342\n",
      "Classifier 44/50: error = 0.4063292370879077, alpha = 0.18958056733178785\n",
      "Classifier 45/50: error = 0.42169133956008825, alpha = 0.15791705855129756\n",
      "Classifier 46/50: error = 0.45262220249940255, alpha = 0.09504072418746687\n",
      "Classifier 47/50: error = 0.4809054680329964, alpha = 0.03820764522405695\n",
      "Classifier 48/50: error = 0.45532001829143454, alpha = 0.08959896200432119\n",
      "Classifier 49/50: error = 0.44433419821580344, alpha = 0.1117950297891751\n",
      "Classifier 50/50: error = 0.452506450447178, alpha = 0.09527433088698359\n",
      "Classifier 1/50: error = 0.09740000000000001, alpha = 1.1132266379142541\n",
      "Classifier 2/50: error = 0.4361714460474774, alpha = 0.12835741597226882\n",
      "Classifier 3/50: error = 0.3936978779169764, alpha = 0.21589730260172724\n",
      "Classifier 4/50: error = 0.35739952964215393, alpha = 0.2933344459239009\n",
      "Classifier 5/50: error = 0.3595336993344556, alpha = 0.28869429675070035\n",
      "Classifier 6/50: error = 0.31916688602054044, alpha = 0.3788015480290446\n",
      "Classifier 7/50: error = 0.15846264318151582, alpha = 0.8348557647155832\n",
      "Classifier 8/50: error = 0.36811466180382557, alpha = 0.2701567400212543\n",
      "Classifier 9/50: error = 0.37070708507455896, alpha = 0.26459230451694565\n",
      "Classifier 10/50: error = 0.4580820020147961, alpha = 0.08403324147057141\n",
      "Classifier 11/50: error = 0.4534601916315131, alpha = 0.09334983096178003\n",
      "Classifier 12/50: error = 0.42441259019328803, alpha = 0.15234251469135887\n",
      "Classifier 13/50: error = 0.41105621622743443, alpha = 0.1798003756140062\n",
      "Classifier 14/50: error = 0.40219082865194966, alpha = 0.19817245867979222\n",
      "Classifier 15/50: error = 0.40978990588021236, alpha = 0.1824169807432304\n",
      "Classifier 16/50: error = 0.45569433469849313, alpha = 0.08884435383038354\n",
      "Classifier 17/50: error = 0.3810063592124795, alpha = 0.24263946666641853\n",
      "Classifier 18/50: error = 0.3604900816267578, alpha = 0.28661884341532495\n",
      "Classifier 19/50: error = 0.44031116336977566, alpha = 0.11994965696384405\n",
      "Classifier 20/50: error = 0.41383757978762814, alpha = 0.17406167060282346\n",
      "Classifier 21/50: error = 0.42861934113619254, alpha = 0.1437432160770262\n",
      "Classifier 22/50: error = 0.3836965753195166, alpha = 0.2369436714465206\n",
      "Classifier 23/50: error = 0.4339940462283553, alpha = 0.13278689074341446\n",
      "Classifier 24/50: error = 0.4225345365772807, alpha = 0.1561887250562615\n",
      "Classifier 25/50: error = 0.4481856109156569, alpha = 0.10400214066963812\n",
      "Classifier 26/50: error = 0.43549043260753717, alpha = 0.12974225226659764\n",
      "Classifier 27/50: error = 0.44675389196341386, alpha = 0.10689753920393365\n",
      "Classifier 28/50: error = 0.4647549956603231, alpha = 0.07060710919899896\n",
      "Classifier 29/50: error = 0.4353781477383517, alpha = 0.12997063019036445\n",
      "Classifier 30/50: error = 0.42501382684724787, alpha = 0.15111214580127663\n",
      "Classifier 31/50: error = 0.44462063266019125, alpha = 0.11121500862573548\n",
      "Classifier 32/50: error = 0.39974462219022067, alpha = 0.20326464778494388\n",
      "Classifier 33/50: error = 0.46896014320723933, alpha = 0.06215964796951277\n",
      "Classifier 34/50: error = 0.4186921895560646, alpha = 0.1640721959499895\n",
      "Classifier 35/50: error = 0.44561655655888627, alpha = 0.10919886994357898\n",
      "Classifier 36/50: error = 0.44715878294269584, alpha = 0.10607853975259098\n",
      "Classifier 37/50: error = 0.4079721404561186, alpha = 0.1861774024319413\n",
      "Classifier 38/50: error = 0.44157212023515646, alpha = 0.11739205932781595\n",
      "Classifier 39/50: error = 0.4568454946279018, alpha = 0.08652428606427151\n",
      "Classifier 40/50: error = 0.4442166486146363, alpha = 0.11203308586661659\n",
      "Classifier 41/50: error = 0.4653644285702938, alpha = 0.06938226193432367\n",
      "Classifier 42/50: error = 0.405192560762259, alpha = 0.19193765438481827\n",
      "Classifier 43/50: error = 0.4626247238869241, alpha = 0.07489024736913949\n",
      "Classifier 44/50: error = 0.4511336958358586, alpha = 0.0980455735164754\n",
      "Classifier 45/50: error = 0.466159722733256, alpha = 0.06778417995195171\n",
      "Classifier 46/50: error = 0.46642979878884383, alpha = 0.06724156207817218\n",
      "Classifier 47/50: error = 0.47386112272353553, alpha = 0.05232545718913135\n",
      "Classifier 48/50: error = 0.4190669470157724, alpha = 0.1633024188425175\n",
      "Classifier 49/50: error = 0.4633129139221106, alpha = 0.07350627566780907\n",
      "Classifier 50/50: error = 0.477645558256206, alpha = 0.04473870855146483\n",
      "Classifier 1/50: error = 0.09033333333333338, alpha = 1.154785849415778\n",
      "Classifier 2/50: error = 0.42088450673147715, alpha = 0.1595717313065343\n",
      "Classifier 3/50: error = 0.4403926915184798, alpha = 0.11978424661711393\n",
      "Classifier 4/50: error = 0.402917448599796, alpha = 0.1966618410126214\n",
      "Classifier 5/50: error = 0.41257232015359563, alpha = 0.17667081412150612\n",
      "Classifier 6/50: error = 0.3878513128740908, alpha = 0.2281765768969063\n",
      "Classifier 7/50: error = 0.44009923200891204, alpha = 0.12037966959252622\n",
      "Classifier 8/50: error = 0.4489601840356916, alpha = 0.10243643047479645\n",
      "Classifier 9/50: error = 0.431532220265475, alpha = 0.13780122874611161\n",
      "Classifier 10/50: error = 0.3721440756487828, alpha = 0.2615148203960775\n",
      "Classifier 11/50: error = 0.38984938605972186, alpha = 0.22397267987696967\n",
      "Classifier 12/50: error = 0.4016076681299136, alpha = 0.1993854749819163\n",
      "Classifier 13/50: error = 0.43700232399007755, alpha = 0.12666849353735699\n",
      "Classifier 14/50: error = 0.4462091832661377, alpha = 0.10799958271824736\n",
      "Classifier 15/50: error = 0.37990334785719904, alpha = 0.24497924185803388\n",
      "Classifier 16/50: error = 0.35620742585976956, alpha = 0.2959316830848228\n",
      "Classifier 17/50: error = 0.42089170674686516, alpha = 0.1595569615175405\n",
      "Classifier 18/50: error = 0.41410957569390394, alpha = 0.17350108419437493\n",
      "Classifier 19/50: error = 0.42287577477718985, alpha = 0.15548953951030503\n",
      "Classifier 20/50: error = 0.42315483545565313, alpha = 0.154917865710557\n",
      "Classifier 21/50: error = 0.4261635992809074, alpha = 0.14876051552419012\n",
      "Classifier 22/50: error = 0.44253673468352084, alpha = 0.11543656603572429\n",
      "Classifier 23/50: error = 0.4471339502935866, alpha = 0.10612876628338345\n",
      "Classifier 24/50: error = 0.4421187841480677, alpha = 0.11628373823331263\n",
      "Classifier 25/50: error = 0.400637186846833, alpha = 0.20140543297915822\n",
      "Classifier 26/50: error = 0.4033070052075999, alpha = 0.19585233150796894\n",
      "Classifier 27/50: error = 0.3969050911653328, alpha = 0.20918869582150526\n",
      "Classifier 28/50: error = 0.4234480228927918, alpha = 0.15431736052022402\n",
      "Classifier 29/50: error = 0.4458404297859039, alpha = 0.10874578542759936\n",
      "Classifier 30/50: error = 0.4227726132002817, alpha = 0.15570089816222324\n",
      "Classifier 31/50: error = 0.4476847217475365, alpha = 0.10501490043568996\n",
      "Classifier 32/50: error = 0.4654336688510653, alpha = 0.0692431150114208\n",
      "Classifier 33/50: error = 0.39694938297941895, alpha = 0.2090961806715182\n",
      "Classifier 34/50: error = 0.44634989541112713, alpha = 0.10771487185178842\n",
      "Classifier 35/50: error = 0.4422096727406303, alpha = 0.11609949590264547\n",
      "Classifier 36/50: error = 0.43690646513453035, alpha = 0.1268633086126551\n",
      "Classifier 37/50: error = 0.4505459587755134, alpha = 0.09923252187856224\n",
      "Classifier 38/50: error = 0.4602702633501301, alpha = 0.07962734028879101\n",
      "Classifier 39/50: error = 0.44514280244785565, alpha = 0.1101578219108776\n",
      "Classifier 40/50: error = 0.4825933970547962, alpha = 0.03482728018227262\n",
      "Classifier 41/50: error = 0.44522582041862624, alpha = 0.10998976609964098\n",
      "Classifier 42/50: error = 0.466746469827931, alpha = 0.06660537916023042\n",
      "Classifier 43/50: error = 0.4572451769553776, alpha = 0.08571897785840807\n",
      "Classifier 44/50: error = 0.4325945480399188, alpha = 0.13563661013975506\n",
      "Classifier 45/50: error = 0.45321445242854397, alpha = 0.09384562757910514\n",
      "Classifier 46/50: error = 0.3891136375044909, alpha = 0.22551976373326044\n",
      "Classifier 47/50: error = 0.47602504147946983, alpha = 0.04798671653955961\n",
      "Classifier 48/50: error = 0.4702233128526132, alpha = 0.059623928566901324\n",
      "Classifier 49/50: error = 0.4534137848552566, alpha = 0.09344345647753344\n",
      "Classifier 50/50: error = 0.46850223603856267, alpha = 0.0630790581428549\n",
      "Classifier 1/50: error = 0.09866666666666671, alpha = 1.1060639909681136\n",
      "Classifier 2/50: error = 0.4127991066003387, alpha = 0.17620297438137134\n",
      "Classifier 3/50: error = 0.30239951564372536, alpha = 0.41794876206420273\n",
      "Classifier 4/50: error = 0.3375607160977597, alpha = 0.3370917564468603\n",
      "Classifier 5/50: error = 0.41675763273877575, alpha = 0.16804899390479833\n",
      "Classifier 6/50: error = 0.2957650344404532, alpha = 0.4337733608221519\n",
      "Classifier 7/50: error = 0.2954048211654743, alpha = 0.43463836625216096\n",
      "Classifier 8/50: error = 0.4264301302323117, alpha = 0.14821561364785885\n",
      "Classifier 9/50: error = 0.3797659962462246, alpha = 0.2452707842266067\n",
      "Classifier 10/50: error = 0.39217668407488315, alpha = 0.21908588895544137\n",
      "Classifier 11/50: error = 0.21884659710693372, alpha = 0.6362002671193464\n",
      "Classifier 12/50: error = 0.35222423854692164, alpha = 0.3046382875472063\n",
      "Classifier 13/50: error = 0.39368835939077934, alpha = 0.21591724095986312\n",
      "Classifier 14/50: error = 0.4528138450618785, alpha = 0.09465398066519287\n",
      "Classifier 15/50: error = 0.34538805569164194, alpha = 0.3196870122335771\n",
      "Classifier 16/50: error = 0.41978747876032607, alpha = 0.16182293601401754\n",
      "Classifier 17/50: error = 0.4249598762108622, alpha = 0.15122253162612057\n",
      "Classifier 18/50: error = 0.34537361640325775, alpha = 0.31971894443374643\n",
      "Classifier 19/50: error = 0.3898565441863437, alpha = 0.22395763342739786\n",
      "Classifier 20/50: error = 0.4056882237111685, alpha = 0.19090955739367613\n",
      "Classifier 21/50: error = 0.4334462241976369, alpha = 0.133902132035499\n",
      "Classifier 22/50: error = 0.42778505063160166, alpha = 0.14544692753129823\n",
      "Classifier 23/50: error = 0.41615469256947923, alpha = 0.16928950742831064\n",
      "Classifier 24/50: error = 0.4432883298244903, alpha = 0.11391352116734746\n",
      "Classifier 25/50: error = 0.39390663328622083, alpha = 0.2154600673682839\n",
      "Classifier 26/50: error = 0.41697620042595274, alpha = 0.16759943059748236\n",
      "Classifier 27/50: error = 0.43576583381318146, alpha = 0.12918216644643937\n",
      "Classifier 28/50: error = 0.44021812733171184, alpha = 0.12013842335040832\n",
      "Classifier 29/50: error = 0.3007327999815626, alpha = 0.4219053832995655\n",
      "Classifier 30/50: error = 0.4262787120678898, alpha = 0.14852516561160178\n",
      "Classifier 31/50: error = 0.4622456557783986, alpha = 0.0756526871843114\n",
      "Classifier 32/50: error = 0.43327470737189544, alpha = 0.13425136930041193\n",
      "Classifier 33/50: error = 0.4427231635082604, alpha = 0.11505873411453318\n",
      "Classifier 34/50: error = 0.40388568929039936, alpha = 0.1946502772322562\n",
      "Classifier 35/50: error = 0.4676220452025002, alpha = 0.06484665225246809\n",
      "Classifier 36/50: error = 0.4227782606177359, alpha = 0.15568932731004692\n",
      "Classifier 37/50: error = 0.46489842797584524, alpha = 0.0703188179159891\n",
      "Classifier 38/50: error = 0.44797744347989765, alpha = 0.10442301344450608\n",
      "Classifier 39/50: error = 0.38276458454536555, alpha = 0.238915180573316\n",
      "Classifier 40/50: error = 0.42052804625315343, alpha = 0.16030304481613697\n",
      "Classifier 41/50: error = 0.464618615117001, alpha = 0.07088123766391409\n",
      "Classifier 42/50: error = 0.47356667642546557, alpha = 0.05291598191608563\n",
      "Classifier 43/50: error = 0.45981745412593655, alpha = 0.08053877922523528\n",
      "Classifier 44/50: error = 0.4296025132289968, alpha = 0.14173653480654055\n",
      "Classifier 45/50: error = 0.45670354804345337, alpha = 0.08681031696367926\n",
      "Classifier 46/50: error = 0.41462642338816835, alpha = 0.1724361528162704\n",
      "Classifier 47/50: error = 0.45506007628195855, alpha = 0.09012305541075227\n",
      "Classifier 48/50: error = 0.4514529190811074, alpha = 0.09740101050805047\n",
      "Classifier 49/50: error = 0.4703182880300385, alpha = 0.059433304297293675\n",
      "Classifier 50/50: error = 0.4567780654181848, alpha = 0.08666015821094634\n",
      "Classifier 1/50: error = 0.1043666666666667, alpha = 1.0748103812589038\n",
      "Classifier 2/50: error = 0.42910467605353975, alpha = 0.14275249145954816\n",
      "Classifier 3/50: error = 0.3591142138608435, alpha = 0.2896053889108918\n",
      "Classifier 4/50: error = 0.3657372933151766, alpha = 0.2752739672522606\n",
      "Classifier 5/50: error = 0.3642308976704378, alpha = 0.2785237249312946\n",
      "Classifier 6/50: error = 0.47071832630566607, alpha = 0.05863043638728726\n",
      "Classifier 7/50: error = 0.4013689973301516, alpha = 0.19988209446386007\n",
      "Classifier 8/50: error = 0.46091388628685703, alpha = 0.07833204795625803\n",
      "Classifier 9/50: error = 0.3675995418476806, alpha = 0.27126434277013217\n",
      "Classifier 10/50: error = 0.4545936295297598, alpha = 0.091063626351397\n",
      "Classifier 11/50: error = 0.3722919081547048, alpha = 0.26119849524756417\n",
      "Classifier 12/50: error = 0.3932352752795991, alpha = 0.21686650741590502\n",
      "Classifier 13/50: error = 0.4128796812415284, alpha = 0.17603677463999198\n",
      "Classifier 14/50: error = 0.2006582849239995, alpha = 0.6910925730605509\n",
      "Classifier 15/50: error = 0.41520440470248376, alpha = 0.17124571796631297\n",
      "Classifier 16/50: error = 0.4268224445365908, alpha = 0.1474137162900184\n",
      "Classifier 17/50: error = 0.3625588278983172, alpha = 0.2821376243069638\n",
      "Classifier 18/50: error = 0.4041496336230549, alpha = 0.19410219134789444\n",
      "Classifier 19/50: error = 0.3952418068014525, alpha = 0.21266547533319005\n",
      "Classifier 20/50: error = 0.46792130772111373, alpha = 0.06424563015504214\n",
      "Classifier 21/50: error = 0.3190118959341457, alpha = 0.3791582220395437\n",
      "Classifier 22/50: error = 0.40772002617108793, alpha = 0.18669936164399292\n",
      "Classifier 23/50: error = 0.3846703351174097, alpha = 0.2348857347476835\n",
      "Classifier 24/50: error = 0.4445765023891863, alpha = 0.11130436623572879\n",
      "Classifier 25/50: error = 0.4262338587007237, alpha = 0.14861686708613844\n",
      "Classifier 26/50: error = 0.4497270666305434, alpha = 0.10088675871473095\n",
      "Classifier 27/50: error = 0.4339687870845145, alpha = 0.13283830538435293\n",
      "Classifier 28/50: error = 0.4259088150205992, alpha = 0.14928148419061063\n",
      "Classifier 29/50: error = 0.404992127177674, alpha = 0.19235350458163697\n",
      "Classifier 30/50: error = 0.369781065948253, alpha = 0.26657807820962787\n",
      "Classifier 31/50: error = 0.41919396446973667, alpha = 0.16304155998732245\n",
      "Classifier 32/50: error = 0.44809189888456225, alpha = 0.10419160303575661\n",
      "Classifier 33/50: error = 0.4551349194914127, alpha = 0.08997215196452726\n",
      "Classifier 34/50: error = 0.42044392041696105, alpha = 0.16047566196194737\n",
      "Classifier 35/50: error = 0.39784479359621144, alpha = 0.2072266332756861\n",
      "Classifier 36/50: error = 0.4224196238376534, alpha = 0.15642421144388954\n",
      "Classifier 37/50: error = 0.4364973823710959, alpha = 0.12769480015939766\n",
      "Classifier 38/50: error = 0.4230341380386282, alpha = 0.1551651098050937\n",
      "Classifier 39/50: error = 0.4188523062224305, alpha = 0.1637432819856545\n",
      "Classifier 40/50: error = 0.45686167438828534, alpha = 0.08649168377208201\n",
      "Classifier 41/50: error = 0.4353582162602034, alpha = 0.13001117053889316\n",
      "Classifier 42/50: error = 0.45047667654523865, alpha = 0.09937245721849336\n",
      "Classifier 43/50: error = 0.4446506767278787, alpha = 0.11115417461409245\n",
      "Classifier 44/50: error = 0.3627062713643188, alpha = 0.2818186623582968\n",
      "Classifier 45/50: error = 0.3680082034634019, alpha = 0.2703855919224596\n",
      "Classifier 46/50: error = 0.441101378753758, alpha = 0.11834668324792631\n",
      "Classifier 47/50: error = 0.4423615351579129, alpha = 0.11579166964834024\n",
      "Classifier 48/50: error = 0.447080127115404, alpha = 0.10623763090697963\n",
      "Classifier 49/50: error = 0.45001005120241555, alpha = 0.10031504229268068\n",
      "Classifier 50/50: error = 0.46848400719065675, alpha = 0.06311566117956915\n",
      "Classifier 1/50: error = 0.09754999999999969, alpha = 1.1123741095134174\n",
      "Classifier 2/50: error = 0.3878355489948723, alpha = 0.2282097750672233\n",
      "Classifier 3/50: error = 0.43585002399795414, alpha = 0.12901096423815953\n",
      "Classifier 4/50: error = 0.3980083474117433, alpha = 0.20688530026712332\n",
      "Classifier 5/50: error = 0.36128186585538624, alpha = 0.2849024034001733\n",
      "Classifier 6/50: error = 0.33578962153086955, alpha = 0.34105704836171985\n",
      "Classifier 7/50: error = 0.4353536112835057, alpha = 0.13002053705769048\n",
      "Classifier 8/50: error = 0.4367338291374907, alpha = 0.1272141829419251\n",
      "Classifier 9/50: error = 0.37127517342694905, alpha = 0.26337509410692195\n",
      "Classifier 10/50: error = 0.4055711427265914, alpha = 0.19115236905019062\n",
      "Classifier 11/50: error = 0.41034057256444667, alpha = 0.18127882442214055\n",
      "Classifier 12/50: error = 0.35731429713338847, alpha = 0.29352001399326044\n",
      "Classifier 13/50: error = 0.40062029092508034, alpha = 0.20144061444079392\n",
      "Classifier 14/50: error = 0.4549749311997159, alpha = 0.09029473509950747\n",
      "Classifier 15/50: error = 0.43821291578887006, alpha = 0.12420901148637246\n",
      "Classifier 16/50: error = 0.42094462437713154, alpha = 0.15944841077920344\n",
      "Classifier 17/50: error = 0.42224268493672645, alpha = 0.15678683938900698\n",
      "Classifier 18/50: error = 0.39298772164730944, alpha = 0.21738532497017907\n",
      "Classifier 19/50: error = 0.4258240968209494, alpha = 0.14945472905459467\n",
      "Classifier 20/50: error = 0.461083209694804, alpha = 0.077991328042785\n",
      "Classifier 21/50: error = 0.46191297576331425, alpha = 0.076321896473328\n",
      "Classifier 22/50: error = 0.4612630529867962, alpha = 0.07762945934941289\n",
      "Classifier 23/50: error = 0.3995873802374672, alpha = 0.20359232654562756\n",
      "Classifier 24/50: error = 0.44292155240882775, alpha = 0.11465669883997501\n",
      "Classifier 25/50: error = 0.376753871201183, alpha = 0.2516746991300189\n",
      "Classifier 26/50: error = 0.4014459129019661, alpha = 0.19972204009228386\n",
      "Classifier 27/50: error = 0.4601317478026319, alpha = 0.0799061378019478\n",
      "Classifier 28/50: error = 0.3945295124361643, alpha = 0.21415593695015525\n",
      "Classifier 29/50: error = 0.3836650083033759, alpha = 0.23701041783562912\n",
      "Classifier 30/50: error = 0.4300289075502949, alpha = 0.14086660559789974\n",
      "Classifier 31/50: error = 0.42985718648353444, alpha = 0.14121692517632814\n",
      "Classifier 32/50: error = 0.4166882868387224, alpha = 0.1681916428234663\n",
      "Classifier 33/50: error = 0.4340597401377717, alpha = 0.13265317495889742\n",
      "Classifier 34/50: error = 0.4669966989201402, alpha = 0.06610271428266638\n",
      "Classifier 35/50: error = 0.4505199006612005, alpha = 0.09928515326127797\n",
      "Classifier 36/50: error = 0.465476985110442, alpha = 0.0691560669829905\n",
      "Classifier 37/50: error = 0.44133425829678363, alpha = 0.11787439649858932\n",
      "Classifier 38/50: error = 0.4391505108071721, alpha = 0.12230518786664345\n",
      "Classifier 39/50: error = 0.4183766614875125, alpha = 0.1647204613260689\n",
      "Classifier 40/50: error = 0.464262504498998, alpha = 0.07159707958402504\n",
      "Classifier 41/50: error = 0.4352744321920804, alpha = 0.13018159081420075\n",
      "Classifier 42/50: error = 0.43757806639039276, alpha = 0.1254986061794877\n",
      "Classifier 43/50: error = 0.4450509759102734, alpha = 0.11034371639184042\n",
      "Classifier 44/50: error = 0.46195982837679883, alpha = 0.0762276450261878\n",
      "Classifier 45/50: error = 0.454150695264809, alpha = 0.09195693404912708\n",
      "Classifier 46/50: error = 0.4589145360041199, alpha = 0.08235662189836238\n",
      "Classifier 47/50: error = 0.4550665964134907, alpha = 0.09010990896098976\n",
      "Classifier 48/50: error = 0.4266080199657911, alpha = 0.14785198050628115\n",
      "Classifier 49/50: error = 0.44360003078166677, alpha = 0.11328203990693683\n",
      "Classifier 50/50: error = 0.4482304612423177, alpha = 0.10391146712265174\n",
      "Classifier 1/50: error = 0.09918333333333307, alpha = 1.1031658844832626\n",
      "Classifier 2/50: error = 0.4086898714429691, alpha = 0.1846920120102757\n",
      "Classifier 3/50: error = 0.4329135835180432, alpha = 0.1349867850836224\n",
      "Classifier 4/50: error = 0.3369636190195476, alpha = 0.33842744635748584\n",
      "Classifier 5/50: error = 0.35159716798970087, alpha = 0.306013023135084\n",
      "Classifier 6/50: error = 0.43380657075406387, alpha = 0.1331685111532994\n",
      "Classifier 7/50: error = 0.4379898747872396, alpha = 0.12466203644434572\n",
      "Classifier 8/50: error = 0.2311942091417446, alpha = 0.6007901500696597\n",
      "Classifier 9/50: error = 0.4223539174206208, alpha = 0.156558868883277\n",
      "Classifier 10/50: error = 0.45303889777244843, alpha = 0.09419984995508889\n",
      "Classifier 11/50: error = 0.4226787430825448, alpha = 0.15589323235495464\n",
      "Classifier 12/50: error = 0.4124988490976249, alpha = 0.17682239461123464\n",
      "Classifier 13/50: error = 0.4305408552808889, alpha = 0.1398224100510372\n",
      "Classifier 14/50: error = 0.44545213332848177, alpha = 0.10953166536806197\n",
      "Classifier 15/50: error = 0.44868119343173996, alpha = 0.10300031972771688\n",
      "Classifier 16/50: error = 0.4428628436951433, alpha = 0.11477566824767754\n",
      "Classifier 17/50: error = 0.47144303569564927, alpha = 0.05717615232140973\n",
      "Classifier 18/50: error = 0.4455497029497551, alpha = 0.10933417987885577\n",
      "Classifier 19/50: error = 0.38337694670607836, alpha = 0.23761960109150085\n",
      "Classifier 20/50: error = 0.41037379338320956, alpha = 0.1812101762016049\n",
      "Classifier 21/50: error = 0.46074655834985034, alpha = 0.0786687703275526\n",
      "Classifier 22/50: error = 0.4234892461404558, alpha = 0.1542329361129964\n",
      "Classifier 23/50: error = 0.4249329371984099, alpha = 0.1512776516240526\n",
      "Classifier 24/50: error = 0.4573963489835119, alpha = 0.0854144147318289\n",
      "Classifier 25/50: error = 0.43766402719682296, alpha = 0.1253239663788793\n",
      "Classifier 26/50: error = 0.41289754677022306, alpha = 0.17599992506633472\n",
      "Classifier 27/50: error = 0.4284020754632768, alpha = 0.14418681588591084\n",
      "Classifier 28/50: error = 0.4458802658745092, alpha = 0.1086651680594649\n",
      "Classifier 29/50: error = 0.4543840338061811, alpha = 0.09148631984828877\n",
      "Classifier 30/50: error = 0.4861568214724963, alpha = 0.02769343448620326\n",
      "Classifier 31/50: error = 0.4478049737804938, alpha = 0.10477174047736232\n",
      "Classifier 32/50: error = 0.4066087574615669, alpha = 0.18900125516360555\n",
      "Classifier 33/50: error = 0.4513619977836639, alpha = 0.0975845869721852\n",
      "Classifier 34/50: error = 0.4560145134178566, alpha = 0.088198965354505\n",
      "Classifier 35/50: error = 0.4376859062794729, alpha = 0.1252795175842522\n",
      "Classifier 36/50: error = 0.440527624385204, alpha = 0.11951049915078525\n",
      "Classifier 37/50: error = 0.4462372010631874, alpha = 0.10794289133063016\n",
      "Classifier 38/50: error = 0.4613608773120724, alpha = 0.07743263228355964\n",
      "Classifier 39/50: error = 0.4747527264629692, alpha = 0.05053752813992372\n",
      "Classifier 40/50: error = 0.47299964905403746, alpha = 0.05405328395853727\n",
      "Classifier 41/50: error = 0.44142791105197565, alpha = 0.11768448064959079\n",
      "Classifier 42/50: error = 0.4160666911253529, alpha = 0.16947060824418084\n",
      "Classifier 43/50: error = 0.4368414285621913, alpha = 0.12699548865374027\n",
      "Classifier 44/50: error = 0.4617237489477256, alpha = 0.07670256994928158\n",
      "Classifier 45/50: error = 0.448082314280721, alpha = 0.10421098113535791\n",
      "Classifier 46/50: error = 0.39501689319320055, alpha = 0.21313600167770516\n",
      "Classifier 47/50: error = 0.4400482951840795, alpha = 0.12048302794523326\n",
      "Classifier 48/50: error = 0.47092358045615057, alpha = 0.058218525248345296\n",
      "Classifier 49/50: error = 0.48117096523129144, alpha = 0.03767588604059046\n",
      "Classifier 50/50: error = 0.47219464799418054, alpha = 0.055668136908361844\n",
      "Multiclass Accuracy: 0.8329\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostMulticlass: # Creamos la clase AdaBoostMulticlass\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.models = [] # Inicializamos los modelos\n",
    "\n",
    "    def fit(self, X, y, verbose=False): # Creamos la función fit\n",
    "        for digit in range(10): # Para cada dígito\n",
    "            y_binary = np.where(y == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "            model = AdaBoost(T=self.T, A=self.A) # Creamos el clasificador AdaBoost\n",
    "            model.fit(X, y_binary, verbose) # Ajustamos el clasificador AdaBoost\n",
    "            self.models.append(model) # Añadimos el clasificador AdaBoost\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        model_preds = np.array([model.predict(X) for model in self.models]) # Realizamos las predicciones\n",
    "        return np.argmax(model_preds, axis=0) # Devolvemos el índice del valor máximo\n",
    "\n",
    "def run_adaboost_multiclass_on_mnist(T=5, A=20, verboseParam=False, n_components=50): # Creamos la función run_adaboost_multiclass_on_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Redimensionamos los datos\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Redimensionamos los datos\n",
    "\n",
    "    # Apply PCA\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components)\n",
    "\n",
    "    adaboost_multiclass = AdaBoostMulticlass(T=T, A=A) # Creamos el clasificador AdaBoostMulticlass\n",
    "    adaboost_multiclass.fit(X_train_reduced, y_train, verboseParam) # Ajustamos el clasificador AdaBoostMulticlass\n",
    "    y_pred = adaboost_multiclass.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test == y_pred) / len(y_test) # Calculamos la precisión\n",
    "    print(f\"Multiclass Accuracy: {accuracy}\") # Mostramos la precisión\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "\n",
    "accuracy = run_adaboost_multiclass_on_mnist(T=50, A=20, verboseParam=True) # Ejecutamos AdaBoostMulticlass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclase con ADABoosti Binario con Mejoras (Version 1: Añadidos los parámetros n_componentes y k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada iteración\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "def select_best_features(X_train, y_train, X_test, k=200): # Creamos la función select_best_features\n",
    "    selector = SelectKBest(f_classif, k=k) # Creamos el objeto SelectKBest\n",
    "    X_train_best = selector.fit_transform(X_train, y_train) # Seleccionamos las mejores características de los datos de entrenamiento\n",
    "    X_test_best = selector.transform(X_test) # Seleccionamos las mejores características de los datos de prueba\n",
    "    return X_train_best, X_test_best # Devolvemos los datos con las mejores características\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False, n_components=50, k=200): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Redimensionamos los datos para reducir la dimensión de las imagénes a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Redimensionamos los datos para reducir la dimensión de las imagénes a 1D\n",
    "\n",
    "    X_train_best, X_test_best = select_best_features(X_train, y_train, X_test, k=k) # Seleccionamos las mejores características\n",
    "\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train_best, X_test_best, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A) # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_reduced, y_train_binary, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Classifier 1/5: error = 0.09578333333333311, alpha = 1.1224901549858723\n",
      "Classifier 2/5: error = 0.423811546699126, alpha = 0.15357294594849832\n",
      "Classifier 3/5: error = 0.44594286075021583, alpha = 0.10853849596112296\n",
      "Classifier 4/5: error = 0.43261482746443425, alpha = 0.13559530076075985\n",
      "Classifier 5/5: error = 0.47357464282143635, alpha = 0.05290000448255655\n",
      "Accuracy for digit 3: 0.9003\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=5, A=20, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 0\n",
      "Classifier 1/5: error = 0.06493333333333336, alpha = 1.3336283620106117\n",
      "Classifier 2/5: error = 0.40729086733956765, alpha = 0.18758809260424628\n",
      "Classifier 3/5: error = 0.45367364809500965, alpha = 0.09291920425006518\n",
      "Classifier 4/5: error = 0.36758716202838615, alpha = 0.2712909696472824\n",
      "Classifier 5/5: error = 0.45306292554869954, alpha = 0.09415136693421174\n",
      "Accuracy for digit 0: 0.934\n",
      "Running AdaBoost for digit: 1\n",
      "Classifier 1/5: error = 0.11243333333333305, alpha = 1.033061590861527\n",
      "Classifier 2/5: error = 0.4453157110817808, alpha = 0.10980780466445449\n",
      "Classifier 3/5: error = 0.1714455120615499, alpha = 0.7877085495093323\n",
      "Classifier 4/5: error = 0.2813040656471276, alpha = 0.46900109941991674\n",
      "Classifier 5/5: error = 0.4102413735063475, alpha = 0.18148382156901965\n",
      "Accuracy for digit 1: 0.8865\n",
      "Running AdaBoost for digit: 2\n",
      "Classifier 1/5: error = 0.09931666666666639, alpha = 1.1024201675356724\n",
      "Classifier 2/5: error = 0.40659765491248473, alpha = 0.18902426304933448\n",
      "Classifier 3/5: error = 0.37921790125569543, alpha = 0.24643457767282734\n",
      "Classifier 4/5: error = 0.3869060627791908, alpha = 0.23016811708369078\n",
      "Classifier 5/5: error = 0.43002896293659254, alpha = 0.14086649261262346\n",
      "Accuracy for digit 2: 0.8965\n",
      "Running AdaBoost for digit: 3\n",
      "Classifier 1/5: error = 0.10219999999999974, alpha = 1.08650782385557\n",
      "Classifier 2/5: error = 0.3392567786858214, alpha = 0.33330400715356173\n",
      "Classifier 3/5: error = 0.4018037495315872, alpha = 0.19897754694905645\n",
      "Classifier 4/5: error = 0.45816307580283977, alpha = 0.08386994841179553\n",
      "Classifier 5/5: error = 0.4404138790016704, alpha = 0.11974126095019975\n",
      "Accuracy for digit 3: 0.899\n",
      "Running AdaBoost for digit: 4\n",
      "Classifier 1/5: error = 0.09738333333333335, alpha = 1.1133214355743424\n",
      "Classifier 2/5: error = 0.4223246683662537, alpha = 0.15661881313152382\n",
      "Classifier 3/5: error = 0.23469215245170938, alpha = 0.5910017509445644\n",
      "Classifier 4/5: error = 0.42255997075733776, alpha = 0.15613660606383142\n",
      "Classifier 5/5: error = 0.4685230618983178, alpha = 0.06303724058271912\n",
      "Accuracy for digit 4: 0.9018\n",
      "Running AdaBoost for digit: 5\n",
      "Classifier 1/5: error = 0.09033333333333338, alpha = 1.154785849415778\n",
      "Classifier 2/5: error = 0.40890621963185436, alpha = 0.18424442383523368\n",
      "Classifier 3/5: error = 0.43706179582771343, alpha = 0.12654763303298114\n",
      "Classifier 4/5: error = 0.44099009882411255, alpha = 0.11857238085094904\n",
      "Classifier 5/5: error = 0.3699171721141806, alpha = 0.26628608101979395\n",
      "Accuracy for digit 5: 0.9108\n",
      "Running AdaBoost for digit: 6\n",
      "Classifier 1/5: error = 0.09865000000000004, alpha = 1.106157703038341\n",
      "Classifier 2/5: error = 0.44660665243817943, alpha = 0.10719540558518503\n",
      "Classifier 3/5: error = 0.4046580016358953, alpha = 0.1930468787727354\n",
      "Classifier 4/5: error = 0.3547516543645769, alpha = 0.299108647530328\n",
      "Classifier 5/5: error = 0.3839636209743459, alpha = 0.23637910374876583\n",
      "Accuracy for digit 6: 0.9042\n",
      "Running AdaBoost for digit: 7\n",
      "Classifier 1/5: error = 0.10443333333333304, alpha = 1.0744538774497296\n",
      "Classifier 2/5: error = 0.37481838141768886, alpha = 0.25580030235815276\n",
      "Classifier 3/5: error = 0.4424931577455386, alpha = 0.11552488735401109\n",
      "Classifier 4/5: error = 0.3778648765692736, alpha = 0.24931032348281731\n",
      "Classifier 5/5: error = 0.4434659988972084, alpha = 0.11355356675307499\n",
      "Accuracy for digit 7: 0.8972\n",
      "Running AdaBoost for digit: 8\n",
      "Classifier 1/5: error = 0.09753333333333303, alpha = 1.1124687771255912\n",
      "Classifier 2/5: error = 0.46229559280251675, alpha = 0.0755522411942153\n",
      "Classifier 3/5: error = 0.3548757528680084, alpha = 0.29883759651876357\n",
      "Classifier 4/5: error = 0.396799965914886, alpha = 0.2094082918019669\n",
      "Classifier 5/5: error = 0.43698193336451585, alpha = 0.1267099328433053\n",
      "Accuracy for digit 8: 0.9026\n",
      "Running AdaBoost for digit: 9\n",
      "Classifier 1/5: error = 0.09919999999999973, alpha = 1.103072621098775\n",
      "Classifier 2/5: error = 0.4691282481951643, alpha = 0.061822144323068706\n",
      "Classifier 3/5: error = 0.4574559232965292, alpha = 0.08529439595326087\n",
      "Classifier 4/5: error = 0.40427478418970286, alpha = 0.19384235386469156\n",
      "Classifier 5/5: error = 0.46933335255988473, alpha = 0.06141037623255637\n",
      "Accuracy for digit 9: 0.8991\n",
      "Accuracies for all digits: {0: 0.934, 1: 0.8865, 2: 0.8965, 3: 0.899, 4: 0.9018, 5: 0.9108, 6: 0.9042, 7: 0.8972, 8: 0.9026, 9: 0.8991}\n"
     ]
    }
   ],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=5, A=20, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  0   1   2   3   4   5   6   7   8   9  10  11  16  17  18  19  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  52  53  54  55  56  57  82  83\n",
      "  84  85 111 112 140 141 168 476 560 644 645 671 672 673 699 700 701 727\n",
      " 728 729 730 754 755 756 757 758 759 780 781 782 783] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Accuracy: 0.8016\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostMulticlass: # Creamos la clase AdaBoostMulticlass\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A# Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.models = [] # Inicializamos los modelos\n",
    "\n",
    "    def fit(self, X, y, verbose=False): # Creamos la función fit\n",
    "        for digit in range(10): # Para cada dígito\n",
    "            y_binary = np.where(y == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "            model = AdaBoost(T=self.T, A=self.A) # Creamos el clasificador AdaBoost\n",
    "            model.fit(X, y_binary, verbose) # Ajustamos el clasificador AdaBoost\n",
    "            self.models.append(model) # Añadimos el clasificador AdaBoost\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        model_preds = np.array([model.predict(X) for model in self.models]) # Realizamos las predicciones\n",
    "        return np.argmax(model_preds, axis=0) # Devolvemos el índice del valor máximo\n",
    "\n",
    "def run_adaboost_multiclass_on_mnist(T=5, A=20, verboseParam=False, n_components=50, k=200): # Creamos la función run_adaboost_multiclass_on_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Redimensionamos los datos\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Redimensionamos los datos\n",
    "\n",
    "    X_train_best, X_test_best = select_best_features(X_train, y_train, X_test, k=k) # Seleccionamos las mejores características\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train_best, X_test_best, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    adaboost_multiclass = AdaBoostMulticlass(T=T, A=A) # Creamos el clasificador AdaBoostMulticlass\n",
    "    adaboost_multiclass.fit(X_train_reduced, y_train, verboseParam) # Ajustamos el clasificador AdaBoostMulticlass\n",
    "    y_pred = adaboost_multiclass.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test == y_pred) / len(y_test) # Calculamos la precisión\n",
    "    print(f\"Multiclass Accuracy: {accuracy}\") # Mostramos la precisión\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "\n",
    "accuracy = run_adaboost_multiclass_on_mnist(T=50, A=20, verboseParam=False) # Ejecutamos AdaBoostMulticlass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 1E: Implementación de una versión de AdaboostBinario.fit que pare automáticamente el entrenamiento \n",
    "cuando detecte sobreentrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, verbose=False): # Creamos la función fit\n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        for t in range(self.T): # Para cada clasificador débil\n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada pixel\n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {t + 1}/{self.T}: error = {min_error}, alpha = {best_clf.alpha}') # Mostramos el error y el alpha\n",
    "\n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "    \n",
    "    def compute_optimal_number_of_weak_classificators(self, X_verification, y_verification, X_test, y_test, \n",
    "                                                      iterNumber = 100, numberOfTries = 10):\n",
    "        bestAccuracy = 0\n",
    "        newAccuracy = 0\n",
    "        optimalNumberOfWeakClassificators = 5\n",
    "        \n",
    "        while round(newAccuracy, 6) >= round(bestAccuracy, 6):\n",
    "            optimalNumberOfWeakClassificators += 1\n",
    "            bestAccuracy = newAccuracy\n",
    "            newAccuracy = 0\n",
    "            for i in range(iterNumber):\n",
    "                adaboost = AdaBoost(T=optimalNumberOfWeakClassificators, A=numberOfTries)\n",
    "                adaboost.fit(X_verification, y_verification)\n",
    "                y_pred = adaboost.predict(X_test)\n",
    "                partialAccuracy = np.sum(y_test == np.sign(y_pred)) / len(y_test)\n",
    "                newAccuracy += partialAccuracy\n",
    "            newAccuracy /= iterNumber\n",
    "        self.T = optimalNumberOfWeakClassificators\n",
    "        #return optimalNumberOfWeakClassificators\n",
    "\n",
    "    \n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False, n_components=50): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Aplanamos los datos de prueba reduciendo la dimensión a 1D   \n",
    "\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) \n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A)  # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_reduced, y_train_binary, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "\n",
    "\n",
    "def run_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    digit, A=20, verboseParam=False, n_components=50, \n",
    "    split_proportion = 0.90, iterNumber = 100, numberOfTries = 10):\n",
    "    \n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "    num_samples = len(X_train)\n",
    "    split_point = int(split_proportion * num_samples)\n",
    "    \n",
    "    X_True_Train = X_train[:split_point]\n",
    "    y_True_Train = y_train[:split_point]\n",
    "\n",
    "    X_Verification = X_train[split_point:]\n",
    "    y_verification = y_train[split_point:]\n",
    "\n",
    "    X_True_Train = X_True_Train.reshape(X_True_Train.shape[0], -1) \n",
    "    X_Verification = X_Verification.reshape(X_Verification.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Aplanamos los datos de prueba reduciendo la dimensión a 1D   \n",
    "\n",
    "    X_true_train_reduced, X_test_train_reduced = apply_pca(X_True_Train, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "    X_verification_reduced, X_test_verification_reduced = apply_pca(X_Verification, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "    \n",
    "    y_true_train_binary = np.where(y_True_Train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_verification_binary = np.where(y_verification == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) \n",
    "    \n",
    "    adaboost = AdaBoost(T=9, A=A)  # Creamos el clasificador AdaBoost\n",
    "    \n",
    "    adaboost.compute_optimal_number_of_weak_classificators(\n",
    "        X_verification_reduced, y_verification_binary, X_test_verification_reduced, y_test_binary,\n",
    "        iterNumber = iterNumber, numberOfTries = numberOfTries)\n",
    "    \n",
    "    print(\"The optimal number of weak classificators is: \", adaboost.T)\n",
    "    \n",
    "    adaboost.fit(X_true_train_reduced, y_true_train_binary, verboseParam)\n",
    "    y_pred = adaboost.predict(X_test_train_reduced)\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\")\n",
    "    \n",
    "    return accuracy \n",
    "\n",
    "def run_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    A=20, verboseParam=False, n_components=50, \n",
    "    split_proportion = 0.90, iterNumber = 100, numberOfTries = 10):\n",
    "    \n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators(\n",
    "            digit, A, verboseParam, n_components, split_proportion, iterNumber, numberOfTries)\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Classifier 1/5: error = 0.10220000000000003, alpha = 1.0865078238555685\n",
      "Classifier 2/5: error = 0.45907977648290865, alpha = 0.08202390342839501\n",
      "Classifier 3/5: error = 0.45479312565178, alpha = 0.09066133095088648\n",
      "Classifier 4/5: error = 0.3101342671000804, alpha = 0.3997458325454976\n",
      "Classifier 5/5: error = 0.3633968464888695, alpha = 0.28032549513346844\n",
      "Accuracy for digit 3: 0.899\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_on_mnist(digit=3, T=5, A=20, verboseParam=True)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies = run_adaboost_for_all_digits(T=5, A=20, verboseParam=True) # Ejecutamos AdaBoost para todos los dígitos\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "The optimal number of weak classificators is:  15\n",
      "Classifier 1/15: error = 0.10196296296296303, alpha = 1.0878008356802709\n",
      "Classifier 2/15: error = 0.35156308685440363, alpha = 0.30608777179690444\n",
      "Classifier 3/15: error = 0.3865630023917194, alpha = 0.23089135158595958\n",
      "Classifier 4/15: error = 0.322423112132842, alpha = 0.371329168979437\n",
      "Classifier 5/15: error = 0.2811924846207723, alpha = 0.46927708830550346\n",
      "Classifier 6/15: error = 0.36809370912709805, alpha = 0.2702017794924095\n",
      "Classifier 7/15: error = 0.36512499866876647, alpha = 0.27659418701997335\n",
      "Classifier 8/15: error = 0.38244125439156956, alpha = 0.2395995701423351\n",
      "Classifier 9/15: error = 0.33262790576124285, alpha = 0.34816164376862696\n",
      "Classifier 10/15: error = 0.4230248133871487, alpha = 0.15518421178380767\n",
      "Classifier 11/15: error = 0.4514260877821634, alpha = 0.09745518409763725\n",
      "Classifier 12/15: error = 0.4197701413615239, alpha = 0.1618585269857741\n",
      "Classifier 13/15: error = 0.4522654354626957, alpha = 0.09576077210610702\n",
      "Classifier 14/15: error = 0.45408758625582846, alpha = 0.0920842238746784\n",
      "Classifier 15/15: error = 0.46422417396328847, alpha = 0.07167413472742619\n",
      "Accuracy for digit 3: 0.9223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = run_adaboost_for_one_digit_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    digit=3, A=20, verboseParam=True, \n",
    "    n_components=50, split_proportion=0.90,\n",
    "    iterNumber = 100, numberOfTries=10)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_accuracies \u001b[38;5;241m=\u001b[39m  \u001b[43mrun_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators\u001b[49m(\n\u001b[0;32m      2\u001b[0m     A\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verboseParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      3\u001b[0m     n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, split_proportion\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m,\n\u001b[0;32m      4\u001b[0m     iterNumber \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, numberOfTries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracies for all digits:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_accuracies) \u001b[38;5;66;03m# Imprimimos las precisiones\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators' is not defined"
     ]
    }
   ],
   "source": [
    "all_accuracies =  run_adaboost_for_all_digits_with_computation_of_optimal_number_of_weak_classificators(\n",
    "    A=100, verboseParam=True, \n",
    "    n_components=50, split_proportion=0.75,\n",
    "    iterNumber = 500, numberOfTries=20)\n",
    "print(\"Accuracies for all digits:\", all_accuracies) # Imprimimos las precisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSIÓN BUENA BUENÍSIMA TAREA 1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # la libreería numpy sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # importamos el dataset MNIST\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis\n",
    "\n",
    "class DecisionStump: # Creamos la clase DecisionStump\n",
    "    def __init__(self, n_features): # Inicializamos la clase\n",
    "        self.feature_index = np.random.randint(0, n_features) # Elegimos un índice de característica aleatorio\n",
    "        self.threshold = None # Inicializamos el umbral\n",
    "        self.polarity = 1 # Inicializamos la polaridad\n",
    "        self.n_features = n_features # Inicializamos el número de características\n",
    "        self.alpha = None # Inicializamos el alpha\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        n_samples = X.shape[0] # Obtenemos el número de muestras\n",
    "        X_column = X[:, self.feature_index] # Obtenemos la columna de la característica\n",
    "        predictions = np.ones(n_samples) # Inicializamos las predicciones\n",
    "        if self.polarity == 1: # Si la polaridad es 1\n",
    "            predictions[X_column < self.threshold] = -1 # Si la columna de la característica es menor que el umbral, la predicción es -1\n",
    "        else: # Si la polaridad no es 1\n",
    "            predictions[X_column >= self.threshold] = -1 # Si la columna de la característica es mayor o igual que el umbral, la predicción es -1\n",
    "        return predictions # Devolvemos las predicciones\n",
    "\n",
    "class AdaBoost: # Creamos la clase AdaBoost\n",
    "    def __init__(self, T=5, A=20): # Inicializamos la clase\n",
    "        self.T = T # Inicializamos el número de clasificadores débiles\n",
    "        self.A = A # Inicializamos el número de píxeles máximos a probar por clasificador débil\n",
    "        self.clfs = [] # Inicializamos los clasificadores débiles\n",
    "\n",
    "    def fit(self, X, Y, X_Verification, Y_Verification, iter_number = 100, \n",
    "            verbose=False, round1 = 3, round2 = 3, \n",
    "            bestAccuracyBreak = 0.999 ,practicalAccuracyBreak=666): # Creamos la función fit\n",
    "        bestAccuracy = 0\n",
    "        newAccuracy = 0\n",
    "        practicalRepeatAccuracy = 0\n",
    "        t = 0\n",
    "        \n",
    "        n_samples, n_features = X.shape # Obtenemos el número de muestras y el número de características\n",
    "        w = np.full(n_samples, (1 / n_samples)) # Inicializamos los pesos\n",
    "\n",
    "        while (True):\n",
    "            if newAccuracy >= bestAccuracyBreak:\n",
    "                break\n",
    "            \n",
    "            if practicalRepeatAccuracy == practicalAccuracyBreak:\n",
    "                break\n",
    "            \n",
    "            if round(newAccuracy, round1) == round(bestAccuracy, round1):\n",
    "                practicalRepeatAccuracy += 1\n",
    "            else:\n",
    "                practicalRepeatAccuracy = 0\n",
    "                \n",
    "            bestAccuracy = newAccuracy\n",
    "            newAccuracy = 0\n",
    "            t += 1\n",
    "            \n",
    "            min_error = float('inf') # Inicializamos el error mínimo\n",
    "            best_clf = None # Inicializamos el mejor clasificador débil\n",
    "\n",
    "            for _ in range(self.A): # Para cada clasificador débil\n",
    "                clf = DecisionStump(n_features) # Creamos un clasificador débil\n",
    "                clf.threshold = np.random.uniform(min(X[:, clf.feature_index]), max(X[:, clf.feature_index])) # Elegimos un umbral aleatorio\n",
    "                clf.polarity = 1 # Elegimos una polaridad\n",
    "                predictions = clf.predict(X) # Realizamos las predicciones\n",
    "                error = np.sum(w[Y != predictions]) # Calculamos el error\n",
    "                if error > 0.5: # Si el error es mayor que 0.5\n",
    "                    error = 1 - error # El error es 1 menos el error\n",
    "                    clf.polarity = -1 # La polaridad es -1\n",
    "\n",
    "                if error < min_error: # Si el error es menor que el error mínimo\n",
    "                    min_error = error # El error mínimo es el error\n",
    "                    best_clf = clf # El mejor clasificador débil es el clasificador débil\n",
    "\n",
    "            EPS = 1e-10 # Definimos un valor muy pequeño\n",
    "            best_clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS)) # Calculamos el alpha\n",
    "            predictions = best_clf.predict(X) # Realizamos las predicciones\n",
    "            w *= np.exp(-best_clf.alpha * Y * predictions) # Actualizamos los pesos\n",
    "            w /= np.sum(w)  # Normalizamos los pesos\n",
    "            self.clfs.append(best_clf) # Añadimos el mejor clasificador débil\n",
    "            \n",
    "            for i in range(iter_number):\n",
    "                y_pred_verification = self.predict(X_Verification)\n",
    "                partialAccuracy = np.sum(Y_Verification == np.sign(y_pred_verification)) / len(Y_Verification)\n",
    "                newAccuracy += partialAccuracy\n",
    "            \n",
    "            newAccuracy /= iter_number\n",
    "            if t == 6:\n",
    "                print(\"\")\n",
    "            #if newAccuracy < bestAccuracy:\n",
    "            if round(newAccuracy, round2) < round(bestAccuracy, round2):\n",
    "                self.clfs.pop() # Borramos el último clasificador débil\n",
    "                t = t - 1\n",
    "                print(\"Se ha detectado sobreentrenamiento. El número óptimo de clasificadores débiles es: \", t)\n",
    "                break\n",
    "\n",
    "            if verbose: # Si verbose es True\n",
    "                print(f'Classifier {len(self.clfs)}: error = {min_error}, alpha = {best_clf.alpha}, newAccuracy = {newAccuracy}, bestAccuracy = {bestAccuracy}') # Mostramos el error y el alpha\n",
    "        self.T = t\n",
    "        \n",
    "    def predict(self, X): # Creamos la función predict\n",
    "        clf_preds = np.zeros(X.shape[0]) # Inicializamos las predicciones\n",
    "        for clf in self.clfs: # Para cada clasificador débil\n",
    "            predictions = clf.predict(X) # Realizamos las predicciones\n",
    "            clf_preds += clf.alpha * predictions # Actualizamos las predicciones\n",
    "        return clf_preds # Devolvemos las predicciones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "\n",
    "def run_adaboost_on_mnist(digit, T=5, A=20, verboseParam=False, n_components=50): # Creamos la función run_adaboost_on_mnist\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Aplanamos los datos de prueba reduciendo la dimensión a 1D   \n",
    "\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components) # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) \n",
    "\n",
    "    adaboost = AdaBoost(T=T, A=A)  # Creamos el clasificador AdaBoost\n",
    "    adaboost.fit(X_train_reduced, y_train_binary, verboseParam) # Ajustamos el clasificador AdaBoost\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits(T=5, A=20, verboseParam=False): # Creamos la función run_adaboost_for_all_digits\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_on_mnist(digit, T, A, verboseParam) # Ejecutamos AdaBoost\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "def run_adaboost_for_one_digit_detecting_overfitting(digit, A=20, \n",
    "                                                    verboseParam=False, n_components=50,\n",
    "                                                    split_proportion = 0.90, iter_number = 100,\n",
    "                                                    round1 = 3, round2 = 3,\n",
    "                                                    bestAccuracyBreak = 0.999, practicalAccuracyBreak = 666): # Creamos la función run_adaboost_for_one_digit\n",
    "    print(f\"Running AdaBoost for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data() # Cargamos los datos\n",
    "    num_samples = len(X_train)\n",
    "    split_point = int(split_proportion * num_samples)\n",
    "    \n",
    "    X_True_Train = X_train[:split_point]\n",
    "    y_True_Train = y_train[:split_point]\n",
    "\n",
    "    X_Verification = X_train[split_point:]\n",
    "    y_verification = y_train[split_point:]\n",
    "    \n",
    "    X_True_Train = X_True_Train.reshape(X_True_Train.shape[0], -1) \n",
    "    X_Verification = X_Verification.reshape(X_Verification.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) # Aplanamos los datos de prueba reduciendo la dimensión a 1D   \n",
    "\n",
    "    \n",
    "    y_true_train_binary = np.where(y_True_Train == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_verification_binary = np.where(y_verification == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1)\n",
    "    \n",
    "    adaboost = AdaBoost(T=0, A=A)\n",
    "    \n",
    "    adaboost.fit(X_True_Train, y_true_train_binary, \n",
    "                 X_Verification, y_verification_binary, \n",
    "                 iter_number, verboseParam,\n",
    "                 round1 = round1, round2 = round2,\n",
    "                 bestAccuracyBreak=bestAccuracyBreak, practicalAccuracyBreak=practicalAccuracyBreak)\n",
    "    y_pred = adaboost.predict(X_test)\n",
    "    accuracy = np.sum(y_test_binary == np.sign(y_pred)) / len(y_test_binary) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit}: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost for digit: 3\n",
      "Classifier 1: error = 0.10151111111111133, alpha = 1.0902730387091557, newAccuracy = 0.895799999999999, bestAccuracy = 0\n",
      "Classifier 2: error = 0.34407308688732896, alpha = 0.322597636486343, newAccuracy = 0.895799999999999, bestAccuracy = 0.895799999999999\n",
      "Classifier 3: error = 0.3539423093701759, alpha = 0.30087743603789063, newAccuracy = 0.895799999999999, bestAccuracy = 0.895799999999999\n",
      "Classifier 4: error = 0.3383115932240719, alpha = 0.3354137080472982, newAccuracy = 0.895799999999999, bestAccuracy = 0.895799999999999\n",
      "Classifier 5: error = 0.32530939712422036, alpha = 0.3647387494482685, newAccuracy = 0.9030666666666655, bestAccuracy = 0.895799999999999\n",
      "\n",
      "Classifier 6: error = 0.36344597444225957, alpha = 0.2802193167424667, newAccuracy = 0.9031333333333332, bestAccuracy = 0.9030666666666655\n",
      "Classifier 7: error = 0.37002788007066967, alpha = 0.2660486049362806, newAccuracy = 0.911000000000001, bestAccuracy = 0.9031333333333332\n",
      "Classifier 8: error = 0.35401746862424877, alpha = 0.3007131018814206, newAccuracy = 0.9130666666666659, bestAccuracy = 0.911000000000001\n",
      "Classifier 9: error = 0.41535068261417196, alpha = 0.17094451410222786, newAccuracy = 0.9108666666666649, bestAccuracy = 0.9130666666666659\n",
      "Classifier 10: error = 0.39078375747876115, alpha = 0.22200946277702405, newAccuracy = 0.9136666666666673, bestAccuracy = 0.9108666666666649\n",
      "Classifier 11: error = 0.3806441347414856, alpha = 0.24340755088867846, newAccuracy = 0.9309999999999985, bestAccuracy = 0.9136666666666673\n",
      "Classifier 12: error = 0.35589182424016297, alpha = 0.29661993323642893, newAccuracy = 0.9325999999999985, bestAccuracy = 0.9309999999999985\n",
      "Classifier 13: error = 0.39097760302665296, alpha = 0.22160238324921444, newAccuracy = 0.9333333333333335, bestAccuracy = 0.9325999999999985\n",
      "Classifier 14: error = 0.3996357218948263, alpha = 0.203491582084317, newAccuracy = 0.9394000000000013, bestAccuracy = 0.9333333333333335\n",
      "Classifier 15: error = 0.4282144467449518, alpha = 0.14456995011567442, newAccuracy = 0.9395333333333311, bestAccuracy = 0.9394000000000013\n",
      "Classifier 16: error = 0.425202853673191, alpha = 0.15072541584123714, newAccuracy = 0.9417333333333321, bestAccuracy = 0.9395333333333311\n",
      "Classifier 17: error = 0.4219230367813508, alpha = 0.15744204699687572, newAccuracy = 0.9407999999999995, bestAccuracy = 0.9417333333333321\n",
      "Classifier 18: error = 0.44413282898517376, alpha = 0.11220284126947205, newAccuracy = 0.9429999999999988, bestAccuracy = 0.9407999999999995\n",
      "Classifier 19: error = 0.42707366154891946, alpha = 0.1469003231859575, newAccuracy = 0.9436000000000022, bestAccuracy = 0.9429999999999988\n",
      "Classifier 20: error = 0.40678451269072946, alpha = 0.1886370627673574, newAccuracy = 0.9438666666666652, bestAccuracy = 0.9436000000000022\n",
      "Classifier 21: error = 0.4117923827922416, alpha = 0.1782803394592158, newAccuracy = 0.9453333333333339, bestAccuracy = 0.9438666666666652\n",
      "Classifier 22: error = 0.4269361349122138, alpha = 0.1471813663739163, newAccuracy = 0.9456666666666674, bestAccuracy = 0.9453333333333339\n",
      "Classifier 23: error = 0.4263607943808572, alpha = 0.14835735700202596, newAccuracy = 0.9470666666666677, bestAccuracy = 0.9456666666666674\n",
      "Classifier 24: error = 0.43963621924090246, alpha = 0.12131928539049681, newAccuracy = 0.947199999999999, bestAccuracy = 0.9470666666666677\n",
      "Classifier 25: error = 0.43758782917482, alpha = 0.12547877151609585, newAccuracy = 0.9482, bestAccuracy = 0.947199999999999\n",
      "Classifier 26: error = 0.462752729351088, alpha = 0.074632802853336, newAccuracy = 0.9476666666666644, bestAccuracy = 0.9482\n",
      "Classifier 27: error = 0.4294492419669693, alpha = 0.1420492906686508, newAccuracy = 0.948866666666665, bestAccuracy = 0.9476666666666644\n",
      "Classifier 28: error = 0.448464661639313, alpha = 0.10343801367297656, newAccuracy = 0.9477333333333323, bestAccuracy = 0.948866666666665\n",
      "Classifier 29: error = 0.4435781303557287, alpha = 0.11332640547784067, newAccuracy = 0.9496666666666681, bestAccuracy = 0.9477333333333323\n",
      "Classifier 30: error = 0.4280023762596486, alpha = 0.14500304471867853, newAccuracy = 0.9491333333333348, bestAccuracy = 0.9496666666666681\n",
      "Classifier 31: error = 0.43525522320441457, alpha = 0.13022066375239633, newAccuracy = 0.9512, bestAccuracy = 0.9491333333333348\n",
      "Classifier 32: error = 0.45544147534994484, alpha = 0.08935409790817993, newAccuracy = 0.950533333333335, bestAccuracy = 0.9512\n",
      "Classifier 33: error = 0.46682171674818707, alpha = 0.06645421821812983, newAccuracy = 0.9506666666666643, bestAccuracy = 0.950533333333335\n",
      "Classifier 34: error = 0.45445143945458444, alpha = 0.09135037874876967, newAccuracy = 0.9512, bestAccuracy = 0.9506666666666643\n",
      "Classifier 35: error = 0.4385127712774871, alpha = 0.12360004631643755, newAccuracy = 0.9508000000000006, bestAccuracy = 0.9512\n",
      "Classifier 36: error = 0.44821116698744745, alpha = 0.10395047395477759, newAccuracy = 0.9523333333333323, bestAccuracy = 0.9508000000000006\n",
      "Classifier 37: error = 0.4338673303771041, alpha = 0.13304482615852564, newAccuracy = 0.9522000000000009, bestAccuracy = 0.9523333333333323\n",
      "Classifier 38: error = 0.4247074932301659, alpha = 0.1517389690330419, newAccuracy = 0.9544666666666654, bestAccuracy = 0.9522000000000009\n",
      "Classifier 39: error = 0.44664490871236584, alpha = 0.10711801110899365, newAccuracy = 0.9546666666666678, bestAccuracy = 0.9544666666666654\n",
      "Classifier 40: error = 0.4562580330044478, alpha = 0.08770814879509604, newAccuracy = 0.9553999999999985, bestAccuracy = 0.9546666666666678\n",
      "Classifier 41: error = 0.45423583170247084, alpha = 0.09178521997227837, newAccuracy = 0.9550666666666671, bestAccuracy = 0.9553999999999985\n",
      "Classifier 42: error = 0.4721079243155275, alpha = 0.055842124009460106, newAccuracy = 0.9554666666666664, bestAccuracy = 0.9550666666666671\n",
      "Classifier 43: error = 0.47261936632091683, alpha = 0.0548161053470333, newAccuracy = 0.9558666666666656, bestAccuracy = 0.9554666666666664\n",
      "Classifier 44: error = 0.471685721646954, alpha = 0.05668920545835346, newAccuracy = 0.9556000000000004, bestAccuracy = 0.9558666666666656\n",
      "Classifier 45: error = 0.46037193416245437, alpha = 0.07942270997112749, newAccuracy = 0.9550666666666671, bestAccuracy = 0.9556000000000004\n",
      "Classifier 46: error = 0.4645073997249528, alpha = 0.07110479138647681, newAccuracy = 0.9551333333333349, bestAccuracy = 0.9550666666666671\n",
      "Classifier 47: error = 0.46724447048461504, alpha = 0.06560501899765793, newAccuracy = 0.9553999999999985, bestAccuracy = 0.9551333333333349\n",
      "Classifier 48: error = 0.4591042653099504, alpha = 0.0819745957158293, newAccuracy = 0.9559333333333341, bestAccuracy = 0.9553999999999985\n",
      "Classifier 49: error = 0.4571986587642607, alpha = 0.0858127002754058, newAccuracy = 0.955733333333332, bestAccuracy = 0.9559333333333341\n",
      "Classifier 50: error = 0.4732424130907945, alpha = 0.05356634867343013, newAccuracy = 0.9559333333333341, bestAccuracy = 0.955733333333332\n",
      "Classifier 51: error = 0.47580416454952246, alpha = 0.04842949784762339, newAccuracy = 0.9562666666666654, bestAccuracy = 0.9559333333333341\n",
      "Classifier 52: error = 0.46630560893059003, alpha = 0.06749107070404467, newAccuracy = 0.9557999999999978, bestAccuracy = 0.9562666666666654\n",
      "Classifier 53: error = 0.448459415251635, alpha = 0.1034486191275299, newAccuracy = 0.9556666666666682, bestAccuracy = 0.9557999999999978\n",
      "Classifier 54: error = 0.4628131925443859, alpha = 0.07451120274950879, newAccuracy = 0.9561999999999992, bestAccuracy = 0.9556666666666682\n",
      "Classifier 55: error = 0.46482983731606087, alpha = 0.07045668001488335, newAccuracy = 0.9560666666666675, bestAccuracy = 0.9561999999999992\n",
      "Classifier 56: error = 0.45723433131669955, alpha = 0.08574082894927491, newAccuracy = 0.9565333333333352, bestAccuracy = 0.9560666666666675\n",
      "Classifier 57: error = 0.4638889196166785, alpha = 0.07234812645173001, newAccuracy = 0.956000000000002, bestAccuracy = 0.9565333333333352\n",
      "Classifier 58: error = 0.4788335368440599, alpha = 0.04235824147861075, newAccuracy = 0.9567333333333327, bestAccuracy = 0.956000000000002\n",
      "Classifier 59: error = 0.47281024691887263, alpha = 0.05443320389387425, newAccuracy = 0.9566666666666646, bestAccuracy = 0.9567333333333327\n",
      "Classifier 60: error = 0.4704685910521751, alpha = 0.059131640559283105, newAccuracy = 0.9569999999999983, bestAccuracy = 0.9566666666666646\n",
      "Classifier 61: error = 0.4730713749341909, alpha = 0.053909413775399324, newAccuracy = 0.9574666666666655, bestAccuracy = 0.9569999999999983\n",
      "Classifier 62: error = 0.4468361960378012, alpha = 0.10673104583887698, newAccuracy = 0.9565333333333352, bestAccuracy = 0.9574666666666655\n",
      "Classifier 63: error = 0.4613300206312083, alpha = 0.07749471670508755, newAccuracy = 0.9569333333333345, bestAccuracy = 0.9565333333333352\n",
      "Classifier 64: error = 0.4597680708034545, alpha = 0.0806381887004208, newAccuracy = 0.9570666666666661, bestAccuracy = 0.9569333333333345\n",
      "Classifier 65: error = 0.46353476589212295, alpha = 0.07306018457008112, newAccuracy = 0.9579999999999987, bestAccuracy = 0.9570666666666661\n",
      "Classifier 66: error = 0.46728545475635325, alpha = 0.06552269759740688, newAccuracy = 0.9585333333333342, bestAccuracy = 0.9579999999999987\n",
      "Classifier 67: error = 0.45853392912504376, alpha = 0.08312305877461695, newAccuracy = 0.9586666666666683, bestAccuracy = 0.9585333333333342\n",
      "Classifier 68: error = 0.46468491783643084, alpha = 0.070747966135053, newAccuracy = 0.9576666666666679, bestAccuracy = 0.9586666666666683\n",
      "Classifier 69: error = 0.4243539173691502, alpha = 0.1524626070402326, newAccuracy = 0.9603333333333343, bestAccuracy = 0.9576666666666679\n",
      "Classifier 70: error = 0.4784681930381752, alpha = 0.04309026370370249, newAccuracy = 0.9605333333333317, bestAccuracy = 0.9603333333333343\n",
      "Classifier 71: error = 0.4830438035675044, alpha = 0.03392540215640195, newAccuracy = 0.9602666666666683, bestAccuracy = 0.9605333333333317\n",
      "Classifier 72: error = 0.4745814006335858, alpha = 0.05088106166311007, newAccuracy = 0.9598666666666689, bestAccuracy = 0.9602666666666683\n",
      "Classifier 73: error = 0.47001883675388567, alpha = 0.06003434637233968, newAccuracy = 0.959800000000001, bestAccuracy = 0.9598666666666689\n",
      "Classifier 74: error = 0.47764079571269386, alpha = 0.044748252720240765, newAccuracy = 0.959800000000001, bestAccuracy = 0.959800000000001\n",
      "Classifier 75: error = 0.4745559790759478, alpha = 0.05093203665099362, newAccuracy = 0.9597333333333326, bestAccuracy = 0.959800000000001\n",
      "Classifier 76: error = 0.46498939705331876, alpha = 0.07013598098594481, newAccuracy = 0.9604666666666656, bestAccuracy = 0.9597333333333326\n",
      "Classifier 77: error = 0.48085132606907344, alpha = 0.038316087753828554, newAccuracy = 0.9610666666666673, bestAccuracy = 0.9604666666666656\n",
      "Classifier 78: error = 0.4769937546522558, alpha = 0.046045003768121893, newAccuracy = 0.9614666666666666, bestAccuracy = 0.9610666666666673\n",
      "Classifier 79: error = 0.470115409877206, alpha = 0.05984050541689306, newAccuracy = 0.9613333333333324, bestAccuracy = 0.9614666666666666\n",
      "Classifier 80: error = 0.4636847744753093, alpha = 0.0727585697491684, newAccuracy = 0.9615333333333344, bestAccuracy = 0.9613333333333324\n",
      "Classifier 81: error = 0.46914223094117513, alpha = 0.06179407185998289, newAccuracy = 0.9617333333333322, bestAccuracy = 0.9615333333333344\n",
      "Classifier 82: error = 0.4663382653421678, alpha = 0.06742546021608886, newAccuracy = 0.9618666666666659, bestAccuracy = 0.9617333333333322\n",
      "Classifier 83: error = 0.4751037961988286, alpha = 0.04983361875166985, newAccuracy = 0.9616666666666684, bestAccuracy = 0.9618666666666659\n",
      "Classifier 84: error = 0.4679588844001037, alpha = 0.06417016653972434, newAccuracy = 0.9617333333333322, bestAccuracy = 0.9616666666666684\n",
      "Classifier 85: error = 0.47307201129649873, alpha = 0.053908137348463, newAccuracy = 0.9613999999999986, bestAccuracy = 0.9617333333333322\n",
      "Classifier 86: error = 0.4567349811844027, alpha = 0.08674697607557745, newAccuracy = 0.9617999999999978, bestAccuracy = 0.9613999999999986\n",
      "Classifier 87: error = 0.4689629127984481, alpha = 0.062154087359022635, newAccuracy = 0.9609999999999989, bestAccuracy = 0.9617999999999978\n",
      "Classifier 88: error = 0.46596586470322443, alpha = 0.06817369046687345, newAccuracy = 0.9614666666666666, bestAccuracy = 0.9609999999999989\n",
      "Classifier 89: error = 0.4772029047689842, alpha = 0.0456258241123, newAccuracy = 0.9612000000000013, bestAccuracy = 0.9614666666666666\n",
      "Classifier 90: error = 0.4830960159069224, alpha = 0.03382085761593956, newAccuracy = 0.9610666666666673, bestAccuracy = 0.9612000000000013\n",
      "Classifier 91: error = 0.45938600755420866, alpha = 0.08140734247472867, newAccuracy = 0.9620000000000022, bestAccuracy = 0.9610666666666673\n",
      "Classifier 92: error = 0.4709695246073683, alpha = 0.05812632564207581, newAccuracy = 0.9616000000000007, bestAccuracy = 0.9620000000000022\n",
      "Classifier 93: error = 0.47947457006992034, alpha = 0.04107394246354283, newAccuracy = 0.9616000000000007, bestAccuracy = 0.9616000000000007\n",
      "Classifier 94: error = 0.4752884894417164, alpha = 0.04946332098656891, newAccuracy = 0.9620000000000022, bestAccuracy = 0.9616000000000007\n",
      "Classifier 95: error = 0.46391819099617804, alpha = 0.07228927698026877, newAccuracy = 0.9621999999999993, bestAccuracy = 0.9620000000000022\n",
      "Classifier 96: error = 0.4865914239611504, alpha = 0.026823583452201094, newAccuracy = 0.9623333333333334, bestAccuracy = 0.9621999999999993\n",
      "Classifier 97: error = 0.4786782796799344, alpha = 0.04266931737775706, newAccuracy = 0.9620000000000022, bestAccuracy = 0.9623333333333334\n",
      "Classifier 98: error = 0.47820934160583295, alpha = 0.043608940050396866, newAccuracy = 0.9621999999999993, bestAccuracy = 0.9620000000000022\n",
      "Classifier 99: error = 0.46682595055045406, alpha = 0.06644571316899554, newAccuracy = 0.9621999999999993, bestAccuracy = 0.9621999999999993\n",
      "Classifier 100: error = 0.47343419394688113, alpha = 0.05318169323334936, newAccuracy = 0.9618666666666659, bestAccuracy = 0.9621999999999993\n",
      "Classifier 101: error = 0.48034676113893693, alpha = 0.03932673932550176, newAccuracy = 0.9620666666666677, bestAccuracy = 0.9618666666666659\n",
      "Classifier 102: error = 0.4892685065263594, alpha = 0.021466283566028112, newAccuracy = 0.9619333333333341, bestAccuracy = 0.9620666666666677\n",
      "Classifier 103: error = 0.46887642166796184, alpha = 0.062327740607581374, newAccuracy = 0.9621333333333315, bestAccuracy = 0.9619333333333341\n",
      "Classifier 104: error = 0.48809192511209776, alpha = 0.02382065421553595, newAccuracy = 0.9616666666666684, bestAccuracy = 0.9621333333333315\n",
      "Classifier 105: error = 0.4791984445885804, alpha = 0.041627138254266186, newAccuracy = 0.9617333333333322, bestAccuracy = 0.9616666666666684\n",
      "Classifier 106: error = 0.4779281731191143, alpha = 0.044172361014199565, newAccuracy = 0.9617999999999978, bestAccuracy = 0.9617333333333322\n",
      "Classifier 107: error = 0.477676866158501, alpha = 0.04467596751036801, newAccuracy = 0.9614666666666666, bestAccuracy = 0.9617999999999978\n",
      "Classifier 108: error = 0.4751619799278098, alpha = 0.04971696274578115, newAccuracy = 0.9618666666666659, bestAccuracy = 0.9614666666666666\n",
      "Classifier 109: error = 0.4758650111609938, alpha = 0.04830751969963032, newAccuracy = 0.9621333333333315, bestAccuracy = 0.9618666666666659\n",
      "Classifier 110: error = 0.4829777641514902, alpha = 0.03405763365433886, newAccuracy = 0.9624000000000014, bestAccuracy = 0.9621333333333315\n",
      "Classifier 111: error = 0.4774395667166637, alpha = 0.04515152442881123, newAccuracy = 0.9620000000000022, bestAccuracy = 0.9624000000000014\n",
      "Classifier 112: error = 0.47611529033999794, alpha = 0.04780580443020297, newAccuracy = 0.9622666666666656, bestAccuracy = 0.9620000000000022\n",
      "Classifier 113: error = 0.4841683827548572, alpha = 0.0316738222888312, newAccuracy = 0.9624000000000014, bestAccuracy = 0.9622666666666656\n",
      "Classifier 114: error = 0.47893183279574536, alpha = 0.04216129827444252, newAccuracy = 0.9624000000000014, bestAccuracy = 0.9624000000000014\n",
      "Classifier 115: error = 0.4701011795799849, alpha = 0.05986906809595106, newAccuracy = 0.9617333333333322, bestAccuracy = 0.9624000000000014\n",
      "Classifier 116: error = 0.4706125887284519, alpha = 0.05884264196713771, newAccuracy = 0.9628000000000011, bestAccuracy = 0.9617333333333322\n",
      "Classifier 117: error = 0.4722576385289863, alpha = 0.055541765914780286, newAccuracy = 0.9618666666666659, bestAccuracy = 0.9628000000000011\n",
      "Classifier 118: error = 0.4749405283164961, alpha = 0.0501609714506227, newAccuracy = 0.9624000000000014, bestAccuracy = 0.9618666666666659\n",
      "Classifier 119: error = 0.4801421983222094, alpha = 0.03973650465843524, newAccuracy = 0.9619333333333341, bestAccuracy = 0.9624000000000014\n",
      "Classifier 120: error = 0.4836422624677513, alpha = 0.03272715437526921, newAccuracy = 0.9624666666666676, bestAccuracy = 0.9619333333333341\n",
      "Classifier 121: error = 0.4859542072136221, alpha = 0.028098978439383273, newAccuracy = 0.9623333333333334, bestAccuracy = 0.9624666666666676\n",
      "Classifier 122: error = 0.48351276903564877, alpha = 0.032986420935557474, newAccuracy = 0.9622666666666656, bestAccuracy = 0.9623333333333334\n",
      "Classifier 123: error = 0.4767768703877765, alpha = 0.046479701319961535, newAccuracy = 0.9624666666666676, bestAccuracy = 0.9622666666666656\n",
      "Classifier 124: error = 0.4836082287546148, alpha = 0.03279529488431739, newAccuracy = 0.9623333333333334, bestAccuracy = 0.9624666666666676\n",
      "Classifier 125: error = 0.47427598977053953, alpha = 0.051493485242930105, newAccuracy = 0.9628000000000011, bestAccuracy = 0.9623333333333334\n",
      "Classifier 126: error = 0.479640022802773, alpha = 0.040742482921614406, newAccuracy = 0.9621999999999993, bestAccuracy = 0.9628000000000011\n",
      "Classifier 127: error = 0.4818499757755401, alpha = 0.03631600517204019, newAccuracy = 0.9627333333333327, bestAccuracy = 0.9621999999999993\n",
      "Classifier 128: error = 0.4801582894589466, alpha = 0.03970427158398646, newAccuracy = 0.9626666666666648, bestAccuracy = 0.9627333333333327\n",
      "Classifier 129: error = 0.4703514195268995, alpha = 0.05936680722831833, newAccuracy = 0.9627333333333327, bestAccuracy = 0.9626666666666648\n",
      "Classifier 130: error = 0.4763198604742517, alpha = 0.04739573641202011, newAccuracy = 0.9634666666666657, bestAccuracy = 0.9627333333333327\n",
      "Classifier 131: error = 0.4569979607611889, alpha = 0.08621707337078935, newAccuracy = 0.9635999999999997, bestAccuracy = 0.9634666666666657\n",
      "Classifier 132: error = 0.47714597029078454, alpha = 0.04573993087016055, newAccuracy = 0.9635333333333318, bestAccuracy = 0.9635999999999997\n",
      "Classifier 133: error = 0.4807003541200612, alpha = 0.0386184786705207, newAccuracy = 0.9634666666666657, bestAccuracy = 0.9635333333333318\n",
      "Classifier 134: error = 0.4804129691308576, alpha = 0.03919411913308297, newAccuracy = 0.9629999999999984, bestAccuracy = 0.9634666666666657\n",
      "Classifier 135: error = 0.47931947063514596, alpha = 0.04138466892765908, newAccuracy = 0.9633333333333344, bestAccuracy = 0.9629999999999984\n",
      "Classifier 136: error = 0.481510849972985, alpha = 0.03699516853184131, newAccuracy = 0.9629999999999984, bestAccuracy = 0.9633333333333344\n",
      "Classifier 137: error = 0.4858242242782005, alpha = 0.028359151528303712, newAccuracy = 0.9634000000000023, bestAccuracy = 0.9629999999999984\n",
      "Classifier 138: error = 0.4806553261502722, alpha = 0.03870866929933416, newAccuracy = 0.9633333333333344, bestAccuracy = 0.9634000000000023\n",
      "Classifier 139: error = 0.478252910284661, alpha = 0.043521637207062897, newAccuracy = 0.9635999999999997, bestAccuracy = 0.9633333333333344\n",
      "Classifier 140: error = 0.47552866919800096, alpha = 0.04898179668062017, newAccuracy = 0.9638666666666653, bestAccuracy = 0.9635999999999997\n",
      "Classifier 141: error = 0.47317400462665304, alpha = 0.05370355956184467, newAccuracy = 0.9641333333333353, bestAccuracy = 0.9638666666666653\n",
      "Classifier 142: error = 0.4791766313528488, alpha = 0.04167084044564346, newAccuracy = 0.9638000000000015, bestAccuracy = 0.9641333333333353\n",
      "Classifier 143: error = 0.4849430448794396, alpha = 0.030123018100249626, newAccuracy = 0.963999999999999, bestAccuracy = 0.9638000000000015\n",
      "Classifier 144: error = 0.4795559497770086, alpha = 0.0409109093983132, newAccuracy = 0.9639333333333311, bestAccuracy = 0.963999999999999\n",
      "Classifier 145: error = 0.4875822592425261, alpha = 0.024840589584965017, newAccuracy = 0.9643333333333325, bestAccuracy = 0.9639333333333311\n",
      "Classifier 146: error = 0.46895404996347634, alpha = 0.06217188161342026, newAccuracy = 0.9635999999999997, bestAccuracy = 0.9643333333333325\n",
      "Classifier 147: error = 0.4788218042192711, alpha = 0.0423817488786017, newAccuracy = 0.9638666666666653, bestAccuracy = 0.9635999999999997\n",
      "Classifier 148: error = 0.4806767470722818, alpha = 0.03866576330190225, newAccuracy = 0.9639333333333311, bestAccuracy = 0.9638666666666653\n",
      "Classifier 149: error = 0.4815655137078735, alpha = 0.036885691806344385, newAccuracy = 0.9632666666666683, bestAccuracy = 0.9639333333333311\n",
      "Classifier 150: error = 0.48228359071498106, alpha = 0.0354476581961651, newAccuracy = 0.9633333333333344, bestAccuracy = 0.9632666666666683\n",
      "Classifier 151: error = 0.4848207673900422, alpha = 0.030367796863586292, newAccuracy = 0.9634666666666657, bestAccuracy = 0.9633333333333344\n",
      "Classifier 152: error = 0.48537215903285635, alpha = 0.02926403281390842, newAccuracy = 0.9637333333333338, bestAccuracy = 0.9634666666666657\n",
      "Classifier 153: error = 0.475849935325103, alpha = 0.048337741832691294, newAccuracy = 0.9635333333333318, bestAccuracy = 0.9637333333333338\n",
      "Classifier 154: error = 0.47098549471138806, alpha = 0.05809427745665959, newAccuracy = 0.9638000000000015, bestAccuracy = 0.9635333333333318\n",
      "Classifier 155: error = 0.47692296172360304, alpha = 0.04618689094704563, newAccuracy = 0.9638000000000015, bestAccuracy = 0.9638000000000015\n",
      "Classifier 156: error = 0.48080810664505425, alpha = 0.03840265385399317, newAccuracy = 0.9636666666666681, bestAccuracy = 0.9638000000000015\n",
      "Classifier 157: error = 0.483455031203575, alpha = 0.03310202273596642, newAccuracy = 0.9638666666666653, bestAccuracy = 0.9636666666666681\n",
      "Classifier 158: error = 0.48146956202752955, alpha = 0.03707785774420532, newAccuracy = 0.9637333333333338, bestAccuracy = 0.9638666666666653\n",
      "Classifier 159: error = 0.48295297634794876, alpha = 0.034107266871413855, newAccuracy = 0.963999999999999, bestAccuracy = 0.9637333333333338\n",
      "Classifier 160: error = 0.4815334589969118, alpha = 0.036949888644200554, newAccuracy = 0.9643999999999987, bestAccuracy = 0.963999999999999\n",
      "Classifier 161: error = 0.47541979070593, alpha = 0.049200078864219864, newAccuracy = 0.9644666666666667, bestAccuracy = 0.9643999999999987\n",
      "Classifier 162: error = 0.4827055114898703, alpha = 0.03460278097697079, newAccuracy = 0.9644666666666667, bestAccuracy = 0.9644666666666667\n",
      "Classifier 163: error = 0.48423404379715085, alpha = 0.03154236896020334, newAccuracy = 0.9642000000000008, bestAccuracy = 0.9644666666666667\n",
      "Classifier 164: error = 0.4764747387449494, alpha = 0.04708528808068593, newAccuracy = 0.9643333333333325, bestAccuracy = 0.9642000000000008\n",
      "Classifier 165: error = 0.48782252561792294, alpha = 0.024359765962951856, newAccuracy = 0.9642000000000008, bestAccuracy = 0.9643333333333325\n",
      "Classifier 166: error = 0.4855743730789272, alpha = 0.028859263048535352, newAccuracy = 0.963999999999999, bestAccuracy = 0.9642000000000008\n",
      "Classifier 167: error = 0.48082303572381, alpha = 0.03837275167541802, newAccuracy = 0.9643333333333325, bestAccuracy = 0.963999999999999\n",
      "Classifier 168: error = 0.4843580004666106, alpha = 0.03129421082205906, newAccuracy = 0.9642666666666646, bestAccuracy = 0.9643333333333325\n",
      "Classifier 169: error = 0.47790272518521426, alpha = 0.044223356369325624, newAccuracy = 0.9643999999999987, bestAccuracy = 0.9642666666666646\n",
      "Classifier 170: error = 0.4830363941908379, alpha = 0.033940237979132454, newAccuracy = 0.9642666666666646, bestAccuracy = 0.9643999999999987\n",
      "Classifier 171: error = 0.47944833723595875, alpha = 0.04112649680786027, newAccuracy = 0.9644666666666667, bestAccuracy = 0.9642666666666646\n",
      "Classifier 172: error = 0.47829493985127014, alpha = 0.04343741906243139, newAccuracy = 0.9639333333333311, bestAccuracy = 0.9644666666666667\n",
      "Classifier 173: error = 0.4748906907427912, alpha = 0.05026089810414775, newAccuracy = 0.964866666666666, bestAccuracy = 0.9639333333333311\n",
      "Classifier 174: error = 0.4756449692487247, alpha = 0.04874864073120805, newAccuracy = 0.9647999999999979, bestAccuracy = 0.964866666666666\n",
      "Classifier 175: error = 0.47659734616811045, alpha = 0.04683953202199872, newAccuracy = 0.9649333333333342, bestAccuracy = 0.9647999999999979\n",
      "Classifier 176: error = 0.48425252279428854, alpha = 0.031505374226591214, newAccuracy = 0.9651999999999994, bestAccuracy = 0.9649333333333342\n",
      "Classifier 177: error = 0.4791027576085414, alpha = 0.04181884555357985, newAccuracy = 0.9650666666666678, bestAccuracy = 0.9651999999999994\n",
      "Se ha detectado sobreentrenamiento. El número óptimo de clasificadores débiles es:  177\n",
      "Accuracy for digit 3: 0.9635\n"
     ]
    }
   ],
   "source": [
    "accuracy = run_adaboost_for_one_digit_detecting_overfitting(digit=3, A=20, \n",
    "                                                            verboseParam=True, n_components=50, \n",
    "                                                            split_proportion=0.75, iter_number=100,\n",
    "                                                            round1 = 2,\n",
    "                                                            round2 = 2,\n",
    "                                                            bestAccuracyBreak = 0.999,\n",
    "                                                            practicalAccuracyBreak = 6666)  # Ejecutamos AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAREA 2A: AdaboostClassifier de la librería sklearn utilizando como clasificador débil la clase \n",
    "DecisionTreeClassifier con profundidad 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución Minimalista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost with sklearn for digit: 0\n",
      "Accuracy for digit 0 with sklearn: 0.9837\n",
      "Running AdaBoost with sklearn for digit: 1\n",
      "Accuracy for digit 1 with sklearn: 0.9868\n",
      "Running AdaBoost with sklearn for digit: 2\n",
      "Accuracy for digit 2 with sklearn: 0.9644\n",
      "Running AdaBoost with sklearn for digit: 3\n",
      "Accuracy for digit 3 with sklearn: 0.962\n",
      "Running AdaBoost with sklearn for digit: 4\n",
      "Accuracy for digit 4 with sklearn: 0.9601\n",
      "Running AdaBoost with sklearn for digit: 5\n",
      "Accuracy for digit 5 with sklearn: 0.9523\n",
      "Running AdaBoost with sklearn for digit: 6\n",
      "Accuracy for digit 6 with sklearn: 0.9767\n",
      "Running AdaBoost with sklearn for digit: 7\n",
      "Accuracy for digit 7 with sklearn: 0.9707\n",
      "Running AdaBoost with sklearn for digit: 8\n",
      "Accuracy for digit 8 with sklearn: 0.9639\n",
      "Running AdaBoost with sklearn for digit: 9\n",
      "Accuracy for digit 9 with sklearn: 0.9559\n",
      "Accuracies for all digits: {0: 0.9837, 1: 0.9868, 2: 0.9644, 3: 0.962, 4: 0.9601, 5: 0.9523, 6: 0.9767, 7: 0.9707, 8: 0.9639, 9: 0.9559}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #Importamos la librería numpy que sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # Importamos el dataset MNIST que contiene imágenes de dígitos escritos a mano\n",
    "from sklearn.ensemble import AdaBoostClassifier # Importamos el clasificador AdaBoost de la librería scikit-learn que se utilizará como clasificador fuerte\n",
    "from sklearn.tree import DecisionTreeClassifier # Importamos el clasificador DecisionTree de la librería scikit-learn que se utilizará como clasificador débil\n",
    "from sklearn.metrics import accuracy_score # Importamos la función accuracy_score de la librería scikit-learn que se utilizará para calcular la precisión\n",
    "\n",
    "def run_adaboost_with_sklearn(digit, T=50, A=20): # Creamos la función run_adaboost_with_sklearn\n",
    "    print(f\"Running AdaBoost with sklearn for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()  # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)  # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)  # Aplanamos los datos de prueba reduciendo la dimensión a 1D\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1)  # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "\n",
    "    weak_clf = DecisionTreeClassifier(max_depth=1, random_state=42) # Creamos un clasificador débil DecisionTree con profundidad 1\n",
    "\n",
    "\n",
    "    adaboost = AdaBoostClassifier(estimator=weak_clf, n_estimators=T, algorithm='SAMME', random_state=42) # Creamos el clasificador AdaBoost con el clasificador débil y el número de iteraciones\n",
    "\n",
    "    adaboost.fit(X_train, y_train_binary) # Ajustamos el clasificador AdaBoost\n",
    "    \n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred) # Calculamos la precisión\n",
    "    \n",
    "    print(f\"Accuracy for digit {digit} with sklearn: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits_sklearn(T=50, A=20): # Creamos la función run_adaboost_for_all_digits_sklearn\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_with_sklearn(digit, T, A) # Ejecutamos AdaBoost con sklearn\n",
    "        accuracies[digit] = accuracy  # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "if __name__ == \"__main__\": # Si el script se ejecuta de forma independiente\n",
    "    accuracies = run_adaboost_for_all_digits_sklearn(T=50, A=20)  # Ejecutamos AdaBoost con sklearn para todos los dígitos\n",
    "    print(\"Accuracies for all digits:\", accuracies) # Imprimimos las precisiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución utilizando PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost with sklearn for digit: 0\n",
      "Accuracy for digit 0 with sklearn: 0.985\n",
      "Running AdaBoost with sklearn for digit: 1\n",
      "Accuracy for digit 1 with sklearn: 0.9914\n",
      "Running AdaBoost with sklearn for digit: 2\n",
      "Accuracy for digit 2 with sklearn: 0.9561\n",
      "Running AdaBoost with sklearn for digit: 3\n",
      "Accuracy for digit 3 with sklearn: 0.959\n",
      "Running AdaBoost with sklearn for digit: 4\n",
      "Accuracy for digit 4 with sklearn: 0.9641\n",
      "Running AdaBoost with sklearn for digit: 5\n",
      "Accuracy for digit 5 with sklearn: 0.9441\n",
      "Running AdaBoost with sklearn for digit: 6\n",
      "Accuracy for digit 6 with sklearn: 0.975\n",
      "Running AdaBoost with sklearn for digit: 7\n",
      "Accuracy for digit 7 with sklearn: 0.9712\n",
      "Running AdaBoost with sklearn for digit: 8\n",
      "Accuracy for digit 8 with sklearn: 0.9506\n",
      "Running AdaBoost with sklearn for digit: 9\n",
      "Accuracy for digit 9 with sklearn: 0.9486\n",
      "Accuracies for all digits: {0: 0.985, 1: 0.9914, 2: 0.9561, 3: 0.959, 4: 0.9641, 5: 0.9441, 6: 0.975, 7: 0.9712, 8: 0.9506, 9: 0.9486}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Importamos la librería numpy que sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # Importamos el dataset MNIST que contiene imágenes de dígitos escritos a mano\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis para reducir la dimensión de los datos\n",
    "from sklearn.ensemble import AdaBoostClassifier # Importamos el clasificador AdaBoost de la librería scikit-learn que se utilizará como clasificador fuerte\n",
    "from sklearn.tree import DecisionTreeClassifier # Importamos el clasificador DecisionTree de la librería scikit-learn que se utilizará como clasificador débil\n",
    "from sklearn.metrics import accuracy_score # Importamos la función accuracy_score de la librería scikit-learn que se utilizará para calcular la precisión\n",
    "\n",
    "def run_adaboost_with_sklearn(digit, T=50, A=20, n_components=50): # Creamos la función run_adaboost_with_sklearn\n",
    "    print(f\"Running AdaBoost with sklearn for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()  # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)  # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)  # Aplanamos los datos de prueba reduciendo la dimensión a 1D\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components)  # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1)  # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    weak_clf = DecisionTreeClassifier(max_depth=1, max_features=A, random_state=42) # Creamos un clasificador débil DecisionTree con profundidad 1\n",
    "\n",
    "\n",
    "    adaboost = AdaBoostClassifier(estimator=weak_clf, n_estimators=T, algorithm='SAMME', random_state=42) # Creamos el clasificador AdaBoost con el clasificador débil y el número de iteraciones\n",
    "\n",
    "    adaboost.fit(X_train_reduced, y_train_binary) # Ajustamos el clasificador AdaBoost\n",
    "\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit} with sklearn: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits_sklearn(T=50, A=20, n_components=50): # Creamos la función run_adaboost_for_all_digits_sklearn\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10):  # Para cada dígito\n",
    "        accuracy = run_adaboost_with_sklearn(digit, T, A, n_components) # Ejecutamos AdaBoost con sklearn\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "if __name__ == \"__main__\": # Si el script se ejecuta de forma independiente\n",
    "    accuracies = run_adaboost_for_all_digits_sklearn(T=50, A=20, n_components=50) # Ejecutamos AdaBoost con sklearn para todos los dígitos\n",
    "    print(\"Accuracies for all digits:\", accuracies) # Imprimimos las precisiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2B: clasificador débil árboles de decisión de \n",
    "profundidad mayor que 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución Minimalista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost with sklearn for digit: 0\n",
      "Accuracy for digit 0 with sklearn: 0.9906\n",
      "Running AdaBoost with sklearn for digit: 1\n",
      "Accuracy for digit 1 with sklearn: 0.9938\n",
      "Running AdaBoost with sklearn for digit: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracies \u001b[38;5;66;03m# Devolvemos las precisiones\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m# Si el script se ejecuta de forma independiente\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mrun_adaboost_for_all_digits_sklearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ejecutamos AdaBoost con sklearn para todos los dígitos\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracies for all digits:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracies) \u001b[38;5;66;03m# Imprimimos las precisiones\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 37\u001b[0m, in \u001b[0;36mrun_adaboost_for_all_digits_sklearn\u001b[1;34m(T, A, max_depth)\u001b[0m\n\u001b[0;32m     35\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# Inicializamos las precisiones\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m digit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \u001b[38;5;66;03m# Para cada dígito\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mrun_adaboost_with_sklearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdigit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Ejecutamos AdaBoost con sklearn\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     accuracies[digit] \u001b[38;5;241m=\u001b[39m accuracy  \u001b[38;5;66;03m# Guardamos la precisión\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracies\n",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m, in \u001b[0;36mrun_adaboost_with_sklearn\u001b[1;34m(digit, T, A, max_depth)\u001b[0m\n\u001b[0;32m     19\u001b[0m weak_clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39mmax_depth, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \u001b[38;5;66;03m# Creamos un clasificador débil DecisionTree con profundidad 1\u001b[39;00m\n\u001b[0;32m     22\u001b[0m adaboost \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(estimator\u001b[38;5;241m=\u001b[39mweak_clf, n_estimators\u001b[38;5;241m=\u001b[39mT, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \u001b[38;5;66;03m# Creamos el clasificador AdaBoost con el clasificador débil y el número de iteraciones\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43madaboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_binary\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Ajustamos el clasificador AdaBoost\u001b[39;00m\n\u001b[0;32m     26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m adaboost\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;66;03m# Realizamos las predicciones\u001b[39;00m\n\u001b[0;32m     28\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_binary, y_pred) \u001b[38;5;66;03m# Calculamos la precisión\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:169\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    166\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:589\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[1;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:656\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_discrete\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    654\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m--> 656\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Crislt\\anaconda3\\envs\\SIPrac2Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np #Importamos la librería numpy que sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # Importamos el dataset MNIST que contiene imágenes de dígitos escritos a mano\n",
    "from sklearn.ensemble import AdaBoostClassifier # Importamos el clasificador AdaBoost de la librería scikit-learn que se utilizará como clasificador fuerte\n",
    "from sklearn.tree import DecisionTreeClassifier # Importamos el clasificador DecisionTree de la librería scikit-learn que se utilizará como clasificador débil\n",
    "from sklearn.metrics import accuracy_score # Importamos la función accuracy_score de la librería scikit-learn que se utilizará para calcular la precisión\n",
    "\n",
    "def run_adaboost_with_sklearn(digit, T=50, A=20, max_depth = 1): # Creamos la función run_adaboost_with_sklearn\n",
    "    print(f\"Running AdaBoost with sklearn for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()  # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)  # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)  # Aplanamos los datos de prueba reduciendo la dimensión a 1D\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1)  # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "\n",
    "    weak_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42) # Creamos un clasificador débil DecisionTree con profundidad 1\n",
    "\n",
    "\n",
    "    adaboost = AdaBoostClassifier(estimator=weak_clf, n_estimators=T, algorithm='SAMME', random_state=42) # Creamos el clasificador AdaBoost con el clasificador débil y el número de iteraciones\n",
    "\n",
    "    adaboost.fit(X_train, y_train_binary) # Ajustamos el clasificador AdaBoost\n",
    "    \n",
    "    y_pred = adaboost.predict(X_test) # Realizamos las predicciones\n",
    "\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred) # Calculamos la precisión\n",
    "    \n",
    "    print(f\"Accuracy for digit {digit} with sklearn: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits_sklearn(T=50, A=20, max_depth = 1): # Creamos la función run_adaboost_for_all_digits_sklearn\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10): # Para cada dígito\n",
    "        accuracy = run_adaboost_with_sklearn(digit, T, A, max_depth=max_depth) # Ejecutamos AdaBoost con sklearn\n",
    "        accuracies[digit] = accuracy  # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "if __name__ == \"__main__\": # Si el script se ejecuta de forma independiente\n",
    "    accuracies = run_adaboost_for_all_digits_sklearn(T=50, A=20, max_depth=2)  # Ejecutamos AdaBoost con sklearn para todos los dígitos\n",
    "    print(\"Accuracies for all digits:\", accuracies) # Imprimimos las precisiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución utilizando PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AdaBoost with sklearn for digit: 0\n",
      "Accuracy for digit 0 with sklearn: 0.9951\n",
      "Running AdaBoost with sklearn for digit: 1\n",
      "Accuracy for digit 1 with sklearn: 0.997\n",
      "Running AdaBoost with sklearn for digit: 2\n",
      "Accuracy for digit 2 with sklearn: 0.9823\n",
      "Running AdaBoost with sklearn for digit: 3\n",
      "Accuracy for digit 3 with sklearn: 0.9791\n",
      "Running AdaBoost with sklearn for digit: 4\n",
      "Accuracy for digit 4 with sklearn: 0.9813\n",
      "Running AdaBoost with sklearn for digit: 5\n",
      "Accuracy for digit 5 with sklearn: 0.9819\n",
      "Running AdaBoost with sklearn for digit: 6\n",
      "Accuracy for digit 6 with sklearn: 0.9904\n",
      "Running AdaBoost with sklearn for digit: 7\n",
      "Accuracy for digit 7 with sklearn: 0.9856\n",
      "Running AdaBoost with sklearn for digit: 8\n",
      "Accuracy for digit 8 with sklearn: 0.9744\n",
      "Running AdaBoost with sklearn for digit: 9\n",
      "Accuracy for digit 9 with sklearn: 0.9728\n",
      "Accuracies for all digits: {0: 0.9951, 1: 0.997, 2: 0.9823, 3: 0.9791, 4: 0.9813, 5: 0.9819, 6: 0.9904, 7: 0.9856, 8: 0.9744, 9: 0.9728}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Importamos la librería numpy que sirve para trabajar con matrices\n",
    "from tensorflow.keras.datasets import mnist # Importamos el dataset MNIST que contiene imágenes de dígitos escritos a mano\n",
    "from sklearn.decomposition import PCA # importamos Principal Component Analysis para reducir la dimensión de los datos\n",
    "from sklearn.ensemble import AdaBoostClassifier # Importamos el clasificador AdaBoost de la librería scikit-learn que se utilizará como clasificador fuerte\n",
    "from sklearn.tree import DecisionTreeClassifier # Importamos el clasificador DecisionTree de la librería scikit-learn que se utilizará como clasificador débil\n",
    "from sklearn.metrics import accuracy_score # Importamos la función accuracy_score de la librería scikit-learn que se utilizará para calcular la precisión\n",
    "\n",
    "def run_adaboost_with_sklearn(digit, T=50, A=20, n_components=50, max_depth = 1): # Creamos la función run_adaboost_with_sklearn\n",
    "    print(f\"Running AdaBoost with sklearn for digit: {digit}\") # Mostramos el dígito\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()  # Cargamos los datos\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)  # Aplanamos los datos de entrenamiento reduciendo la dimensión a 1D\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)  # Aplanamos los datos de prueba reduciendo la dimensión a 1D\n",
    "\n",
    "    X_train_reduced, X_test_reduced = apply_pca(X_train, X_test, n_components=n_components)  # Aplicamos PCA a los datos\n",
    "\n",
    "    y_train_binary = np.where(y_train == digit, 1, -1)  # Convertimos las etiquetas a binarias\n",
    "    y_test_binary = np.where(y_test == digit, 1, -1) # Convertimos las etiquetas a binarias\n",
    "\n",
    "    weak_clf = DecisionTreeClassifier(max_depth=max_depth, max_features=A, random_state=42) # Creamos un clasificador débil DecisionTree con profundidad 1\n",
    "\n",
    "\n",
    "    adaboost = AdaBoostClassifier(estimator=weak_clf, n_estimators=T, algorithm='SAMME', random_state=42) # Creamos el clasificador AdaBoost con el clasificador débil y el número de iteraciones\n",
    "\n",
    "    adaboost.fit(X_train_reduced, y_train_binary) # Ajustamos el clasificador AdaBoost\n",
    "\n",
    "    y_pred = adaboost.predict(X_test_reduced) # Realizamos las predicciones\n",
    "\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred) # Calculamos la precisión\n",
    "    print(f\"Accuracy for digit {digit} with sklearn: {accuracy}\") # Mostramos la precisión\n",
    "\n",
    "    return accuracy # Devolvemos la precisión\n",
    "\n",
    "def run_adaboost_for_all_digits_sklearn(T=50, A=20, n_components=50, max_depth = 1): # Creamos la función run_adaboost_for_all_digits_sklearn\n",
    "    accuracies = {} # Inicializamos las precisiones\n",
    "    for digit in range(10):  # Para cada dígito\n",
    "        accuracy = run_adaboost_with_sklearn(digit, T, A, n_components, max_depth=max_depth) # Ejecutamos AdaBoost con sklearn\n",
    "        accuracies[digit] = accuracy # Guardamos la precisión\n",
    "    return accuracies # Devolvemos las precisiones\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=50): # Creamos la función apply_pca\n",
    "    pca = PCA(n_components=n_components) # Creamos el objeto PCA\n",
    "    X_train_reduced = pca.fit_transform(X_train) # Aplicamos PCA a los datos de entrenamiento\n",
    "    X_test_reduced = pca.transform(X_test) # Aplicamos PCA a los datos de prueba\n",
    "    return X_train_reduced, X_test_reduced # Devolvemos los datos reducidos\n",
    "\n",
    "if __name__ == \"__main__\": # Si el script se ejecuta de forma independiente\n",
    "    accuracies = run_adaboost_for_all_digits_sklearn(T=50, A=20, n_components=50, max_depth=4) # Ejecutamos AdaBoost con sklearn para todos los dígitos\n",
    "    print(\"Accuracies for all digits:\", accuracies) # Imprimimos las precisiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2C:  Utilización de la librería Keras para implementar un Multi-Layer \n",
    "Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1 (less fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    Y_train = to_categorical(Y_train, 10)\n",
    "    Y_test = to_categorical(Y_test, 10)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def create_mlp_model(input_shape, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    X_train, Y_train, X_test, Y_test = load_and_preprocess_data()\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    model = create_mlp_model(input_shape, learning_rate)\n",
    "    \n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    validation_split = 0.1\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2 (Faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8676 - loss: 0.4463 - val_accuracy: 0.9682 - val_loss: 0.1007\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9685 - loss: 0.0994 - val_accuracy: 0.9740 - val_loss: 0.0834\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0667 - val_accuracy: 0.9775 - val_loss: 0.0707\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9838 - loss: 0.0505 - val_accuracy: 0.9769 - val_loss: 0.0736\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0419 - val_accuracy: 0.9804 - val_loss: 0.0677\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.0355 - val_accuracy: 0.9815 - val_loss: 0.0644\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9886 - loss: 0.0339 - val_accuracy: 0.9823 - val_loss: 0.0678\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9910 - loss: 0.0262 - val_accuracy: 0.9837 - val_loss: 0.0632\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0278 - val_accuracy: 0.9830 - val_loss: 0.0681\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0238 - val_accuracy: 0.9818 - val_loss: 0.0720\n",
      "Test loss: 0.0720\n",
      "Test accuracy:  0.9818\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    Y_train = to_categorical(Y_train, 10)\n",
    "    Y_test = to_categorical(Y_test, 10)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def create_mlp_model(input_shape, learning_rate, num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.2))  # Dropout layer to prevent overfitting\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    X_train, Y_train, X_test, Y_test = load_and_preprocess_data()\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "    learning_rate = 0.001\n",
    "    num_layers = 2  # Specify the number of layers here\n",
    "    \n",
    "    model = create_mlp_model(input_shape, learning_rate, num_layers)\n",
    "    \n",
    "    batch_size = 128  # Updated to match the image's batch size\n",
    "    epochs = 10\n",
    "    validation_split = 0.1\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, verbose=True, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=False)\n",
    "    print(f'Test loss: {score[0]:.4f}')\n",
    "    print(f'Test accuracy: {score[1]: .4f}')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2D:  Modelado de un clasificador mediante CNN para MNIST \n",
    "con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - accuracy: 0.8324 - loss: 0.5472 - val_accuracy: 0.5864 - val_loss: 1.3409\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9659 - loss: 0.1171 - val_accuracy: 0.9799 - val_loss: 0.0595\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.9735 - loss: 0.0866 - val_accuracy: 0.9881 - val_loss: 0.0352\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9790 - loss: 0.0731 - val_accuracy: 0.9910 - val_loss: 0.0288\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.9825 - loss: 0.0603 - val_accuracy: 0.9915 - val_loss: 0.0265\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9826 - loss: 0.0575 - val_accuracy: 0.9903 - val_loss: 0.0278\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9844 - loss: 0.0524 - val_accuracy: 0.9912 - val_loss: 0.0242\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9852 - loss: 0.0478 - val_accuracy: 0.9927 - val_loss: 0.0225\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9862 - loss: 0.0462 - val_accuracy: 0.9909 - val_loss: 0.0258\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9878 - loss: 0.0391 - val_accuracy: 0.9918 - val_loss: 0.0263\n",
      "Test loss: 0.0263\n",
      "Test accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Reshape the data to fit the model\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "    \n",
    "    # Normalize the data to the range [0, 1]\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def build_cnn_model(num_conv_layers=2, num_dense_layers=1, conv_filters=[32, 64], dense_units=[128], input_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add convolutional layers\n",
    "    for i in range(num_conv_layers):\n",
    "        if i == 0:\n",
    "            # First layer needs to specify the input shape\n",
    "            model.add(Conv2D(conv_filters[i], kernel_size=(3, 3), input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(Conv2D(conv_filters[i], kernel_size=(3, 3)))\n",
    "        \n",
    "        # Add batch normalization\n",
    "        model.add(BatchNormalization())\n",
    "        # Add a LeakyReLU activation function with negative_slope\n",
    "        model.add(LeakyReLU(negative_slope=0.1))\n",
    "        # Add a max pooling layer\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        # Add a dropout layer to prevent overfitting\n",
    "        model.add(Dropout(0.25))\n",
    "    \n",
    "    # Flatten the layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(negative_slope=0.1))\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # Add the output layer with 10 units and softmax activation function\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(x_test, y_test))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss: {:.4f}'.format(score[0]))\n",
    "    print('Test accuracy: {:.4f}'.format(score[1]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the data\n",
    "    (x_train, y_train), (x_test, y_test) = load_and_preprocess_data()\n",
    "    \n",
    "    # Build the CNN model\n",
    "    model = build_cnn_model(num_conv_layers=2, num_dense_layers=1, conv_filters=[32, 64], dense_units=[128])\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    train_and_evaluate_model(model, x_train, y_train, x_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIPrac2Python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
